diff --git a/meson.build b/meson.build
index ed1e43a..60a5667 100644
--- a/meson.build
+++ b/meson.build
@@ -71,7 +71,7 @@ add_project_arguments(cc.get_supported_arguments(flags), language : 'c')
 prog_python = import('python').find_installation('python3')
 
 not_found = dependency('', required: false)
-libdrm_dep = dependency('libdrm', version : '>=2.4.50', required: get_option('drm').enabled() or get_option('venus'))
+libdrm_dep = dependency('libdrm', version : '>=2.4.50', required: get_option('drm').enabled())
 gbm_dep = not_found
 thread_dep = dependency('threads')
 epoxy_dep = dependency('epoxy', version: '>= 1.5.4')
diff --git a/src/virglrenderer.c b/src/virglrenderer.c
index 2f0d6d8..4852bb9 100644
--- a/src/virglrenderer.c
+++ b/src/virglrenderer.c
@@ -42,14 +42,19 @@
 #include "util/u_math.h"
 #include "vkr_allocator.h"
 #include "drm_renderer.h"
-#include "vrend_renderer.h"
 #include "proxy/proxy_renderer.h"
-#include "vrend_winsys.h"
+#include "vrend/vrend_renderer.h"
+#include "vrend/vrend_winsys.h"
+
+#ifndef WIN32
+#include "util/libsync.h"
+#endif
 
 #include "virglrenderer.h"
-#include "virglrenderer_hw.h"
+#include "virtgpu_drm.h"
 
 #include "virgl_context.h"
+#include "virgl_fence.h"
 #include "virgl_resource.h"
 #include "virgl_util.h"
 
@@ -66,6 +71,7 @@ struct global_state {
    bool proxy_initialized;
    bool external_winsys_initialized;
    bool drm_initialized;
+   bool fence_initialized;
 };
 
 static struct global_state state;
@@ -79,14 +85,18 @@ static int virgl_renderer_resource_create_internal(struct virgl_renderer_resourc
    struct virgl_resource *res;
    struct pipe_resource *pipe_res;
    struct vrend_renderer_resource_create_args vrend_args =  { 0 };
+   uint32_t map_info;
 
-   if (!state.vrend_initialized)
+   if (!state.vrend_initialized && !state.drm_initialized)
       return EINVAL;
 
    /* do not accept handle 0 */
    if (args->handle == 0)
       return EINVAL;
 
+   if (virgl_resource_lookup(args->handle))
+      return -EINVAL;
+
    vrend_args.target = args->target;
    vrend_args.format = args->format;
    vrend_args.bind = args->bind;
@@ -102,13 +112,12 @@ static int virgl_renderer_resource_create_internal(struct virgl_renderer_resourc
    if (!pipe_res)
       return EINVAL;
 
+   map_info = vrend_renderer_resource_get_map_info(pipe_res);
    res = virgl_resource_create_from_pipe(args->handle, pipe_res, iov, num_iovs);
-   if (!res) {
-      vrend_renderer_resource_destroy((struct vrend_resource *)pipe_res);
+   if (!res)
       return -ENOMEM;
-   }
 
-   res->map_info = vrend_renderer_resource_get_map_info(pipe_res);
+   res->map_info = map_info;
 
    return 0;
 }
@@ -169,17 +178,43 @@ void virgl_renderer_resource_unref(uint32_t res_handle)
 void virgl_renderer_fill_caps(uint32_t set, uint32_t version,
                               void *caps)
 {
+   if (getenv("VIRGL_DEBUG_CAPS")) {
+      fprintf(stderr, "DEBUG virgl_renderer_fill_caps: set=%u version=%u\n", set, version);
+      fflush(stderr);
+   }
+   
    switch (set) {
-   case VIRGL_RENDERER_CAPSET_VIRGL:
-   case VIRGL_RENDERER_CAPSET_VIRGL2:
-      if (state.vrend_initialized)
+   case VIRTGPU_DRM_CAPSET_VIRGL:
+   case VIRTGPU_DRM_CAPSET_VIRGL2:
+      if (state.vrend_initialized) {
          vrend_renderer_fill_caps(set, version, (union virgl_caps *)caps);
+         if (getenv("VIRGL_DEBUG_CAPS")) {
+            union virgl_caps *vcaps = (union virgl_caps *)caps;
+            fprintf(stderr, "DEBUG virgl_renderer_fill_caps returning to QEMU:\n");
+            fprintf(stderr, "  max_version=%u\n", vcaps->max_version);
+            fprintf(stderr, "  v1.glsl_level=%u at address %p\n", vcaps->v1.glsl_level, (void*)&vcaps->v1.glsl_level);
+            fprintf(stderr, "  v2.v1.glsl_level=%u at address %p\n", vcaps->v2.v1.glsl_level, (void*)&vcaps->v2.v1.glsl_level);
+            fprintf(stderr, "  caps base address=%p\n", (void*)caps);
+            
+            /* Calculate actual offset */
+            size_t glsl_offset = (uint8_t*)&vcaps->v1.glsl_level - (uint8_t*)caps;
+            fprintf(stderr, "  glsl_level offset from caps base: %zu bytes\n", glsl_offset);
+            
+            /* Dump bytes at that offset */
+            uint8_t *bytes = (uint8_t *)caps;
+            fprintf(stderr, "  Bytes at glsl_level offset (%zu-%zu): ", glsl_offset, glsl_offset+7);
+            for (size_t i = glsl_offset; i < glsl_offset + 8 && i < 1024; i++) 
+               fprintf(stderr, "%02x ", bytes[i]);
+            fprintf(stderr, "\n");
+            fflush(stderr);
+         }
+      }
       break;
-   case VIRGL_RENDERER_CAPSET_VENUS:
+   case VIRTGPU_DRM_CAPSET_VENUS:
       if (state.proxy_initialized)
          proxy_get_capset(set, caps);
       break;
-   case VIRGL_RENDERER_CAPSET_DRM:
+   case VIRTGPU_DRM_CAPSET_DRM:
       if (state.drm_initialized)
          drm_renderer_capset(caps);
       break;
@@ -203,8 +238,7 @@ int virgl_renderer_context_create_with_flags(uint32_t ctx_id,
                                              uint32_t nlen,
                                              const char *name)
 {
-   const enum virgl_renderer_capset capset_id =
-      ctx_flags & VIRGL_RENDERER_CONTEXT_FLAG_CAPSET_ID_MASK;
+   uint32_t capset_id = ctx_flags & VIRGL_RENDERER_CONTEXT_FLAG_CAPSET_ID_MASK;
    struct virgl_context *ctx;
    int ret;
 
@@ -224,18 +258,18 @@ int virgl_renderer_context_create_with_flags(uint32_t ctx_id,
    }
 
    switch (capset_id) {
-   case VIRGL_RENDERER_CAPSET_VIRGL:
-   case VIRGL_RENDERER_CAPSET_VIRGL2:
+   case VIRTGPU_DRM_CAPSET_VIRGL:
+   case VIRTGPU_DRM_CAPSET_VIRGL2:
       if (!state.vrend_initialized)
          return EINVAL;
       ctx = vrend_renderer_context_create(ctx_id, nlen, name);
       break;
-   case VIRGL_RENDERER_CAPSET_VENUS:
+   case VIRTGPU_DRM_CAPSET_VENUS:
       if (!state.proxy_initialized)
          return EINVAL;
       ctx = proxy_context_create(ctx_id, ctx_flags, nlen, name);
       break;
-   case VIRGL_RENDERER_CAPSET_DRM:
+   case VIRTGPU_DRM_CAPSET_DRM:
       if (!state.drm_initialized)
          return EINVAL;
       ctx = drm_renderer_create(nlen, name);
@@ -248,6 +282,7 @@ int virgl_renderer_context_create_with_flags(uint32_t ctx_id,
       return ENOMEM;
 
    ctx->ctx_id = ctx_id;
+   ctx->in_fence_fd = -1;
    ctx->capset_id = capset_id;
    ctx->fence_retire = per_context_fence_retire;
 
@@ -263,7 +298,7 @@ int virgl_renderer_context_create_with_flags(uint32_t ctx_id,
 int virgl_renderer_context_create(uint32_t handle, uint32_t nlen, const char *name)
 {
    return virgl_renderer_context_create_with_flags(handle,
-                                                   VIRGL_RENDERER_CAPSET_VIRGL2,
+                                                   VIRTGPU_DRM_CAPSET_VIRGL2,
                                                    nlen,
                                                    name);
 }
@@ -286,7 +321,10 @@ int virgl_renderer_submit_cmd(void *buffer,
    if (ndw < 0 || (unsigned)ndw > UINT32_MAX / sizeof(uint32_t))
       return EINVAL;
 
-   return ctx->submit_cmd(ctx, buffer, ndw * sizeof(uint32_t));
+   if (((uintptr_t)buffer & 3) != 0)
+      return EFAULT;
+
+   return ctx->submit_cmd(ctx, buffer, (uint32_t)ndw * sizeof(uint32_t));
 }
 
 int virgl_renderer_transfer_write_iov(uint32_t handle,
@@ -422,6 +460,7 @@ int virgl_renderer_context_create_fence(uint32_t ctx_id,
 
 void virgl_renderer_context_poll(uint32_t ctx_id)
 {
+   TRACE_FUNC();
    struct virgl_context *ctx = virgl_context_lookup(ctx_id);
    if (!ctx)
       return;
@@ -482,7 +521,7 @@ static int virgl_renderer_resource_get_info_common(int res_handle,
    info->fd = res->fd;
 
    if (!res->pipe_resource)
-	   return 0;
+      return 0;
 
    vrend_renderer_resource_get_info(res->pipe_resource,
                                     (struct vrend_renderer_resource_info *)info);
@@ -541,6 +580,25 @@ int virgl_renderer_resource_get_info_ext(int res_handle,
    return 0;
 }
 
+int virgl_renderer_borrow_texture_for_scanout(int res_handle,
+                                              struct virgl_renderer_resource_info_ext *info)
+{
+   TRACE_FUNC();
+   struct virgl_resource *res = virgl_resource_lookup(res_handle);
+
+   if (!res)
+      return EINVAL;
+   if (!info)
+      return EINVAL;
+
+   if (!res->pipe_resource)
+      return 0;
+
+   vrend_renderer_borrow_texture_for_scanout(res->pipe_resource);
+
+   return virgl_renderer_resource_get_info_ext(res_handle, info);
+}
+
 void virgl_renderer_get_cap_set(uint32_t cap_set, uint32_t *max_ver,
                                 uint32_t *max_size)
 {
@@ -548,15 +606,15 @@ void virgl_renderer_get_cap_set(uint32_t cap_set, uint32_t *max_ver,
 
    /* this may be called before virgl_renderer_init */
    switch (cap_set) {
-   case VIRGL_RENDERER_CAPSET_VIRGL:
-   case VIRGL_RENDERER_CAPSET_VIRGL2:
+   case VIRTGPU_DRM_CAPSET_VIRGL:
+   case VIRTGPU_DRM_CAPSET_VIRGL2:
       vrend_renderer_get_cap_set(cap_set, max_ver, max_size);
       break;
-   case VIRGL_RENDERER_CAPSET_VENUS:
+   case VIRTGPU_DRM_CAPSET_VENUS:
       *max_ver = 0;
       *max_size = proxy_get_capset(cap_set, NULL);
       break;
-   case VIRGL_RENDERER_CAPSET_DRM:
+   case VIRTGPU_DRM_CAPSET_DRM:
       *max_ver = 0;
       *max_size = drm_renderer_capset(NULL);
       break;
@@ -595,8 +653,9 @@ static virgl_renderer_gl_context create_gl_context(int scanout_idx, struct virgl
    if (state.winsys_initialized)
       return vrend_winsys_create_context(param);
 
-   vparam.version = 1;
+   vparam.version = 2;
    vparam.shared = param->shared;
+   vparam.compat_ctx = param->compat_ctx;
    vparam.major_ver = param->major_ver;
    vparam.minor_ver = param->minor_ver;
    return state.cbs->create_gl_context(state.cookie, scanout_idx, &vparam);
@@ -621,7 +680,7 @@ static int make_current(virgl_renderer_gl_context ctx)
 
    ret = state.cbs->make_current(state.cookie, 0, ctx);
    if (ret && state.cbs->version >= 4) {
-      vrend_printf("%s: Error switching context: %d\n", __func__, ret);
+      virgl_error("%s: Error switching context: %d\n", __func__, ret);
       assert(!ret && "Failed to switch GL context");
       return -1;
    }
@@ -636,10 +695,11 @@ static virgl_renderer_gl_context create_gl_context_surfaceless(int scanout_idx,
    if (state.winsys_initialized || state.external_winsys_initialized)
       return vrend_winsys_create_context(param);
 
-   vparam.version = 1;
+   vparam.version = 2;
    vparam.shared = param->shared;
    vparam.major_ver = param->major_ver;
    vparam.minor_ver = param->minor_ver;
+   vparam.compat_ctx = param->compat_ctx;
    return state.cbs->create_gl_context(state.cookie, scanout_idx, &vparam);
 }
 
@@ -662,8 +722,8 @@ static int make_current_surfaceless(virgl_renderer_gl_context ctx)
 
    ret = state.cbs->make_current(state.cookie, 0, ctx);
    if (ret && state.cbs->version >= 4) {
-      vrend_printf("%s: Error switching surfaceless context: %d\n",
-                   __func__, ret);
+      virgl_error("%s: Error switching surfaceless context: %d\n",
+                  __func__, ret);
       assert(!ret && "Failed to switch GL context");
       return -1;
    }
@@ -720,8 +780,8 @@ virgl_context_foreach_retire_fences(struct virgl_context *ctx,
                                     UNUSED void* data)
 {
    /* vrend contexts are polled explicitly by the caller */
-   if (ctx->capset_id != VIRGL_RENDERER_CAPSET_VIRGL &&
-       ctx->capset_id != VIRGL_RENDERER_CAPSET_VIRGL2 &&
+   if (ctx->capset_id != VIRTGPU_DRM_CAPSET_VIRGL &&
+       ctx->capset_id != VIRTGPU_DRM_CAPSET_VIRGL2 &&
        !(state.flags & VIRGL_RENDERER_ASYNC_FENCE_CB))
    {
       assert(ctx->retire_fences);
@@ -759,6 +819,9 @@ void virgl_renderer_cleanup(UNUSED void *cookie)
    if (state.vrend_initialized)
       vrend_renderer_fini();
 
+   if (state.fence_initialized)
+      virgl_fence_table_cleanup();
+
    if (state.winsys_initialized || state.external_winsys_initialized)
       vrend_winsys_cleanup();
 
@@ -784,14 +847,18 @@ int virgl_renderer_init(void *cookie, int flags, struct virgl_renderer_callbacks
 
    if (state.client_initialized && (state.cookie != cookie ||
                                     state.flags != flags ||
-                                    state.cbs != cbs))
+                                    state.cbs != cbs)) {
+      virgl_error("renderer already initialized");
       return -EBUSY;
+   }
 
    if (!state.client_initialized) {
       if (!cbs ||
           cbs->version < 1 ||
-          cbs->version > VIRGL_RENDERER_CALLBACKS_VERSION)
+          cbs->version > VIRGL_RENDERER_CALLBACKS_VERSION) {
+         virgl_error("invalid renderer callbacks");
          return -1;
+      }
 
       state.cookie = cookie;
       state.flags = flags;
@@ -805,15 +872,19 @@ int virgl_renderer_init(void *cookie, int flags, struct virgl_renderer_callbacks
          vrend_renderer_get_pipe_callbacks();
 
       ret = virgl_resource_table_init(pipe_cbs);
-      if (ret)
+      if (ret) {
+         virgl_error("failed to initialize virgl resources");
          goto fail;
+      }
       state.resource_initialized = true;
    }
 
    if (!state.context_initialized) {
       ret = virgl_context_table_init();
-      if (ret)
+      if (ret) {
+         virgl_error("failed to initialize virgl context");
          goto fail;
+      }
       state.context_initialized = true;
    }
 
@@ -830,6 +901,7 @@ int virgl_renderer_init(void *cookie, int flags, struct virgl_renderer_callbacks
       if (ret) {
          if (drm_fd >= 0)
             close(drm_fd);
+         virgl_error("failed to initialize vrend winsys");
          goto fail;
       }
       state.winsys_initialized = true;
@@ -841,6 +913,7 @@ int virgl_renderer_init(void *cookie, int flags, struct virgl_renderer_callbacks
 
       if (!cbs->create_gl_context || !cbs->destroy_gl_context ||
           !cbs->make_current) {
+         virgl_error("invalid renderer gl callbacks");
          ret = EINVAL;
          goto fail;
       }
@@ -848,12 +921,14 @@ int virgl_renderer_init(void *cookie, int flags, struct virgl_renderer_callbacks
       egl_display = state.cbs->get_egl_display(cookie);
 
       if (!egl_display) {
+         virgl_error("failed to get egl display");
          ret = -1;
          goto fail;
       }
       ret = vrend_winsys_init_external(egl_display);
 
       if (ret) {
+         virgl_error("failed to initialize vrend winsys");
          ret = -1;
          goto fail;
       }
@@ -865,6 +940,7 @@ int virgl_renderer_init(void *cookie, int flags, struct virgl_renderer_callbacks
       uint32_t renderer_flags = 0;
 
       if (!cookie || !cbs) {
+         virgl_error("invalid renderer vrend callbacks");
          ret = -1;
          goto fail;
       }
@@ -879,17 +955,25 @@ int virgl_renderer_init(void *cookie, int flags, struct virgl_renderer_callbacks
          renderer_flags |= VREND_USE_VIDEO;
       if (flags & VIRGL_RENDERER_D3D11_SHARE_TEXTURE)
          renderer_flags |= VREND_D3D11_SHARE_TEXTURE;
+      if (flags & VIRGL_RENDERER_COMPAT_PROFILE)
+         renderer_flags |= VREND_USE_COMPAT_CONTEXT;
+      if (flags & VIRGL_RENDERER_USE_GLES)
+         renderer_flags |= VREND_USE_GLES;
 
       ret = vrend_renderer_init(&vrend_cbs, renderer_flags);
-      if (ret)
+      if (ret) {
+         virgl_error("failed to initialize vrend renderer");
          goto fail;
+      }
       state.vrend_initialized = true;
    }
 
    if (!state.proxy_initialized && (flags & VIRGL_RENDERER_RENDER_SERVER)) {
       ret = proxy_renderer_init(&proxy_cbs, flags | VIRGL_RENDERER_NO_VIRGL);
-      if (ret)
+      if (ret) {
+         virgl_error("failed to initialize venus renderer");
          goto fail;
+      }
       state.proxy_initialized = true;
    }
 
@@ -901,11 +985,22 @@ int virgl_renderer_init(void *cookie, int flags, struct virgl_renderer_callbacks
          drm_fd = cbs->get_drm_fd(cookie);
 
       ret = drm_renderer_init(drm_fd);
-      if (ret)
+      if (ret) {
+         virgl_error("failed to initialize drm renderer");
          goto fail;
+      }
       state.drm_initialized = true;
    }
 
+   if (!state.fence_initialized) {
+      ret = virgl_fence_table_init();
+      if (ret) {
+         virgl_error("failed to initialize fence table");
+         goto fail;
+      }
+      state.fence_initialized = true;
+   }
+
    return 0;
 
 fail:
@@ -960,9 +1055,50 @@ int virgl_renderer_get_poll_fd(void)
    return -1;
 }
 
+static
+void virgl_null_logger(UNUSED const char *fmt, UNUSED va_list va)
+{
+}
+
+/* Compatibility layer for the virgl_set_debug_callback function */
+static inline void virgl_legacy_logger_wrapper(virgl_debug_callback_type cb,
+                                               const char *fmt,
+                                               ...)
+{
+   va_list va;
+   va_start(va, fmt);
+   cb(fmt, va);
+   va_end(va);
+}
+
+/* The logger need to be wrapped into a structure to be given as void* */
+struct virgl_legacy_logger_holder {
+   virgl_debug_callback_type logger;
+};
+
+static void virgl_legacy_logger(UNUSED enum virgl_log_level_flags log_level,
+                                const char *message,
+                                void* user_data)
+{
+   struct virgl_legacy_logger_holder *log_cb = user_data;
+   virgl_legacy_logger_wrapper(log_cb->logger, "%s", message);
+}
+
+static struct virgl_legacy_logger_holder legacy_logger = { virgl_null_logger };
+
 virgl_debug_callback_type virgl_set_debug_callback(virgl_debug_callback_type cb)
 {
-   return virgl_log_set_logger(cb);
+   virgl_debug_callback_type previous_cb = legacy_logger.logger;
+   legacy_logger.logger = cb;
+   virgl_log_set_handler(virgl_legacy_logger, &legacy_logger, NULL);
+   return previous_cb;
+}
+
+void virgl_set_log_callback(virgl_log_callback_type cb,
+                            void* user_data,
+                            virgl_free_data_callback_type free_user_data_cb)
+{
+   virgl_log_set_handler(cb, user_data, free_user_data_cb);
 }
 
 static int virgl_renderer_export_query(void *execute_args, uint32_t execute_size)
@@ -1114,19 +1250,15 @@ int virgl_renderer_resource_create_blob(const struct virgl_renderer_resource_cre
                                           args->iovecs,
                                           args->num_iovs,
                                           &blob.vulkan_info);
-      if (!res) {
-         close(blob.u.fd);
+      if (!res)
          return -ENOMEM;
-      }
    } else {
       res = virgl_resource_create_from_pipe(args->res_handle,
                                             blob.u.pipe_resource,
                                             args->iovecs,
                                             args->num_iovs);
-      if (!res) {
-         vrend_renderer_resource_destroy((struct vrend_resource *)blob.u.pipe_resource);
+      if (!res)
          return -ENOMEM;
-      }
    }
 
    res->map_info = blob.map_info;
@@ -1150,10 +1282,18 @@ int virgl_renderer_resource_map(uint32_t res_handle, void **out_map, uint64_t *o
       if (!ret)
          res->map_size = map_size;
    } else {
-      switch (res->fd_type) {
+      enum virgl_resource_fd_type fd_type = res->fd_type;
+      enum virgl_resource_fd_type export_fd_type = res->fd_type;
+      int fd = res->fd;
+
+      /* Create a transient dmabuf. */
+      if (fd_type == VIRGL_RESOURCE_OPAQUE_HANDLE)
+         export_fd_type = virgl_resource_export_fd(res, &fd);
+
+      switch (export_fd_type) {
       case VIRGL_RESOURCE_FD_DMABUF:
       case VIRGL_RESOURCE_FD_SHM:
-         map = mmap(NULL, res->map_size, PROT_WRITE | PROT_READ, MAP_SHARED, res->fd, 0);
+         map = mmap(NULL, res->map_size, PROT_WRITE | PROT_READ, MAP_SHARED, fd, 0);
          map_size = res->map_size;
          break;
       case VIRGL_RESOURCE_FD_OPAQUE:
@@ -1166,6 +1306,9 @@ int virgl_renderer_resource_map(uint32_t res_handle, void **out_map, uint64_t *o
           */
          break;
       }
+
+      if (export_fd_type != fd_type)
+         close(fd);
    }
 
    if (!map || map == MAP_FAILED)
@@ -1191,12 +1334,12 @@ int virgl_renderer_resource_unmap(uint32_t res_handle)
       switch (res->fd_type) {
       case VIRGL_RESOURCE_FD_DMABUF:
       case VIRGL_RESOURCE_FD_SHM:
+      case VIRGL_RESOURCE_OPAQUE_HANDLE:
          ret = munmap(res->mapped, res->map_size);
          break;
       case VIRGL_RESOURCE_FD_OPAQUE:
          ret = vkr_allocator_resource_unmap(res);
          break;
-      case VIRGL_RESOURCE_OPAQUE_HANDLE:
       case VIRGL_RESOURCE_FD_INVALID:
          /* Avoid a default case so that -Wswitch will tell us at compile time
           * if a new virgl resource type is added without being handled here.
@@ -1313,8 +1456,134 @@ virgl_renderer_resource_import_blob(const struct virgl_renderer_resource_import_
 }
 
 int
-virgl_renderer_export_fence(uint32_t client_fence_id, int *fd)
+virgl_renderer_export_fence(uint64_t client_fence_id, int *fd)
+{
+   TRACE_FUNC();
+
+   /* transfers FD ownership to caller */
+   *fd = virgl_fence_get_fd(client_fence_id);
+   if (*fd >= 0)
+      return 0;
+
+   return -EINVAL;
+}
+
+int virgl_renderer_export_signalled_fence(void)
+{
+   TRACE_FUNC();
+
+   /* transfers FD ownership to caller, returns -1 on failure */
+   return virgl_fence_get_last_signalled_fence_fd();
+}
+
+static int attach_in_fence_fd(struct virgl_context *ctx, int fence_fd)
+{
+   int ret = -EINVAL;
+
+#ifndef WIN32
+   ret = sync_accumulate("virglrenderer", &ctx->in_fence_fd, fence_fd);
+#endif
+   close(fence_fd);
+
+   return ret;
+}
+
+/* Special entrypoint for vtest, which has received a real fence fd,
+ * not a fence-id
+ */
+int virgl_renderer_attach_fence(int ctx_id, int fence_fd)
 {
    TRACE_FUNC();
-   return vrend_renderer_export_ctx0_fence(client_fence_id, fd);
+   struct virgl_context *ctx = virgl_context_lookup(ctx_id);
+   if (!ctx)
+      return EINVAL;
+
+   return attach_in_fence_fd(ctx, fence_fd);
+}
+
+int virgl_renderer_get_fence_fd(uint64_t fence_id)
+{
+   return virgl_fence_get_fd(fence_id);
+}
+
+static int virgl_renderer_context_attach_in_fence(struct virgl_context *ctx,
+                                                  uint64_t fence_id)
+{
+   int ret;
+
+   /*
+    * FD will be -1 in two cases:
+    *
+    *    1. Fence was signalled and retired.
+    *    2. Fence ID is invalid. Virglrenderer doesn't take responsibility
+    *       for handling invalid fences and assumes that all supplied fence
+    *       IDs are always valid. It's caller's responsibility to validate
+    *       fence IDs.
+    */
+   int fd = virgl_fence_get_fd(fence_id);
+   if (fd < 0)
+      return 0;
+
+   ret = attach_in_fence_fd(ctx, fd);
+   if (ret)
+      virgl_error("%s: sync_accumulate failed for fence_id=%" PRIu64 " err=%d\n",
+                  __func__, fence_id, ret);
+
+   return ret;
+}
+
+static int virgl_renderer_context_attach_in_fences(struct virgl_context *ctx,
+                                                   uint64_t *fence_ids,
+                                                   uint32_t num_fences)
+{
+   TRACE_FUNC();
+
+   if (!ctx->supports_fence_sharing)
+      return -EINVAL;
+
+   for (uint32_t i = 0; i < num_fences; i++) {
+      int ret = virgl_renderer_context_attach_in_fence(ctx, fence_ids[i]);
+      if (ret)
+         return ret;
+   }
+
+   return 0;
+}
+
+int virgl_renderer_submit_cmd2(void *buffer,
+                               int ctx_id,
+                               int ndw,
+                               uint64_t *in_fence_ids,
+                               uint32_t num_in_fences)
+{
+   TRACE_FUNC();
+   struct virgl_context *ctx = virgl_context_lookup(ctx_id);
+   if (!ctx)
+      return EINVAL;
+
+   if (((uintptr_t)buffer & 3) != 0)
+      return EFAULT;
+
+   if (ndw < 0 || (unsigned)ndw > UINT32_MAX / sizeof(uint32_t))
+      return EINVAL;
+
+   if (num_in_fences) {
+      int err = virgl_renderer_context_attach_in_fences(ctx, in_fence_ids, num_in_fences);
+      if (err)
+         return err;
+   }
+
+   return ctx->submit_cmd(ctx, buffer, (uint32_t)ndw * sizeof(uint32_t));
+}
+
+int virgl_renderer_get_dev_fd(int ctx_id)
+{
+   struct virgl_context *ctx = virgl_context_lookup(ctx_id);
+   if (!ctx)
+      return -EINVAL;
+
+   if (!ctx->get_device_fd)
+      return -ENODEV;
+
+   return ctx->get_device_fd(ctx);
 }
diff --git a/src/virglrenderer.h b/src/virglrenderer.h
index 3687b57..6df9269 100644
--- a/src/virglrenderer.h
+++ b/src/virglrenderer.h
@@ -31,6 +31,8 @@
 #include <stdbool.h>
 #include <stdarg.h>
 
+#include "virgl-version.h"
+
 struct virgl_box;
 struct iovec;
 
@@ -43,6 +45,7 @@ struct virgl_renderer_gl_ctx_param {
    bool shared;
    int major_ver;
    int minor_ver;
+   int compat_ctx;
 };
 
 #define VIRGL_RENDERER_CALLBACKS_VERSION 4
@@ -161,6 +164,10 @@ struct virgl_renderer_callbacks {
 
 
 #define VIRGL_RENDERER_D3D11_SHARE_TEXTURE (1 << 12)
+#define VIRGL_RENDERER_COMPAT_PROFILE (1 << 13)
+
+/* Blob allocations must be done by guest from dedicated heap (Host visible memory). */
+#define VIRGL_RENDERER_USE_GUEST_VRAM (1 << 14)
 
 VIRGL_EXPORT int virgl_renderer_init(void *cookie, int flags, struct virgl_renderer_callbacks *cb);
 VIRGL_EXPORT void virgl_renderer_poll(void); /* force fences */
@@ -252,6 +259,22 @@ struct virgl_renderer_supported_structures {
 /* This typedef must be kept in sync with vrend_debug.h */
 typedef void (*virgl_debug_callback_type)(const char *fmt, va_list ap);
 
+enum virgl_log_level_flags {
+   VIRGL_LOG_LEVEL_DEBUG,
+   VIRGL_LOG_LEVEL_INFO,
+   VIRGL_LOG_LEVEL_WARNING,
+   VIRGL_LOG_LEVEL_ERROR,
+
+   /* "SILENT" must be enum with the highest absolute value and it should not
+    * be used as actual log level in calls to virgl_logv and siblings .*/
+   VIRGL_LOG_LEVEL_SILENT,
+};
+
+typedef void (*virgl_free_data_callback_type)(void* user_data);
+typedef void (*virgl_log_callback_type) (enum virgl_log_level_flags log_level,
+                                         const char *message,
+                                         void* user_data);
+
 VIRGL_EXPORT int virgl_renderer_resource_create(struct virgl_renderer_resource_create_args *args, struct iovec *iov, uint32_t num_iovs);
 VIRGL_EXPORT int virgl_renderer_resource_import_eglimage(struct virgl_renderer_resource_create_args *args, void *image);
 VIRGL_EXPORT void virgl_renderer_resource_unref(uint32_t res_handle);
@@ -262,6 +285,22 @@ VIRGL_EXPORT void *virgl_renderer_resource_get_priv(uint32_t res_handle);
 VIRGL_EXPORT int virgl_renderer_context_create(uint32_t handle, uint32_t nlen, const char *name);
 VIRGL_EXPORT void virgl_renderer_context_destroy(uint32_t handle);
 
+/* Submit a command buffer for execution.  ctx_id is the context ID.
+ * ndw is the length of the buffer in 4-byte words.
+ *
+ * The buffer must be at least 4-byte aligned.  Starting in 1.0.2, this
+ * is checked and violations result in EFAULT being returned.  In 1.0.1
+ * and below, a misaligned buffer caused undefined behavior.
+ *
+ * Some renderers require that the buffer is 8-byte aligned.  These
+ * renderers deal with less-aligned buffers by copying the input data.
+ * You can avoid the copy by passing a sufficiently-aligned buffer.
+ *
+ * This function will never mutate the buffer, and is secure against
+ * malicious buffer contents.  However, it is _not_ secure against
+ * concurrent modification of the buffer by other threads while it
+ * is running.
+ */
 VIRGL_EXPORT int virgl_renderer_submit_cmd(void *buffer,
                                            int ctx_id,
                                            int ndw);
@@ -300,7 +339,14 @@ VIRGL_EXPORT void virgl_renderer_force_ctx_0(void);
 VIRGL_EXPORT void virgl_renderer_ctx_attach_resource(int ctx_id, int res_handle);
 VIRGL_EXPORT void virgl_renderer_ctx_detach_resource(int ctx_id, int res_handle);
 
+/* This API is deprecated, use virgl_set_log_callback instead */
 VIRGL_EXPORT virgl_debug_callback_type virgl_set_debug_callback(virgl_debug_callback_type cb);
+/* Redirects all the logs to the callback, the callback will be called with the given
+ * user_data, free_user_data_cb will be called if the callback is replaced or if
+ * the program ends to free user_data */
+VIRGL_EXPORT void virgl_set_log_callback(virgl_log_callback_type cb,
+                                         void* user_data,
+                                         virgl_free_data_callback_type free_user_data_cb);
 
 /* return information about a resource */
 
@@ -334,6 +380,9 @@ VIRGL_EXPORT int virgl_renderer_resource_get_info(int res_handle,
 VIRGL_EXPORT int virgl_renderer_resource_get_info_ext(int res_handle,
                                                       struct virgl_renderer_resource_info_ext *info);
 
+VIRGL_EXPORT int virgl_renderer_borrow_texture_for_scanout(int res_handle,
+                                                           struct virgl_renderer_resource_info_ext *info);
+
 VIRGL_EXPORT void virgl_renderer_cleanup(void *cookie);
 
 /* reset the rendererer - destroy all contexts and resource */
@@ -411,6 +460,9 @@ VIRGL_EXPORT int virgl_renderer_context_create_fence(uint32_t ctx_id,
                                                      uint32_t ring_idx,
                                                      uint64_t fence_id);
 
+VIRGL_EXPORT void virgl_renderer_context_poll(uint32_t ctx_id); /* force fences */
+VIRGL_EXPORT int virgl_renderer_context_get_poll_fd(uint32_t ctx_id);
+
 /*
  * These are unstable APIs for development only. Use these for development/testing purposes
  * only, not in production
@@ -418,10 +470,41 @@ VIRGL_EXPORT int virgl_renderer_context_create_fence(uint32_t ctx_id,
 #ifdef VIRGL_RENDERER_UNSTABLE_APIS
 
 VIRGL_EXPORT int
-virgl_renderer_export_fence(uint32_t client_fence_id, int *fd);
+virgl_renderer_export_fence(uint64_t client_fence_id, int *fd);
 
-VIRGL_EXPORT void virgl_renderer_context_poll(uint32_t ctx_id); /* force fences */
-VIRGL_EXPORT int virgl_renderer_context_get_poll_fd(uint32_t ctx_id);
+VIRGL_EXPORT int
+virgl_renderer_export_signalled_fence(void);
+
+/* Submit a command buffer for execution.  ctx_id is the context ID.
+ * ndw is the length of the buffer in 4-byte words.
+ *
+ * The buffer must be at least 4-byte aligned.  Starting in 1.0.2, this
+ * is checked and violations result in EFAULT being returned.  In 1.0.1
+ * and below, a misaligned buffer caused undefined behavior.
+ *
+ * Some renderers require that the buffer is 8-byte aligned.  These
+ * renderers deal with less-aligned buffers by copying the input data.
+ * You can avoid the copy by passing a sufficiently-aligned buffer.
+ *
+ * This function will never mutate the buffer, and is secure against
+ * malicious buffer contents.  However, it is _not_ secure against
+ * concurrent modification of the buffer by other threads while it
+ * is running.
+ *
+ * Unlike virgl_renderer_submitt_cmd(), this function allows passing
+ * fences for explicit synchronization.
+ */
+VIRGL_EXPORT int
+virgl_renderer_submit_cmd2(void *buffer,
+                           int ctx_id,
+                           int ndw,
+                           uint64_t *in_fence_ids,
+                           uint32_t num_in_fences);
+
+/* vtest semi-private APIs: */
+VIRGL_EXPORT int virgl_renderer_attach_fence(int ctx_id, int fence_fd);
+VIRGL_EXPORT int virgl_renderer_get_fence_fd(uint64_t fence_id);
+VIRGL_EXPORT int virgl_renderer_get_dev_fd(int ctx_id);
 
 #endif /* VIRGL_RENDERER_UNSTABLE_APIS */
 
diff --git a/src/vrend_blitter.h b/src/vrend_blitter.h
index 91e2435..7a03571 100644
--- a/src/vrend_blitter.h
+++ b/src/vrend_blitter.h
@@ -35,6 +35,12 @@
    "%s"                                         \
 
 #define FS_HEADER_GLES                             \
+   "#version 300 es\n"                          \
+   "// Blitter\n"                               \
+   "%s"                                         \
+   "precision mediump float;\n"                 \
+
+#define FS_HEADER_GLES_MS                       \
    "#version 310 es\n"                          \
    "// Blitter\n"                               \
    "%s"                                         \
@@ -52,6 +58,11 @@
    "// Blitter\n"                               \
 
 #define HEADER_GLES                             \
+   "#version 300 es\n"                          \
+   "// Blitter\n"                               \
+   "precision mediump float;\n"                 \
+
+#define HEADER_GLES_MS                          \
    "#version 310 es\n"                          \
    "// Blitter\n"                               \
    "precision mediump float;\n"                 \
@@ -145,7 +156,7 @@
    "}\n"
 
 #define FS_TEXFETCH_COL_MSAA_GL FS_HEADER_GL FS_TEXFETCH_COL_MSAA_BODY
-#define FS_TEXFETCH_COL_MSAA_GLES FS_HEADER_GLES FS_TEXFETCH_COL_MSAA_BODY
+#define FS_TEXFETCH_COL_MSAA_GLES FS_HEADER_GLES_MS FS_TEXFETCH_COL_MSAA_BODY
 #define FS_TEXFETCH_COL_MSAA_ARRAY_GLES FS_HEADER_GLES_MS_ARRAY FS_TEXFETCH_COL_MSAA_BODY
 
 #define FS_TEXFETCH_DS_BODY                             \
@@ -178,7 +189,7 @@ struct vrend_context;
 struct vrend_resource;
 struct vrend_blit_info;
 #define FS_TEXFETCH_DS_MSAA_GL HEADER_GL FS_TEXFETCH_DS_MSAA_BODY
-#define FS_TEXFETCH_DS_MSAA_GLES HEADER_GLES FS_TEXFETCH_DS_MSAA_BODY_GLES
+#define FS_TEXFETCH_DS_MSAA_GLES HEADER_GLES_MS FS_TEXFETCH_DS_MSAA_BODY_GLES
 #define FS_TEXFETCH_DS_MSAA_ARRAY_GLES HEADER_GLES_MS_ARRAY FS_TEXFETCH_DS_MSAA_BODY_GLES
 
 /* implement blitting using OpenGL. */
diff --git a/src/vrend_decode.c b/src/vrend_decode.c
index 3ab52f9..657f374 100644
--- a/src/vrend_decode.c
+++ b/src/vrend_decode.c
@@ -45,6 +45,9 @@
 #include "vrend_video.h"
 #endif
 
+#define XXH_INLINE_ALL
+#include "util/xxhash.h"
+
 /* decode side */
 #define DECODE_MAX_TOKENS 8000
 
@@ -69,7 +72,7 @@ static int vrend_decode_create_shader(struct vrend_context *ctx,
                                       uint16_t length)
 {
    struct pipe_stream_output_info so_info;
-   uint i;
+   unsigned i;
    int ret;
    uint32_t shader_offset, req_local_mem = 0;
    unsigned num_tokens, num_so_outputs, offlen;
@@ -80,6 +83,10 @@ static int vrend_decode_create_shader(struct vrend_context *ctx,
       return EINVAL;
 
    type = get_buf_entry(buf, VIRGL_OBJ_SHADER_TYPE);
+
+   if (type >= PIPE_SHADER_TYPES)
+      return EINVAL;
+
    num_tokens = get_buf_entry(buf, VIRGL_OBJ_SHADER_NUM_TOKENS);
    offlen = get_buf_entry(buf, VIRGL_OBJ_SHADER_OFFSET);
 
@@ -221,6 +228,13 @@ static int vrend_decode_clear_texture(struct vrend_context *ctx, const uint32_t
       return EINVAL;
 
    handle = get_buf_entry(buf, VIRGL_TEXTURE_HANDLE);
+
+   struct vrend_resource *res = vrend_renderer_ctx_res_lookup(ctx, handle);
+   if (!res || !res->gl_id) {
+      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, handle);
+      return EINVAL;
+   }
+
    level = get_buf_entry(buf, VIRGL_TEXTURE_LEVEL);
    box.x = get_buf_entry(buf, VIRGL_TEXTURE_SRC_X);
    box.y = get_buf_entry(buf, VIRGL_TEXTURE_SRC_Y);
@@ -233,13 +247,43 @@ static int vrend_decode_clear_texture(struct vrend_context *ctx, const uint32_t
    arr[2] = get_buf_entry(buf, VIRGL_TEXTURE_ARRAY_C);
    arr[3] = get_buf_entry(buf, VIRGL_TEXTURE_ARRAY_D);
 
-   return vrend_clear_texture(ctx, handle, level, &box, (void *) &arr);
+   return vrend_clear_texture(ctx, res, level, &box, (void *) &arr);
+}
+
+static int vrend_decode_clear_surface(struct vrend_context *ctx,
+                                      const uint32_t *buf, uint32_t length) {
+   union pipe_color_union color;
+   unsigned buffers, dstx, dsty, width, height;
+   uint32_t s0, surf_handle;
+   bool render_condition_enable;
+   int i;
+
+   if (length != VIRGL_CLEAR_SURFACE_SIZE)
+      return EINVAL;
+
+   s0 = get_buf_entry(buf, VIRGL_CLEAR_SURFACE_S0);
+   render_condition_enable = s0 & 0x1;
+   buffers = (s0 >> 1) & 0x7;
+
+   surf_handle = get_buf_entry(buf, VIRGL_CLEAR_SURFACE_HANDLE);
+
+   for (i = 0; i < 4; i++)
+      color.ui[i] = get_buf_entry(buf, VIRGL_CLEAR_SURFACE_COLOR_0 + i);
+
+   dstx = get_buf_entry(buf, VIRGL_CLEAR_SURFACE_DST_X);
+   dsty = get_buf_entry(buf, VIRGL_CLEAR_SURFACE_DST_Y);
+   width = get_buf_entry(buf, VIRGL_CLEAR_SURFACE_WIDTH);
+   height = get_buf_entry(buf, VIRGL_CLEAR_SURFACE_HEIGHT);
+
+   vrend_clear_surface(ctx, surf_handle, buffers, &color, dstx, dsty, width,
+                       height, render_condition_enable);
+   return 0;
 }
 
 static int vrend_decode_set_viewport_state(struct vrend_context *ctx, const uint32_t *buf, uint32_t length)
 {
    struct pipe_viewport_state vps[PIPE_MAX_VIEWPORTS];
-   uint i, v;
+   unsigned i, v;
    uint32_t num_viewports, start_slot;
    if (length < 1)
       return EINVAL;
@@ -280,6 +324,7 @@ static int vrend_decode_set_constant_buffer(struct vrend_context *ctx, const uin
 {
    uint32_t shader;
    int nc = (length - 2);
+   const float *data = NULL;
 
    if (length < 2)
       return EINVAL;
@@ -290,7 +335,10 @@ static int vrend_decode_set_constant_buffer(struct vrend_context *ctx, const uin
    if (shader >= PIPE_SHADER_TYPES)
       return EINVAL;
 
-   vrend_set_constants(ctx, shader, nc, get_buf_ptr(buf, VIRGL_SET_CONSTANT_BUFFER_DATA_START));
+   if (length > 2)
+      data = get_buf_ptr(buf, VIRGL_SET_CONSTANT_BUFFER_DATA_START);
+
+   vrend_set_constants(ctx, shader, nc, data);
    return 0;
 }
 
@@ -601,15 +649,36 @@ static int vrend_decode_create_rasterizer(struct vrend_context *ctx, const uint3
 
 static int vrend_decode_create_surface_common(struct vrend_context *ctx, const uint32_t *buf, uint32_t handle, uint32_t sample_count)
 {
-   uint32_t res_handle, format, val0, val1;
+   uint32_t res_handle = get_buf_entry(buf, VIRGL_OBJ_SURFACE_RES_HANDLE);
+
+   struct vrend_resource *res = vrend_renderer_ctx_res_lookup(ctx, res_handle);
+   if (!res || !res->gl_id) {
+      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, res_handle);
+      return EINVAL;
+   }
+
+   enum virgl_formats format = get_buf_entry(buf, VIRGL_OBJ_SURFACE_FORMAT);
+
+   if (format >= PIPE_FORMAT_COUNT) {
+      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_FORMAT, format);
+      return EINVAL;
+   }
 
-   res_handle = get_buf_entry(buf, VIRGL_OBJ_SURFACE_RES_HANDLE);
-   format = get_buf_entry(buf, VIRGL_OBJ_SURFACE_FORMAT);
    /* decide later if these are texture or buffer */
-   val0 = get_buf_entry(buf, VIRGL_OBJ_SURFACE_BUFFER_FIRST_ELEMENT);
-   val1 = get_buf_entry(buf, VIRGL_OBJ_SURFACE_BUFFER_LAST_ELEMENT);
+   uint32_t val0 = get_buf_entry(buf, VIRGL_OBJ_SURFACE_BUFFER_FIRST_ELEMENT);
+   uint32_t val1 = get_buf_entry(buf, VIRGL_OBJ_SURFACE_BUFFER_LAST_ELEMENT);
+
+   int first_layer = val1 & 0xffff;
+   int last_layer = (val1 >> 16) & 0xffff;
+
+   int num_layers = last_layer - first_layer + 1;
 
-   return vrend_create_surface(ctx, handle, res_handle, format, val0, val1, sample_count);
+   if (num_layers <= 0) {
+      virgl_error("%s: Invalid number of layers (%d) requested\n", __func__, num_layers);
+      return EINVAL;
+   }
+
+   return vrend_create_surface(ctx, handle, res, format, val0, first_layer, last_layer, sample_count);
 }
 
 static int vrend_decode_create_surface(struct vrend_context *ctx, const uint32_t *buf, uint32_t handle, uint16_t length)
@@ -631,17 +700,36 @@ static int vrend_decode_create_msaa_surface(struct vrend_context *ctx, const uin
 
 static int vrend_decode_create_sampler_view(struct vrend_context *ctx, const uint32_t *buf, uint32_t handle, uint16_t length)
 {
-   uint32_t res_handle, format, val0, val1, swizzle_packed;
-
    if (length != VIRGL_OBJ_SAMPLER_VIEW_SIZE)
       return EINVAL;
 
-   res_handle = get_buf_entry(buf, VIRGL_OBJ_SAMPLER_VIEW_RES_HANDLE);
-   format = get_buf_entry(buf, VIRGL_OBJ_SAMPLER_VIEW_FORMAT);
-   val0 = get_buf_entry(buf, VIRGL_OBJ_SAMPLER_VIEW_BUFFER_FIRST_ELEMENT);
-   val1 = get_buf_entry(buf, VIRGL_OBJ_SAMPLER_VIEW_BUFFER_LAST_ELEMENT);
-   swizzle_packed = get_buf_entry(buf, VIRGL_OBJ_SAMPLER_VIEW_SWIZZLE);
-   return vrend_create_sampler_view(ctx, handle, res_handle, format, val0, val1,swizzle_packed);
+   uint32_t res_handle = get_buf_entry(buf, VIRGL_OBJ_SAMPLER_VIEW_RES_HANDLE);
+   uint32_t format_data = get_buf_entry(buf, VIRGL_OBJ_SAMPLER_VIEW_FORMAT);
+
+   enum virgl_formats format = format_data & 0xffffff;
+   if (!format || format >= VIRGL_FORMAT_MAX) {
+      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_FORMAT, format_data);
+      return EINVAL;
+   }
+
+   enum pipe_texture_target pipe_target = (format_data >> 24) & 0xff;
+   if (pipe_target >= PIPE_MAX_TEXTURE_TYPES) {
+      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_SAMPLER_VIEW_TARGET,
+                                 format_data);
+      return EINVAL;
+   }
+
+   struct vrend_resource *res = vrend_renderer_ctx_res_lookup(ctx, res_handle);
+   if (!res || !res->gl_id) {
+      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, res_handle);
+      return EINVAL;
+   }
+
+   uint32_t val0 = get_buf_entry(buf, VIRGL_OBJ_SAMPLER_VIEW_BUFFER_FIRST_ELEMENT);
+   uint32_t val1 = get_buf_entry(buf, VIRGL_OBJ_SAMPLER_VIEW_BUFFER_LAST_ELEMENT);
+
+   uint32_t swizzle_packed = get_buf_entry(buf, VIRGL_OBJ_SAMPLER_VIEW_SWIZZLE);
+   return vrend_create_sampler_view(ctx, handle, res, format, pipe_target, val0, val1, swizzle_packed);
 }
 
 static int vrend_decode_create_sampler_state(struct vrend_context *ctx, const uint32_t *buf, uint32_t handle, uint16_t length)
@@ -656,14 +744,13 @@ static int vrend_decode_create_sampler_state(struct vrend_context *ctx, const ui
    state.wrap_s = tmp & 0x7;
    state.wrap_t = (tmp >> 3) & 0x7;
    state.wrap_r = (tmp >> 6) & 0x7;
-   state.min_img_filter = (tmp >> 9) & 0x3;
+   state.min_img_filter = (tmp >> 9) & 0x1;
    state.min_mip_filter = (tmp >> 11) & 0x3;
-   state.mag_img_filter = (tmp >> 13) & 0x3;
+   state.mag_img_filter = (tmp >> 13) & 0x1;
    state.compare_mode = (tmp >> 15) & 0x1;
    state.compare_func = (tmp >> 16) & 0x7;
    state.seamless_cube_map = (tmp >> 19) & 0x1;
-   state.max_anisotropy = (float)((tmp >> 20) & 0x3f);
-   state.normalized_coords = 0;
+   state.max_anisotropy = (tmp >> 20) & 0x1f;
 
    state.lod_bias = uif(get_buf_entry(buf, VIRGL_OBJ_SAMPLER_STATE_LOD_BIAS));
    state.min_lod = uif(get_buf_entry(buf, VIRGL_OBJ_SAMPLER_STATE_MIN_LOD));
@@ -695,6 +782,9 @@ static int vrend_decode_create_ve(struct vrend_context *ctx, const uint32_t *buf
 
    num_elements = (length - 1) / 4;
 
+   if (num_elements > PIPE_MAX_ATTRIBS)
+      return EINVAL;
+
    if (num_elements) {
       ve = calloc(num_elements, sizeof(struct pipe_vertex_element));
 
@@ -798,6 +888,17 @@ static int vrend_decode_create_object(struct vrend_context *ctx, const uint32_t
       return EINVAL;
    }
 
+   /* Surface GL errors with object metadata to pinpoint bad creations. */
+   if (!vrend_check_no_error(ctx) && ret == 0) {
+      virgl_error("GL error during CREATE_OBJECT type=%s handle=0x%x len=%u\n",
+                  vrend_get_object_type_name(obj_type), handle, length);
+      /* Dump a small slice of the payload for quick diagnosis. */
+      for (uint32_t i = 0; i < length && i < 12; i++) {
+         virgl_error("  dword[%u]=0x%x\n", i, buf[i]);
+      }
+      ret = EINVAL;
+   }
+
    return ret;
 }
 
@@ -843,7 +944,7 @@ static int vrend_decode_destroy_object(struct vrend_context *ctx, const uint32_t
 
    VREND_DEBUG_EXT(dbg_object, ctx,
                uint32_t obj = (get_buf_entry(buf, 0) >> 8) & 0xFF;
-               vrend_printf("  DESTROY %-17s handle:0x%x\n",
+               virgl_debug("  DESTROY %-17s handle:0x%x\n",
                        vrend_get_object_type_name(obj), handle));
 
    vrend_renderer_object_destroy(ctx, handle);
@@ -881,31 +982,33 @@ static int vrend_decode_set_blend_color(struct vrend_context *ctx, const uint32_
 
 static int vrend_decode_set_scissor_state(struct vrend_context *ctx, const uint32_t *buf, uint32_t length)
 {
-   struct pipe_scissor_state ss[PIPE_MAX_VIEWPORTS];
-   uint32_t temp;
-   int32_t num_scissor;
-   uint32_t start_slot;
-   int s;
    if (length < 1)
       return EINVAL;
 
    if ((length - 1) % 2)
       return EINVAL;
 
-   num_scissor = (length - 1) / 2;
+   const int32_t num_scissor = (length - 1) / 2;
    if (num_scissor > PIPE_MAX_VIEWPORTS)
       return EINVAL;
 
-   start_slot = get_buf_entry(buf, VIRGL_SET_SCISSOR_START_SLOT);
+   const uint32_t start_slot = get_buf_entry(buf, VIRGL_SET_SCISSOR_START_SLOT);
 
-   for (s = 0; s < num_scissor; s++) {
-      temp = get_buf_entry(buf, VIRGL_SET_SCISSOR_MINX_MINY(s));
-      ss[s].minx = temp & 0xffff;
-      ss[s].miny = (temp >> 16) & 0xffff;
+   if (start_slot >= PIPE_MAX_VIEWPORTS ||
+       start_slot + num_scissor > PIPE_MAX_VIEWPORTS) {
+      vrend_report_buffer_error(ctx, 0);
+      return EINVAL;
+   }
 
-      temp = get_buf_entry(buf, VIRGL_SET_SCISSOR_MAXX_MAXY(s));
-      ss[s].maxx = temp & 0xffff;
-      ss[s].maxy = (temp >> 16) & 0xffff;
+   struct pipe_scissor_state ss[PIPE_MAX_VIEWPORTS];
+   for (int s = 0; s < num_scissor; s++) {
+      const uint32_t minxy = get_buf_entry(buf, VIRGL_SET_SCISSOR_MINX_MINY(s));
+      ss[s].minx = minxy & 0xffff;
+      ss[s].miny = (minxy >> 16) & 0xffff;
+
+      const uint32_t maxxy = get_buf_entry(buf, VIRGL_SET_SCISSOR_MAXX_MAXY(s));
+      ss[s].maxx = maxxy & 0xffff;
+      ss[s].maxy = (maxxy >> 16) & 0xffff;
    }
 
    vrend_set_scissor_state(ctx, start_slot, num_scissor, ss);
@@ -1041,6 +1144,7 @@ static int vrend_decode_blit(struct vrend_context *ctx, const uint32_t *buf, uin
 
 static int vrend_decode_bind_sampler_states(struct vrend_context *ctx, const uint32_t *buf, uint32_t length)
 {
+   const uint32_t *handles = NULL;
    if (length < 2)
       return EINVAL;
 
@@ -1051,8 +1155,10 @@ static int vrend_decode_bind_sampler_states(struct vrend_context *ctx, const uin
    if (shader_type >= PIPE_SHADER_TYPES)
       return EINVAL;
 
-   vrend_bind_sampler_states(ctx, shader_type, start_slot, num_states,
-                             get_buf_ptr(buf, VIRGL_BIND_SAMPLER_STATES_S0_HANDLE));
+   if (num_states > 0)
+      handles = get_buf_ptr(buf, VIRGL_BIND_SAMPLER_STATES_S0_HANDLE);
+
+   vrend_bind_sampler_states(ctx, shader_type, start_slot, num_states, handles);
    return 0;
 }
 
@@ -1084,8 +1190,7 @@ static int vrend_decode_get_query_result(struct vrend_context *ctx, const uint32
    uint32_t handle = get_buf_entry(buf, VIRGL_QUERY_RESULT_HANDLE);
    uint32_t wait = get_buf_entry(buf, VIRGL_QUERY_RESULT_WAIT);
 
-   vrend_get_query_result(ctx, handle, wait);
-   return 0;
+   return vrend_get_query_result(ctx, handle, wait);
 }
 
 static int vrend_decode_get_query_result_qbo(struct vrend_context *ctx, const uint32_t *buf, uint32_t length)
@@ -1100,8 +1205,7 @@ static int vrend_decode_get_query_result_qbo(struct vrend_context *ctx, const ui
    uint32_t offset = get_buf_entry(buf, VIRGL_QUERY_RESULT_QBO_OFFSET);
    int32_t index = get_buf_entry(buf, VIRGL_QUERY_RESULT_QBO_INDEX);
 
-   vrend_get_query_result_qbo(ctx, handle, qbo_handle, wait, result_type, offset, index);
-   return 0;
+   return vrend_get_query_result_qbo(ctx, handle, qbo_handle, wait, result_type, offset, index);
 }
 
 static int vrend_decode_set_render_condition(struct vrend_context *ctx, const uint32_t *buf, uint32_t length)
@@ -1111,7 +1215,7 @@ static int vrend_decode_set_render_condition(struct vrend_context *ctx, const ui
 
    uint32_t handle = get_buf_entry(buf, VIRGL_RENDER_CONDITION_HANDLE);
    bool condition = get_buf_entry(buf, VIRGL_RENDER_CONDITION_CONDITION) & 1;
-   uint mode = get_buf_entry(buf, VIRGL_RENDER_CONDITION_MODE);
+   uint32_t mode = get_buf_entry(buf, VIRGL_RENDER_CONDITION_MODE);
 
    vrend_render_condition(ctx, handle, condition, mode);
    return 0;
@@ -1181,7 +1285,7 @@ static int vrend_decode_bind_shader(struct vrend_context *ctx, const uint32_t *b
 }
 
 static int vrend_decode_set_tess_state(struct vrend_context *ctx,
-				       const uint32_t *buf, uint32_t length)
+                                       const uint32_t *buf, uint32_t length)
 {
    float tess_factors[6];
    int i;
@@ -1282,8 +1386,10 @@ static int vrend_decode_set_shader_images(struct vrend_context *ctx, const uint3
       uint32_t layer_offset = get_buf_entry(buf, VIRGL_SET_SHADER_IMAGE_LAYER_OFFSET(i));
       uint32_t level_size = get_buf_entry(buf, VIRGL_SET_SHADER_IMAGE_LEVEL_SIZE(i));
       uint32_t handle = get_buf_entry(buf, VIRGL_SET_SHADER_IMAGE_RES_HANDLE(i));
-      vrend_set_single_image_view(ctx, shader_type, start_slot + i, format, access,
+      int ret = vrend_set_single_image_view(ctx, shader_type, start_slot + i, format, access,
                                   layer_offset, level_size, handle);
+      if (ret)
+         return ret;
    }
    return 0;
 }
@@ -1323,7 +1429,7 @@ static int vrend_decode_set_streamout_targets(struct vrend_context *ctx,
    uint32_t handles[16];
    uint32_t num_handles = length - 1;
    uint32_t append_bitmask;
-   uint i;
+   unsigned i;
 
    if (length < 1)
       return EINVAL;
@@ -1350,7 +1456,7 @@ static int vrend_decode_texture_barrier(struct vrend_context *ctx, const uint32_
 static int vrend_decode_set_debug_mask(struct vrend_context *ctx, const uint32_t *buf, uint32_t length)
 {
    char *flagstring;
-   size_t slen = sizeof(uint32_t) * length;
+   size_t slen = sizeof(uint32_t) * (length - VIRGL_SET_DEBUG_FLAGSTRING_OFFSET);
 
    if (length < VIRGL_SET_DEBUG_FLAGS_MIN_SIZE)
       return EINVAL;
@@ -1409,12 +1515,28 @@ static int vrend_decode_transfer3d(struct vrend_context *ctx, const uint32_t *bu
                                       transfer_mode);
 }
 
+static int check_copy_transfer3d_handles(struct vrend_context *ctx, uint32_t src_handle,  uint32_t dst_handle,
+                                         struct vrend_resource *src_res, struct vrend_resource *dst_res)
+{
+   if (!src_res || !src_res->iov) {
+      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, src_handle);
+      return EINVAL;
+   }
+
+   if (!dst_res) {
+      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, dst_handle);
+      return EINVAL;
+   }
+   return 0;
+}
+
 static int vrend_decode_copy_transfer3d(struct vrend_context *ctx, const uint32_t *buf, uint32_t length)
 {
    struct pipe_box box;
    struct vrend_transfer_info info;
    uint32_t dst_handle;
    uint32_t src_handle;
+   struct vrend_resource *src_res, *dst_res;
 
    if (length != VIRGL_COPY_TRANSFER3D_SIZE)
       return EINVAL;
@@ -1427,23 +1549,40 @@ static int vrend_decode_copy_transfer3d(struct vrend_context *ctx, const uint32_
    uint32_t flags = get_buf_entry(buf, VIRGL_COPY_TRANSFER3D_FLAGS);
    bool read_from_host = (flags & VIRGL_COPY_TRANSFER3D_FLAGS_READ_FROM_HOST) != 0;
    info.synchronized = (flags & VIRGL_COPY_TRANSFER3D_FLAGS_SYNCHRONIZED) != 0;
+   info.offset = get_buf_entry(buf, VIRGL_COPY_TRANSFER3D_SRC_RES_OFFSET);
+
+#ifdef ENABLE_TESTS
+   info.no_gbm_mapping = (flags & VIRGL_COPY_TRANSFER3D_FLAGS_DEBUG_TEST_NO_GBM_MAPPING) != 0;
+#endif
+
+   if (unlikely(read_from_host)) {
+      vrend_decode_transfer_common(buf, &src_handle, &info);
+      dst_handle = get_buf_entry(buf, VIRGL_COPY_TRANSFER3D_SRC_RES_HANDLE);
+
+      src_res = vrend_renderer_ctx_res_lookup(ctx, src_handle);
+      dst_res = vrend_renderer_ctx_res_lookup(ctx, dst_handle);
+
+      int ret = check_copy_transfer3d_handles(ctx, src_handle, dst_handle, src_res, dst_res);
+      if (ret)
+         return ret;
 
-   if (!read_from_host) {
+      return vrend_renderer_copy_transfer3d_from_host(ctx, dst_handle, src_handle, dst_res, src_res,
+                                                      &info);
+   } else {
       // this means that guest would like to make transfer to host
       // it can also mean that guest is using legacy copy transfer path
       vrend_decode_transfer_common(buf, &dst_handle, &info);
-      info.offset = get_buf_entry(buf, VIRGL_COPY_TRANSFER3D_SRC_RES_OFFSET);
       src_handle = get_buf_entry(buf, VIRGL_COPY_TRANSFER3D_SRC_RES_HANDLE);
 
-      return vrend_renderer_copy_transfer3d(ctx, dst_handle, src_handle,
-                                             &info);
-   } else {
-      vrend_decode_transfer_common(buf, &src_handle, &info);
-      info.offset = get_buf_entry(buf, VIRGL_COPY_TRANSFER3D_SRC_RES_OFFSET);
-      dst_handle = get_buf_entry(buf, VIRGL_COPY_TRANSFER3D_SRC_RES_HANDLE);
+      src_res = vrend_renderer_ctx_res_lookup(ctx, src_handle);
+      dst_res = vrend_renderer_ctx_res_lookup(ctx, dst_handle);
 
-      return vrend_renderer_copy_transfer3d_from_host(ctx, dst_handle, src_handle,
-                                                      &info);
+      int ret = check_copy_transfer3d_handles(ctx, src_handle, dst_handle, src_res, dst_res);
+      if (ret)
+         return ret;
+
+      return vrend_renderer_copy_transfer3d(ctx, dst_handle, dst_res, src_res,
+                                            &info);
    }
 }
 
@@ -1608,13 +1747,13 @@ static int vrend_decode_send_string_marker(struct vrend_context *ctx, const uint
    size_t buf_len = sizeof(uint32_t) * (length - 1);
 
    if (length < VIRGL_SEND_STRING_MARKER_MIN_SIZE) {
-      fprintf(stderr, "minimal command length not okay\n");
+      virgl_error("VIRGL_SEND_STRING_MARKER: minimal command length not okay\n");
       return EINVAL;
    }
 
    uint32_t str_len = get_buf_entry(buf, VIRGL_SEND_STRING_MARKER_STRING_SIZE);
    if (str_len > buf_len) {
-       fprintf(stderr, "String len %u > buf_len %zu\n", str_len, buf_len);
+       virgl_error("VIRGL_SEND_STRING_MARKER: String len %u > buf_len %zu\n", str_len, buf_len);
        return EINVAL;
    }
 
@@ -1821,6 +1960,7 @@ static const vrend_decode_callback decode_table[VIRGL_MAX_COMMANDS] = {
    [VIRGL_CCMD_DESTROY_OBJECT] = vrend_decode_destroy_object,
    [VIRGL_CCMD_CLEAR] = vrend_decode_clear,
    [VIRGL_CCMD_CLEAR_TEXTURE] = vrend_decode_clear_texture,
+   [VIRGL_CCMD_CLEAR_SURFACE] = vrend_decode_clear_surface,
    [VIRGL_CCMD_DRAW_VBO] = vrend_decode_draw_vbo,
    [VIRGL_CCMD_SET_FRAMEBUFFER_STATE] = vrend_decode_set_framebuffer_state,
    [VIRGL_CCMD_SET_VERTEX_BUFFERS] = vrend_decode_set_vertex_buffers,
@@ -1891,6 +2031,19 @@ static const vrend_decode_callback decode_table[VIRGL_MAX_COMMANDS] = {
 #endif
 };
 
+static void dump_command_stream_to_file(const void *buffer, size_t size)
+{
+   uint64_t hash = XXH64(buffer, size, 0);
+   char fname[64];
+   snprintf(fname,
+            sizeof(fname), "buffer_%016" PRIx64 ".seed",  hash);
+   FILE *f = fopen(fname, "wb");
+   if (f) {
+      fwrite(buffer, 1, size, f);
+      fclose(f);
+   }
+}
+
 static int vrend_decode_ctx_submit_cmd(struct virgl_context *ctx,
                                        const void *buffer,
                                        size_t size)
@@ -1900,10 +2053,19 @@ static int vrend_decode_ctx_submit_cmd(struct virgl_context *ctx,
    bool bret;
    int ret;
 
+#define TRANSFER_HEADER_SIZE 4096
+
    bret = vrend_hw_switch_context(gdctx->grctx, true);
    if (bret == false)
       return EINVAL;
 
+   if (VREND_DEBUG_ENABLED &&
+       vrend_debug(gdctx->grctx, dbg_dump_cmd_streams) &&
+       size > TRANSFER_HEADER_SIZE) {
+      dump_command_stream_to_file((char *)buffer + TRANSFER_HEADER_SIZE,
+                                  size - TRANSFER_HEADER_SIZE);
+   }
+
    const uint32_t *typed_buf = (const uint32_t *)buffer;
    const uint32_t buf_total = (uint32_t)(size / sizeof(uint32_t));
    uint32_t buf_offset = 0;
@@ -1931,11 +2093,24 @@ static int vrend_decode_ctx_submit_cmd(struct virgl_context *ctx,
 
       TRACE_SCOPE_SLOW(vrend_get_comand_name(cmd));
 
+      /* If video is disabled at runtime, drop video commands quietly to avoid
+       * noisy errors from guest probes (e.g., gst-plugin-scan).
+       */
+      if (!vrend_renderer_video_available() &&
+          cmd >= VIRGL_CCMD_CREATE_VIDEO_CODEC && cmd <= VIRGL_CCMD_END_FRAME) {
+         continue;
+      }
+
       ret = decode_table[cmd](gdctx->grctx, buf, len);
-      if (!vrend_check_no_error(gdctx->grctx) && !ret)
+      if (!vrend_check_no_error(gdctx->grctx) && !ret) {
+         /* Surface the offending command when a GL error is observed. */
+         virgl_error("GL error after %s (ctx %d cmd=0x%x len=%u offset=%u)\n",
+                     vrend_get_comand_name(cmd), gdctx->base.ctx_id,
+                     cmd, len, cur_offset);
          ret = EINVAL;
+      }
       if (ret) {
-         vrend_printf("context %d failed to dispatch %s: %d\n",
+         virgl_error("context %d failed to dispatch %s: %d\n",
                gdctx->base.ctx_id, vrend_get_comand_name(cmd), ret);
          if (ret == EINVAL)
             vrend_report_buffer_error(gdctx->grctx, *buf);
@@ -1965,6 +2140,9 @@ static int vrend_decode_ctx_submit_fence(struct virgl_context *ctx,
    if (ring_idx)
       return -EINVAL;
 
+   if (!dctx->grctx)
+      return EINVAL;
+
    return vrend_renderer_create_fence(dctx->grctx, flags, fence_id);
 }
 
diff --git a/src/vrend_formats.c b/src/vrend_formats.c
index 679eb10..3ba7238 100644
--- a/src/vrend_formats.c
+++ b/src/vrend_formats.c
@@ -29,13 +29,13 @@
 
 #define SWIZZLE_INVALID 0xff
 #define NO_SWIZZLE { SWIZZLE_INVALID, SWIZZLE_INVALID, SWIZZLE_INVALID, SWIZZLE_INVALID }
-#define RRR1_SWIZZLE { PIPE_SWIZZLE_RED, PIPE_SWIZZLE_RED, PIPE_SWIZZLE_RED, PIPE_SWIZZLE_ONE }
-#define RRRG_SWIZZLE { PIPE_SWIZZLE_RED, PIPE_SWIZZLE_RED, PIPE_SWIZZLE_RED, PIPE_SWIZZLE_GREEN }
-#define RGB1_SWIZZLE { PIPE_SWIZZLE_RED, PIPE_SWIZZLE_GREEN, PIPE_SWIZZLE_BLUE, PIPE_SWIZZLE_ONE }
-#define OOOR_SWIZZLE { PIPE_SWIZZLE_ZERO, PIPE_SWIZZLE_ZERO, PIPE_SWIZZLE_ZERO, PIPE_SWIZZLE_RED  }
+#define RRR1_SWIZZLE { PIPE_SWIZZLE_X, PIPE_SWIZZLE_X, PIPE_SWIZZLE_X, PIPE_SWIZZLE_1 }
+#define RRRG_SWIZZLE { PIPE_SWIZZLE_X, PIPE_SWIZZLE_X, PIPE_SWIZZLE_X, PIPE_SWIZZLE_Y }
+#define RGB1_SWIZZLE { PIPE_SWIZZLE_X, PIPE_SWIZZLE_Y, PIPE_SWIZZLE_Z, PIPE_SWIZZLE_1 }
+#define OOOR_SWIZZLE { PIPE_SWIZZLE_0, PIPE_SWIZZLE_0, PIPE_SWIZZLE_0, PIPE_SWIZZLE_X  }
 
-#define BGR1_SWIZZLE { PIPE_SWIZZLE_BLUE, PIPE_SWIZZLE_GREEN, PIPE_SWIZZLE_RED, PIPE_SWIZZLE_ONE }
-#define BGRA_SWIZZLE { PIPE_SWIZZLE_BLUE, PIPE_SWIZZLE_GREEN, PIPE_SWIZZLE_RED, PIPE_SWIZZLE_ALPHA }
+#define BGR1_SWIZZLE { PIPE_SWIZZLE_Z, PIPE_SWIZZLE_Y, PIPE_SWIZZLE_X, PIPE_SWIZZLE_1 }
+#define BGRA_SWIZZLE { PIPE_SWIZZLE_Z, PIPE_SWIZZLE_Y, PIPE_SWIZZLE_X, PIPE_SWIZZLE_W }
 
 #ifdef __GNUC__
 /* The warning missing-field-initializers is misleading: If at least one field
@@ -55,317 +55,313 @@
 /* fill the format table */
 static struct vrend_format_table base_rgba_formats[] =
   {
-    { VIRGL_FORMAT_R8G8B8X8_UNORM, GL_RGBA8, GL_RGBA, GL_UNSIGNED_BYTE, RGB1_SWIZZLE },
-    { VIRGL_FORMAT_R8G8B8A8_UNORM, GL_RGBA8, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
+    { VIRGL_FORMAT_R8G8_R8B8_UNORM, GL_RGBA8, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_32 },
+    { VIRGL_FORMAT_R8G8B8X8_UNORM, GL_RGBA8, GL_RGBA, GL_UNSIGNED_BYTE, RGB1_SWIZZLE, view_class_32 },
+    { VIRGL_FORMAT_R8G8B8A8_UNORM, GL_RGBA8, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_32 },
 
-    { VIRGL_FORMAT_A8R8G8B8_UNORM, GL_RGBA8, GL_BGRA, GL_UNSIGNED_INT_8_8_8_8, NO_SWIZZLE },
-    { VIRGL_FORMAT_X8R8G8B8_UNORM, GL_RGBA8, GL_BGRA, GL_UNSIGNED_INT_8_8_8_8, NO_SWIZZLE },
+    { VIRGL_FORMAT_A8R8G8B8_UNORM, GL_RGBA8, GL_BGRA, GL_UNSIGNED_INT_8_8_8_8, NO_SWIZZLE, view_class_32 },
+    { VIRGL_FORMAT_X8R8G8B8_UNORM, GL_RGBA8, GL_BGRA, GL_UNSIGNED_INT_8_8_8_8, NO_SWIZZLE, view_class_32 },
 
-    { VIRGL_FORMAT_A8B8G8R8_UNORM, GL_RGBA8, GL_ABGR_EXT, GL_UNSIGNED_BYTE, NO_SWIZZLE },
+    { VIRGL_FORMAT_A8B8G8R8_UNORM, GL_RGBA8, GL_ABGR_EXT, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_32 },
 
-    { VIRGL_FORMAT_B4G4R4X4_UNORM, GL_RGBA4, GL_BGRA, GL_UNSIGNED_SHORT_4_4_4_4_REV, RGB1_SWIZZLE },
-    { VIRGL_FORMAT_A4B4G4R4_UNORM, GL_RGBA4, GL_RGBA, GL_UNSIGNED_SHORT_4_4_4_4, NO_SWIZZLE },
-    { VIRGL_FORMAT_B5G5R5X1_UNORM, GL_RGB5_A1, GL_BGRA, GL_UNSIGNED_SHORT_1_5_5_5_REV, RGB1_SWIZZLE },
+    { VIRGL_FORMAT_B4G4R4X4_UNORM, GL_RGBA4, GL_BGRA, GL_UNSIGNED_SHORT_4_4_4_4_REV, RGB1_SWIZZLE, view_class_unsupported },
+    { VIRGL_FORMAT_A4B4G4R4_UNORM, GL_RGBA4, GL_RGBA, GL_UNSIGNED_SHORT_4_4_4_4, NO_SWIZZLE, view_class_unsupported },
+    { VIRGL_FORMAT_B5G5R5X1_UNORM, GL_RGB5_A1, GL_BGRA, GL_UNSIGNED_SHORT_1_5_5_5_REV, RGB1_SWIZZLE, view_class_unsupported },
 
-    { VIRGL_FORMAT_B5G6R5_UNORM, GL_RGB565, GL_RGB, GL_UNSIGNED_SHORT_5_6_5, NO_SWIZZLE },
-    { VIRGL_FORMAT_B2G3R3_UNORM, GL_R3_G3_B2, GL_RGB, GL_UNSIGNED_BYTE_3_3_2, NO_SWIZZLE },
+    { VIRGL_FORMAT_B5G6R5_UNORM, GL_RGB565, GL_RGB, GL_UNSIGNED_SHORT_5_6_5, NO_SWIZZLE, view_class_unsupported },
+    { VIRGL_FORMAT_B2G3R3_UNORM, GL_R3_G3_B2, GL_RGB, GL_UNSIGNED_BYTE_3_3_2, NO_SWIZZLE, view_class_unsupported },
 
-    { VIRGL_FORMAT_R16G16B16X16_UNORM, GL_RGBA16, GL_RGBA, GL_UNSIGNED_SHORT, RGB1_SWIZZLE },
+    { VIRGL_FORMAT_R16G16B16X16_UNORM, GL_RGBA16, GL_RGBA, GL_UNSIGNED_SHORT, RGB1_SWIZZLE, view_class_64 },
 
-    { VIRGL_FORMAT_R16G16B16A16_UNORM, GL_RGBA16, GL_RGBA, GL_UNSIGNED_SHORT, NO_SWIZZLE },
+    { VIRGL_FORMAT_R16G16B16A16_UNORM, GL_RGBA16, GL_RGBA, GL_UNSIGNED_SHORT, NO_SWIZZLE, view_class_64 },
   };
 
 static struct vrend_format_table gl_base_rgba_formats[] =
   {
-    { VIRGL_FORMAT_B4G4R4A4_UNORM, GL_RGBA4, GL_BGRA, GL_UNSIGNED_SHORT_4_4_4_4_REV, NO_SWIZZLE },
-    { VIRGL_FORMAT_B5G5R5A1_UNORM, GL_RGB5_A1, GL_BGRA, GL_UNSIGNED_SHORT_1_5_5_5_REV, NO_SWIZZLE },
+    { VIRGL_FORMAT_B4G4R4A4_UNORM, GL_RGBA4, GL_BGRA, GL_UNSIGNED_SHORT_4_4_4_4_REV, NO_SWIZZLE, view_class_unsupported },
+    { VIRGL_FORMAT_B5G5R5A1_UNORM, GL_RGB5_A1, GL_BGRA, GL_UNSIGNED_SHORT_1_5_5_5_REV, NO_SWIZZLE, view_class_unsupported },
   };
 
 static struct vrend_format_table base_depth_formats[] =
   {
-    { VIRGL_FORMAT_Z16_UNORM, GL_DEPTH_COMPONENT16, GL_DEPTH_COMPONENT, GL_UNSIGNED_SHORT, NO_SWIZZLE },
-    { VIRGL_FORMAT_Z32_UNORM, GL_DEPTH_COMPONENT32, GL_DEPTH_COMPONENT, GL_UNSIGNED_INT, NO_SWIZZLE },
-    { VIRGL_FORMAT_S8_UINT_Z24_UNORM, GL_DEPTH24_STENCIL8_EXT, GL_DEPTH_STENCIL, GL_UNSIGNED_INT_24_8, NO_SWIZZLE },
-    { VIRGL_FORMAT_Z24X8_UNORM, GL_DEPTH_COMPONENT24, GL_DEPTH_COMPONENT, GL_UNSIGNED_INT, NO_SWIZZLE },
-    { VIRGL_FORMAT_Z32_FLOAT, GL_DEPTH_COMPONENT32F, GL_DEPTH_COMPONENT, GL_FLOAT, NO_SWIZZLE },
+    { VIRGL_FORMAT_Z16_UNORM, GL_DEPTH_COMPONENT16, GL_DEPTH_COMPONENT, GL_UNSIGNED_SHORT, NO_SWIZZLE, view_class_unsupported },
+    { VIRGL_FORMAT_S8_UINT_Z24_UNORM, GL_DEPTH24_STENCIL8_EXT, GL_DEPTH_STENCIL, GL_UNSIGNED_INT_24_8, NO_SWIZZLE, view_class_unsupported },
+    { VIRGL_FORMAT_Z24X8_UNORM, GL_DEPTH_COMPONENT24, GL_DEPTH_COMPONENT, GL_UNSIGNED_INT, NO_SWIZZLE, view_class_unsupported },
+    { VIRGL_FORMAT_Z32_FLOAT, GL_DEPTH_COMPONENT32F, GL_DEPTH_COMPONENT, GL_FLOAT, NO_SWIZZLE, view_class_unsupported },
     /* this is probably a separate format */
-    { VIRGL_FORMAT_Z32_FLOAT_S8X24_UINT, GL_DEPTH32F_STENCIL8, GL_DEPTH_STENCIL, GL_FLOAT_32_UNSIGNED_INT_24_8_REV, NO_SWIZZLE },
-    { VIRGL_FORMAT_X24S8_UINT, GL_STENCIL_INDEX8, GL_STENCIL_INDEX, GL_UNSIGNED_BYTE, NO_SWIZZLE },
+    { VIRGL_FORMAT_Z32_FLOAT_S8X24_UINT, GL_DEPTH32F_STENCIL8, GL_DEPTH_STENCIL, GL_FLOAT_32_UNSIGNED_INT_24_8_REV, NO_SWIZZLE, view_class_unsupported },
+    { VIRGL_FORMAT_X24S8_UINT, GL_STENCIL_INDEX8, GL_STENCIL_INDEX, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_unsupported },
   };
 
+static struct vrend_format_table gl_z32_format[] = {
+   { VIRGL_FORMAT_Z32_UNORM, GL_DEPTH_COMPONENT32, GL_DEPTH_COMPONENT, GL_UNSIGNED_INT, NO_SWIZZLE, view_class_unsupported },
+};
+
+static struct vrend_format_table gles_z32_format[] = {
+   { VIRGL_FORMAT_Z32_UNORM, GL_DEPTH_COMPONENT24, GL_DEPTH_COMPONENT, GL_UNSIGNED_INT, NO_SWIZZLE, view_class_unsupported },
+};
+
 static struct vrend_format_table rg_base_formats[] = {
-  { VIRGL_FORMAT_R8_UNORM, GL_R8, GL_RED, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-  { VIRGL_FORMAT_R8G8_UNORM, GL_RG8, GL_RG, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-  { VIRGL_FORMAT_R16_UNORM, GL_R16, GL_RED, GL_UNSIGNED_SHORT, NO_SWIZZLE },
-  { VIRGL_FORMAT_R16G16_UNORM, GL_RG16, GL_RG, GL_UNSIGNED_SHORT, NO_SWIZZLE },
+  { VIRGL_FORMAT_R8_UNORM, GL_R8, GL_RED, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_8 },
+  { VIRGL_FORMAT_R8G8_UNORM, GL_RG8, GL_RG, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_16 },
+  { VIRGL_FORMAT_R16_UNORM, GL_R16, GL_RED, GL_UNSIGNED_SHORT, NO_SWIZZLE, view_class_16 },
+  { VIRGL_FORMAT_R16G16_UNORM, GL_RG16, GL_RG, GL_UNSIGNED_SHORT, NO_SWIZZLE, view_class_32 },
 };
 
 static struct vrend_format_table integer_base_formats[] = {
-  { VIRGL_FORMAT_R8G8B8A8_UINT, GL_RGBA8UI, GL_RGBA_INTEGER, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-  { VIRGL_FORMAT_R8G8B8A8_SINT, GL_RGBA8I, GL_RGBA_INTEGER, GL_BYTE, NO_SWIZZLE },
+  { VIRGL_FORMAT_R8G8B8A8_UINT, GL_RGBA8UI, GL_RGBA_INTEGER, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_32 },
+  { VIRGL_FORMAT_R8G8B8A8_SINT, GL_RGBA8I, GL_RGBA_INTEGER, GL_BYTE, NO_SWIZZLE, view_class_32 },
 
-  { VIRGL_FORMAT_R16G16B16A16_UINT, GL_RGBA16UI, GL_RGBA_INTEGER, GL_UNSIGNED_SHORT, NO_SWIZZLE },
-  { VIRGL_FORMAT_R16G16B16A16_SINT, GL_RGBA16I, GL_RGBA_INTEGER, GL_SHORT, NO_SWIZZLE },
+  { VIRGL_FORMAT_R16G16B16A16_UINT, GL_RGBA16UI, GL_RGBA_INTEGER, GL_UNSIGNED_SHORT, NO_SWIZZLE, view_class_32 },
+  { VIRGL_FORMAT_R16G16B16A16_SINT, GL_RGBA16I, GL_RGBA_INTEGER, GL_SHORT, NO_SWIZZLE, view_class_32 },
 
-  { VIRGL_FORMAT_R32G32B32A32_UINT, GL_RGBA32UI, GL_RGBA_INTEGER, GL_UNSIGNED_INT, NO_SWIZZLE },
-  { VIRGL_FORMAT_R32G32B32A32_SINT, GL_RGBA32I, GL_RGBA_INTEGER, GL_INT, NO_SWIZZLE },
+  { VIRGL_FORMAT_R32G32B32A32_UINT, GL_RGBA32UI, GL_RGBA_INTEGER, GL_UNSIGNED_INT, NO_SWIZZLE, view_class_32 },
+  { VIRGL_FORMAT_R32G32B32A32_SINT, GL_RGBA32I, GL_RGBA_INTEGER, GL_INT, NO_SWIZZLE, view_class_32 },
 };
 
 static struct vrend_format_table integer_3comp_formats[] = {
-  { VIRGL_FORMAT_R8G8B8X8_UINT, GL_RGBA8UI, GL_RGBA_INTEGER, GL_UNSIGNED_BYTE, RGB1_SWIZZLE },
-  { VIRGL_FORMAT_R8G8B8X8_SINT, GL_RGBA8I, GL_RGBA_INTEGER, GL_BYTE, RGB1_SWIZZLE },
-  { VIRGL_FORMAT_R16G16B16X16_UINT, GL_RGBA16UI, GL_RGBA_INTEGER, GL_UNSIGNED_SHORT, RGB1_SWIZZLE },
-  { VIRGL_FORMAT_R16G16B16X16_SINT, GL_RGBA16I, GL_RGBA_INTEGER, GL_SHORT, RGB1_SWIZZLE },
-  { VIRGL_FORMAT_R32G32B32_UINT, GL_RGB32UI, GL_RGB_INTEGER, GL_UNSIGNED_INT, NO_SWIZZLE },
-  { VIRGL_FORMAT_R32G32B32_SINT, GL_RGB32I, GL_RGB_INTEGER, GL_INT, NO_SWIZZLE },
+  { VIRGL_FORMAT_R8G8B8X8_UINT, GL_RGBA8UI, GL_RGBA_INTEGER, GL_UNSIGNED_BYTE, RGB1_SWIZZLE, view_class_32 },
+  { VIRGL_FORMAT_R8G8B8X8_SINT, GL_RGBA8I, GL_RGBA_INTEGER, GL_BYTE, RGB1_SWIZZLE, view_class_32 },
+  { VIRGL_FORMAT_R16G16B16X16_UINT, GL_RGBA16UI, GL_RGBA_INTEGER, GL_UNSIGNED_SHORT, RGB1_SWIZZLE, view_class_64 },
+  { VIRGL_FORMAT_R16G16B16X16_SINT, GL_RGBA16I, GL_RGBA_INTEGER, GL_SHORT, RGB1_SWIZZLE, view_class_64 },
+  { VIRGL_FORMAT_R32G32B32_UINT, GL_RGB32UI, GL_RGB_INTEGER, GL_UNSIGNED_INT, NO_SWIZZLE, view_class_96 },
+  { VIRGL_FORMAT_R32G32B32_SINT, GL_RGB32I, GL_RGB_INTEGER, GL_INT, NO_SWIZZLE, view_class_96 },
 };
 
 static struct vrend_format_table float_base_formats[] = {
-  { VIRGL_FORMAT_R16G16B16A16_FLOAT, GL_RGBA16F, GL_RGBA, GL_HALF_FLOAT, NO_SWIZZLE },
-  { VIRGL_FORMAT_R32G32B32A32_FLOAT, GL_RGBA32F, GL_RGBA, GL_FLOAT, NO_SWIZZLE },
+  { VIRGL_FORMAT_R16G16B16A16_FLOAT, GL_RGBA16F, GL_RGBA, GL_HALF_FLOAT, NO_SWIZZLE, view_class_64 },
+  { VIRGL_FORMAT_R32G32B32A32_FLOAT, GL_RGBA32F, GL_RGBA, GL_FLOAT, NO_SWIZZLE, view_class_128 },
 };
 
 static struct vrend_format_table integer_rg_formats[] = {
-  { VIRGL_FORMAT_R8_UINT, GL_R8UI, GL_RED_INTEGER, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-  { VIRGL_FORMAT_R8G8_UINT, GL_RG8UI, GL_RG_INTEGER, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-  { VIRGL_FORMAT_R8_SINT, GL_R8I, GL_RED_INTEGER, GL_BYTE, NO_SWIZZLE },
-  { VIRGL_FORMAT_R8G8_SINT, GL_RG8I, GL_RG_INTEGER, GL_BYTE, NO_SWIZZLE },
-
-  { VIRGL_FORMAT_R16_UINT, GL_R16UI, GL_RED_INTEGER, GL_UNSIGNED_SHORT, NO_SWIZZLE },
-  { VIRGL_FORMAT_R16G16_UINT, GL_RG16UI, GL_RG_INTEGER, GL_UNSIGNED_SHORT, NO_SWIZZLE },
-  { VIRGL_FORMAT_R16_SINT, GL_R16I, GL_RED_INTEGER, GL_SHORT, NO_SWIZZLE },
-  { VIRGL_FORMAT_R16G16_SINT, GL_RG16I, GL_RG_INTEGER, GL_SHORT, NO_SWIZZLE },
-
-  { VIRGL_FORMAT_R32_UINT, GL_R32UI, GL_RED_INTEGER, GL_UNSIGNED_INT, NO_SWIZZLE },
-  { VIRGL_FORMAT_R32G32_UINT, GL_RG32UI, GL_RG_INTEGER, GL_UNSIGNED_INT, NO_SWIZZLE },
-  { VIRGL_FORMAT_R32_SINT, GL_R32I, GL_RED_INTEGER, GL_INT, NO_SWIZZLE },
-  { VIRGL_FORMAT_R32G32_SINT, GL_RG32I, GL_RG_INTEGER, GL_INT, NO_SWIZZLE },
+  { VIRGL_FORMAT_R8_UINT, GL_R8UI, GL_RED_INTEGER, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_8 },
+  { VIRGL_FORMAT_R8G8_UINT, GL_RG8UI, GL_RG_INTEGER, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_16 },
+  { VIRGL_FORMAT_R8_SINT, GL_R8I, GL_RED_INTEGER, GL_BYTE, NO_SWIZZLE, view_class_8  },
+  { VIRGL_FORMAT_R8G8_SINT, GL_RG8I, GL_RG_INTEGER, GL_BYTE, NO_SWIZZLE, view_class_16  },
+
+  { VIRGL_FORMAT_R16_UINT, GL_R16UI, GL_RED_INTEGER, GL_UNSIGNED_SHORT, NO_SWIZZLE, view_class_16  },
+  { VIRGL_FORMAT_R16G16_UINT, GL_RG16UI, GL_RG_INTEGER, GL_UNSIGNED_SHORT, NO_SWIZZLE, view_class_32  },
+  { VIRGL_FORMAT_R16_SINT, GL_R16I, GL_RED_INTEGER, GL_SHORT, NO_SWIZZLE, view_class_16  },
+  { VIRGL_FORMAT_R16G16_SINT, GL_RG16I, GL_RG_INTEGER, GL_SHORT, NO_SWIZZLE, view_class_32  },
+
+  { VIRGL_FORMAT_R32_UINT, GL_R32UI, GL_RED_INTEGER, GL_UNSIGNED_INT, NO_SWIZZLE, view_class_32  },
+  { VIRGL_FORMAT_R32G32_UINT, GL_RG32UI, GL_RG_INTEGER, GL_UNSIGNED_INT, NO_SWIZZLE, view_class_64  },
+  { VIRGL_FORMAT_R32_SINT, GL_R32I, GL_RED_INTEGER, GL_INT, NO_SWIZZLE, view_class_32  },
+  { VIRGL_FORMAT_R32G32_SINT, GL_RG32I, GL_RG_INTEGER, GL_INT, NO_SWIZZLE, view_class_64  },
 };
 
 static struct vrend_format_table float_rg_formats[] = {
-  { VIRGL_FORMAT_R16_FLOAT, GL_R16F, GL_RED, GL_HALF_FLOAT, NO_SWIZZLE },
-  { VIRGL_FORMAT_R16G16_FLOAT, GL_RG16F, GL_RG, GL_HALF_FLOAT, NO_SWIZZLE },
-  { VIRGL_FORMAT_R32_FLOAT, GL_R32F, GL_RED, GL_FLOAT, NO_SWIZZLE },
-  { VIRGL_FORMAT_R32G32_FLOAT, GL_RG32F, GL_RG, GL_FLOAT, NO_SWIZZLE },
+  { VIRGL_FORMAT_R16_FLOAT, GL_R16F, GL_RED, GL_HALF_FLOAT, NO_SWIZZLE, view_class_16 },
+  { VIRGL_FORMAT_R16G16_FLOAT, GL_RG16F, GL_RG, GL_HALF_FLOAT, NO_SWIZZLE, view_class_32 },
+  { VIRGL_FORMAT_R32_FLOAT, GL_R32F, GL_RED, GL_FLOAT, NO_SWIZZLE, view_class_32 },
+  { VIRGL_FORMAT_R32G32_FLOAT, GL_RG32F, GL_RG, GL_FLOAT, NO_SWIZZLE, view_class_64 },
 };
 
 static struct vrend_format_table float_3comp_formats[] = {
-  { VIRGL_FORMAT_R16G16B16X16_FLOAT, GL_RGBA16F, GL_RGBA, GL_HALF_FLOAT, RGB1_SWIZZLE },
-  { VIRGL_FORMAT_R32G32B32_FLOAT, GL_RGB32F, GL_RGB, GL_FLOAT, NO_SWIZZLE },
+  { VIRGL_FORMAT_R16G16B16X16_FLOAT, GL_RGBA16F, GL_RGBA, GL_HALF_FLOAT, RGB1_SWIZZLE, view_class_64 },
+  { VIRGL_FORMAT_R32G32B32_FLOAT, GL_RGB32F, GL_RGB, GL_FLOAT, NO_SWIZZLE, view_class_96 },
 };
 
 
 static struct vrend_format_table la_formats_fallback[] = {
-  { VIRGL_FORMAT_A8_UNORM, GL_R8, GL_RED, GL_UNSIGNED_BYTE, OOOR_SWIZZLE},
-  { VIRGL_FORMAT_L8_UNORM, GL_R8, GL_RED, GL_UNSIGNED_BYTE, RRR1_SWIZZLE },
-  { VIRGL_FORMAT_A16_UNORM, GL_R16, GL_RED, GL_UNSIGNED_SHORT, OOOR_SWIZZLE},
-  { VIRGL_FORMAT_L16_UNORM, GL_R16, GL_RED, GL_UNSIGNED_SHORT, RRR1_SWIZZLE },
+  { VIRGL_FORMAT_A8_UNORM, GL_R8, GL_RED, GL_UNSIGNED_BYTE, OOOR_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_L8_UNORM, GL_R8, GL_RED, GL_UNSIGNED_BYTE, RRR1_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_A16_UNORM, GL_R16, GL_RED, GL_UNSIGNED_SHORT, OOOR_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_L16_UNORM, GL_R16, GL_RED, GL_UNSIGNED_SHORT, RRR1_SWIZZLE, view_class_unsupported },
 
-  { VIRGL_FORMAT_A8_UINT, GL_R8UI, GL_RED_INTEGER, GL_UNSIGNED_BYTE, OOOR_SWIZZLE },
-  { VIRGL_FORMAT_L8_UINT, GL_R8UI, GL_RED_INTEGER, GL_UNSIGNED_BYTE, RRR1_SWIZZLE },
+  { VIRGL_FORMAT_A8_UINT, GL_R8UI, GL_RED_INTEGER, GL_UNSIGNED_BYTE, OOOR_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_L8_UINT, GL_R8UI, GL_RED_INTEGER, GL_UNSIGNED_BYTE, RRR1_SWIZZLE, view_class_unsupported },
 
-  { VIRGL_FORMAT_A8_SINT, GL_R8I, GL_RED_INTEGER, GL_BYTE, OOOR_SWIZZLE },
-  { VIRGL_FORMAT_L8_SINT, GL_R8I, GL_RED_INTEGER, GL_BYTE, RRR1_SWIZZLE },
+  { VIRGL_FORMAT_A8_SINT, GL_R8I, GL_RED_INTEGER, GL_BYTE, OOOR_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_L8_SINT, GL_R8I, GL_RED_INTEGER, GL_BYTE, RRR1_SWIZZLE, view_class_unsupported },
 
-  { VIRGL_FORMAT_A16_UINT, GL_R16UI, GL_RED_INTEGER, GL_UNSIGNED_SHORT, OOOR_SWIZZLE },
-  { VIRGL_FORMAT_L16_UINT, GL_R16UI, GL_RED_INTEGER, GL_UNSIGNED_SHORT, RRR1_SWIZZLE },
+  { VIRGL_FORMAT_A16_UINT, GL_R16UI, GL_RED_INTEGER, GL_UNSIGNED_SHORT, OOOR_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_L16_UINT, GL_R16UI, GL_RED_INTEGER, GL_UNSIGNED_SHORT, RRR1_SWIZZLE, view_class_unsupported },
 
-  { VIRGL_FORMAT_A16_SINT, GL_R16I, GL_RED_INTEGER, GL_SHORT, OOOR_SWIZZLE },
-  { VIRGL_FORMAT_L16_SINT, GL_R16I, GL_RED_INTEGER, GL_SHORT, RRR1_SWIZZLE },
+  { VIRGL_FORMAT_A16_SINT, GL_R16I, GL_RED_INTEGER, GL_SHORT, OOOR_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_L16_SINT, GL_R16I, GL_RED_INTEGER, GL_SHORT, RRR1_SWIZZLE, view_class_unsupported },
 
-  { VIRGL_FORMAT_A32_UINT, GL_R32UI, GL_RED_INTEGER, GL_UNSIGNED_INT, OOOR_SWIZZLE },
-  { VIRGL_FORMAT_L32_UINT, GL_R32UI, GL_RED_INTEGER, GL_UNSIGNED_INT, RRR1_SWIZZLE },
+  { VIRGL_FORMAT_A32_UINT, GL_R32UI, GL_RED_INTEGER, GL_UNSIGNED_INT, OOOR_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_L32_UINT, GL_R32UI, GL_RED_INTEGER, GL_UNSIGNED_INT, RRR1_SWIZZLE, view_class_unsupported },
 
-  { VIRGL_FORMAT_A32_SINT, GL_R32I, GL_RED_INTEGER, GL_INT, OOOR_SWIZZLE },
-  { VIRGL_FORMAT_L32_SINT, GL_R32I, GL_RED_INTEGER, GL_INT, RRR1_SWIZZLE },
+  { VIRGL_FORMAT_A32_SINT, GL_R32I, GL_RED_INTEGER, GL_INT, OOOR_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_L32_SINT, GL_R32I, GL_RED_INTEGER, GL_INT, RRR1_SWIZZLE, view_class_unsupported },
 
-  { VIRGL_FORMAT_L16_FLOAT, GL_R16F, GL_RED, GL_HALF_FLOAT, RRR1_SWIZZLE },
+  { VIRGL_FORMAT_L16_FLOAT, GL_R16F, GL_RED, GL_HALF_FLOAT, RRR1_SWIZZLE, view_class_unsupported },
 
-  { VIRGL_FORMAT_L32_FLOAT, GL_R32F, GL_RED, GL_FLOAT, RRR1_SWIZZLE},
+  { VIRGL_FORMAT_L32_FLOAT, GL_R32F, GL_RED, GL_FLOAT, RRR1_SWIZZLE, view_class_unsupported },
 };
 
 
 static struct vrend_format_table la_formats_compat[] = {
-  { VIRGL_FORMAT_A8_UNORM, GL_ALPHA8, GL_ALPHA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-  { VIRGL_FORMAT_A16_UNORM, GL_ALPHA16, GL_ALPHA, GL_UNSIGNED_SHORT, NO_SWIZZLE },
-
-  { VIRGL_FORMAT_A8_UINT, GL_ALPHA8UI_EXT, GL_ALPHA_INTEGER, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-  { VIRGL_FORMAT_L8_UINT, GL_LUMINANCE8UI_EXT, GL_LUMINANCE_INTEGER_EXT, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-  { VIRGL_FORMAT_L8A8_UINT, GL_LUMINANCE_ALPHA8UI_EXT, GL_LUMINANCE_ALPHA_INTEGER_EXT, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-  { VIRGL_FORMAT_A8_SINT, GL_ALPHA8I_EXT, GL_ALPHA_INTEGER, GL_BYTE, NO_SWIZZLE },
-  { VIRGL_FORMAT_L8_SINT, GL_LUMINANCE8I_EXT, GL_LUMINANCE_INTEGER_EXT, GL_BYTE, NO_SWIZZLE },
-  { VIRGL_FORMAT_L8A8_SINT, GL_LUMINANCE_ALPHA8I_EXT, GL_LUMINANCE_ALPHA_INTEGER_EXT, GL_BYTE, NO_SWIZZLE },
-
-  { VIRGL_FORMAT_A16_UINT, GL_ALPHA16UI_EXT, GL_ALPHA_INTEGER,  GL_UNSIGNED_SHORT, NO_SWIZZLE },
-  { VIRGL_FORMAT_L16_UINT, GL_LUMINANCE16UI_EXT, GL_LUMINANCE_INTEGER_EXT, GL_UNSIGNED_SHORT, RRR1_SWIZZLE },
-  { VIRGL_FORMAT_L16A16_UINT, GL_LUMINANCE_ALPHA16UI_EXT, GL_LUMINANCE_ALPHA_INTEGER_EXT, GL_UNSIGNED_SHORT, NO_SWIZZLE },
-
-  { VIRGL_FORMAT_A16_SINT, GL_ALPHA16I_EXT, GL_ALPHA_INTEGER, GL_SHORT, NO_SWIZZLE },
-  { VIRGL_FORMAT_L16_SINT, GL_LUMINANCE16I_EXT, GL_LUMINANCE_INTEGER_EXT, GL_SHORT, NO_SWIZZLE },
-  { VIRGL_FORMAT_L16A16_SINT, GL_LUMINANCE_ALPHA16I_EXT, GL_LUMINANCE_ALPHA_INTEGER_EXT, GL_SHORT, NO_SWIZZLE },
-
-  { VIRGL_FORMAT_A32_UINT, GL_ALPHA32UI_EXT, GL_ALPHA_INTEGER, GL_UNSIGNED_INT, NO_SWIZZLE },
-  { VIRGL_FORMAT_L32_UINT, GL_R32UI, GL_LUMINANCE_INTEGER_EXT, GL_UNSIGNED_INT, NO_SWIZZLE },
-  { VIRGL_FORMAT_L32A32_UINT, GL_LUMINANCE_ALPHA32UI_EXT, GL_LUMINANCE_ALPHA_INTEGER_EXT, GL_UNSIGNED_INT, NO_SWIZZLE },
-
-  { VIRGL_FORMAT_A32_SINT, GL_ALPHA32I_EXT, GL_ALPHA_INTEGER, GL_INT, NO_SWIZZLE },
-  { VIRGL_FORMAT_L32_SINT, GL_LUMINANCE32I_EXT, GL_LUMINANCE_INTEGER_EXT, GL_INT, NO_SWIZZLE },
-  { VIRGL_FORMAT_L32A32_SINT, GL_LUMINANCE_ALPHA32I_EXT, GL_LUMINANCE_ALPHA_INTEGER_EXT, GL_INT, NO_SWIZZLE },
-
-  { VIRGL_FORMAT_A16_FLOAT, GL_ALPHA16F_ARB, GL_ALPHA, GL_HALF_FLOAT, NO_SWIZZLE },
-  { VIRGL_FORMAT_L16_FLOAT, GL_LUMINANCE16F_ARB, GL_LUMINANCE, GL_HALF_FLOAT, NO_SWIZZLE },
-  { VIRGL_FORMAT_L16A16_FLOAT, GL_LUMINANCE_ALPHA16F_ARB, GL_LUMINANCE_ALPHA, GL_HALF_FLOAT_ARB, NO_SWIZZLE },
-
-  { VIRGL_FORMAT_A32_FLOAT, GL_ALPHA32F_ARB, GL_ALPHA, GL_FLOAT, NO_SWIZZLE },
-  { VIRGL_FORMAT_L32_FLOAT, GL_LUMINANCE32F_ARB, GL_LUMINANCE, GL_FLOAT, NO_SWIZZLE },
-  { VIRGL_FORMAT_L32A32_FLOAT, GL_LUMINANCE_ALPHA32F_ARB, GL_LUMINANCE_ALPHA, GL_FLOAT, NO_SWIZZLE },
+  { VIRGL_FORMAT_A8_UNORM, GL_ALPHA8, GL_ALPHA, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_A16_UNORM, GL_ALPHA16, GL_ALPHA, GL_UNSIGNED_SHORT, NO_SWIZZLE, view_class_unsupported },
+
+  { VIRGL_FORMAT_A8_UINT, GL_ALPHA8UI_EXT, GL_ALPHA_INTEGER, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_L8_UINT, GL_LUMINANCE8UI_EXT, GL_LUMINANCE_INTEGER_EXT, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_L8A8_UINT, GL_LUMINANCE_ALPHA8UI_EXT, GL_LUMINANCE_ALPHA_INTEGER_EXT, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_A8_SINT, GL_ALPHA8I_EXT, GL_ALPHA_INTEGER, GL_BYTE, NO_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_L8_SINT, GL_LUMINANCE8I_EXT, GL_LUMINANCE_INTEGER_EXT, GL_BYTE, NO_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_L8A8_SINT, GL_LUMINANCE_ALPHA8I_EXT, GL_LUMINANCE_ALPHA_INTEGER_EXT, GL_BYTE, NO_SWIZZLE, view_class_unsupported },
+
+  { VIRGL_FORMAT_A16_UINT, GL_ALPHA16UI_EXT, GL_ALPHA_INTEGER,  GL_UNSIGNED_SHORT, NO_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_L16_UINT, GL_LUMINANCE16UI_EXT, GL_LUMINANCE_INTEGER_EXT, GL_UNSIGNED_SHORT, RRR1_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_L16A16_UINT, GL_LUMINANCE_ALPHA16UI_EXT, GL_LUMINANCE_ALPHA_INTEGER_EXT, GL_UNSIGNED_SHORT, NO_SWIZZLE, view_class_unsupported },
+
+  { VIRGL_FORMAT_A16_SINT, GL_ALPHA16I_EXT, GL_ALPHA_INTEGER, GL_SHORT, NO_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_L16_SINT, GL_LUMINANCE16I_EXT, GL_LUMINANCE_INTEGER_EXT, GL_SHORT, NO_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_L16A16_SINT, GL_LUMINANCE_ALPHA16I_EXT, GL_LUMINANCE_ALPHA_INTEGER_EXT, GL_SHORT, NO_SWIZZLE, view_class_unsupported },
+
+  { VIRGL_FORMAT_A32_UINT, GL_ALPHA32UI_EXT, GL_ALPHA_INTEGER, GL_UNSIGNED_INT, NO_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_L32_UINT, GL_R32UI, GL_LUMINANCE_INTEGER_EXT, GL_UNSIGNED_INT, NO_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_L32A32_UINT, GL_LUMINANCE_ALPHA32UI_EXT, GL_LUMINANCE_ALPHA_INTEGER_EXT, GL_UNSIGNED_INT, NO_SWIZZLE, view_class_unsupported },
+
+  { VIRGL_FORMAT_A32_SINT, GL_ALPHA32I_EXT, GL_ALPHA_INTEGER, GL_INT, NO_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_L32_SINT, GL_LUMINANCE32I_EXT, GL_LUMINANCE_INTEGER_EXT, GL_INT, NO_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_L32A32_SINT, GL_LUMINANCE_ALPHA32I_EXT, GL_LUMINANCE_ALPHA_INTEGER_EXT, GL_INT, NO_SWIZZLE, view_class_unsupported },
+
+  { VIRGL_FORMAT_A16_FLOAT, GL_ALPHA16F_ARB, GL_ALPHA, GL_HALF_FLOAT, NO_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_L16_FLOAT, GL_LUMINANCE16F_ARB, GL_LUMINANCE, GL_HALF_FLOAT, NO_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_L16A16_FLOAT, GL_LUMINANCE_ALPHA16F_ARB, GL_LUMINANCE_ALPHA, GL_HALF_FLOAT_ARB, NO_SWIZZLE, view_class_unsupported },
+
+  { VIRGL_FORMAT_A32_FLOAT, GL_ALPHA32F_ARB, GL_ALPHA, GL_FLOAT, NO_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_L32_FLOAT, GL_LUMINANCE32F_ARB, GL_LUMINANCE, GL_FLOAT, NO_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_L32A32_FLOAT, GL_LUMINANCE_ALPHA32F_ARB, GL_LUMINANCE_ALPHA, GL_FLOAT, NO_SWIZZLE, view_class_unsupported },
 };
 
 static struct vrend_format_table snorm_formats[] = {
-  { VIRGL_FORMAT_R8_SNORM, GL_R8_SNORM, GL_RED, GL_BYTE, NO_SWIZZLE },
-  { VIRGL_FORMAT_R8G8_SNORM, GL_RG8_SNORM, GL_RG, GL_BYTE, NO_SWIZZLE },
+  { VIRGL_FORMAT_R8_SNORM, GL_R8_SNORM, GL_RED, GL_BYTE, NO_SWIZZLE, view_class_8 },
+  { VIRGL_FORMAT_R8G8_SNORM, GL_RG8_SNORM, GL_RG, GL_BYTE, NO_SWIZZLE, view_class_16 },
 
-  { VIRGL_FORMAT_R8G8B8A8_SNORM, GL_RGBA8_SNORM, GL_RGBA, GL_BYTE, NO_SWIZZLE },
-  { VIRGL_FORMAT_R8G8B8X8_SNORM, GL_RGBA8_SNORM, GL_RGBA, GL_BYTE, RGB1_SWIZZLE },
+  { VIRGL_FORMAT_R8G8B8A8_SNORM, GL_RGBA8_SNORM, GL_RGBA, GL_BYTE, NO_SWIZZLE, view_class_32 },
+  { VIRGL_FORMAT_R8G8B8X8_SNORM, GL_RGBA8_SNORM, GL_RGBA, GL_BYTE, RGB1_SWIZZLE, view_class_32 },
 
-  { VIRGL_FORMAT_R16_SNORM, GL_R16_SNORM, GL_RED, GL_SHORT, NO_SWIZZLE },
-  { VIRGL_FORMAT_R16G16_SNORM, GL_RG16_SNORM, GL_RG, GL_SHORT, NO_SWIZZLE },
-  { VIRGL_FORMAT_R16G16B16A16_SNORM, GL_RGBA16_SNORM, GL_RGBA, GL_SHORT, NO_SWIZZLE },
+  { VIRGL_FORMAT_R16_SNORM, GL_R16_SNORM, GL_RED, GL_SHORT, NO_SWIZZLE, view_class_16 },
+  { VIRGL_FORMAT_R16G16_SNORM, GL_RG16_SNORM, GL_RG, GL_SHORT, NO_SWIZZLE, view_class_32 },
+  { VIRGL_FORMAT_R16G16B16A16_SNORM, GL_RGBA16_SNORM, GL_RGBA, GL_SHORT, NO_SWIZZLE, view_class_64 },
 
-  { VIRGL_FORMAT_R16G16B16X16_SNORM, GL_RGBA16_SNORM, GL_RGBA, GL_SHORT, RGB1_SWIZZLE },
+  { VIRGL_FORMAT_R16G16B16X16_SNORM, GL_RGBA16_SNORM, GL_RGBA, GL_SHORT, RGB1_SWIZZLE, view_class_64 },
 };
 
 static struct vrend_format_table snorm_la_formats[] = {
-  { VIRGL_FORMAT_A8_SNORM, GL_ALPHA8_SNORM, GL_ALPHA, GL_BYTE, NO_SWIZZLE },
-  { VIRGL_FORMAT_L8_SNORM, GL_R8_SNORM, GL_RED, GL_BYTE, RRR1_SWIZZLE },
-  { VIRGL_FORMAT_L8A8_SNORM, GL_LUMINANCE8_ALPHA8_SNORM, GL_LUMINANCE_ALPHA, GL_BYTE, NO_SWIZZLE },
-  { VIRGL_FORMAT_A16_SNORM, GL_ALPHA16_SNORM, GL_ALPHA, GL_SHORT, NO_SWIZZLE },
-  { VIRGL_FORMAT_L16_SNORM, GL_R16_SNORM, GL_RED, GL_SHORT, RRR1_SWIZZLE },
-  { VIRGL_FORMAT_L16A16_SNORM, GL_LUMINANCE16_ALPHA16_SNORM, GL_LUMINANCE_ALPHA, GL_SHORT, NO_SWIZZLE },
+  { VIRGL_FORMAT_A8_SNORM, GL_ALPHA8_SNORM, GL_ALPHA, GL_BYTE, NO_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_L8_SNORM, GL_R8_SNORM, GL_RED, GL_BYTE, RRR1_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_L8A8_SNORM, GL_LUMINANCE8_ALPHA8_SNORM, GL_LUMINANCE_ALPHA, GL_BYTE, NO_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_A16_SNORM, GL_ALPHA16_SNORM, GL_ALPHA, GL_SHORT, NO_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_L16_SNORM, GL_R16_SNORM, GL_RED, GL_SHORT, RRR1_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_L16A16_SNORM, GL_LUMINANCE16_ALPHA16_SNORM, GL_LUMINANCE_ALPHA, GL_SHORT, NO_SWIZZLE, view_class_unsupported },
 };
 
 static struct vrend_format_table dxtn_formats[] = {
-  { VIRGL_FORMAT_DXT1_RGB, GL_COMPRESSED_RGB_S3TC_DXT1_EXT, GL_RGB, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-  { VIRGL_FORMAT_DXT1_RGBA, GL_COMPRESSED_RGBA_S3TC_DXT1_EXT, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-  { VIRGL_FORMAT_DXT3_RGBA, GL_COMPRESSED_RGBA_S3TC_DXT3_EXT, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-  { VIRGL_FORMAT_DXT5_RGBA, GL_COMPRESSED_RGBA_S3TC_DXT5_EXT, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
+  { VIRGL_FORMAT_DXT1_RGB, GL_COMPRESSED_RGB_S3TC_DXT1_EXT, GL_RGB, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_dxt1_rgb },
+  { VIRGL_FORMAT_DXT1_RGBA, GL_COMPRESSED_RGBA_S3TC_DXT1_EXT, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_dxt1_rgba },
+  { VIRGL_FORMAT_DXT3_RGBA, GL_COMPRESSED_RGBA_S3TC_DXT3_EXT, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_dxt3_rgba },
+  { VIRGL_FORMAT_DXT5_RGBA, GL_COMPRESSED_RGBA_S3TC_DXT5_EXT, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_dxt5_rgba },
 };
 
 static struct vrend_format_table dxtn_srgb_formats[] = {
-  { VIRGL_FORMAT_DXT1_SRGB, GL_COMPRESSED_SRGB_S3TC_DXT1_EXT, GL_RGB, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-  { VIRGL_FORMAT_DXT1_SRGBA, GL_COMPRESSED_SRGB_ALPHA_S3TC_DXT1_EXT, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-  { VIRGL_FORMAT_DXT3_SRGBA, GL_COMPRESSED_SRGB_ALPHA_S3TC_DXT3_EXT, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-  { VIRGL_FORMAT_DXT5_SRGBA, GL_COMPRESSED_SRGB_ALPHA_S3TC_DXT5_EXT, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
+  { VIRGL_FORMAT_DXT1_SRGB, GL_COMPRESSED_SRGB_S3TC_DXT1_EXT, GL_RGB, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_dxt1_rgb },
+  { VIRGL_FORMAT_DXT1_SRGBA, GL_COMPRESSED_SRGB_ALPHA_S3TC_DXT1_EXT, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_dxt1_rgba },
+  { VIRGL_FORMAT_DXT3_SRGBA, GL_COMPRESSED_SRGB_ALPHA_S3TC_DXT3_EXT, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_dxt3_rgba },
+  { VIRGL_FORMAT_DXT5_SRGBA, GL_COMPRESSED_SRGB_ALPHA_S3TC_DXT5_EXT, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_dxt5_rgba },
 };
 
 static struct vrend_format_table etc2_formats[] = {
-  {VIRGL_FORMAT_ETC2_RGB8, GL_COMPRESSED_RGB8_ETC2, GL_RGB, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-  {VIRGL_FORMAT_ETC2_SRGB8, GL_COMPRESSED_SRGB8_ETC2, GL_RGB, GL_BYTE, NO_SWIZZLE },
-  {VIRGL_FORMAT_ETC2_RGB8A1, GL_COMPRESSED_RGB8_PUNCHTHROUGH_ALPHA1_ETC2, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-  {VIRGL_FORMAT_ETC2_SRGB8A1, GL_COMPRESSED_SRGB8_PUNCHTHROUGH_ALPHA1_ETC2, GL_RGBA, GL_BYTE, NO_SWIZZLE },
-  {VIRGL_FORMAT_ETC2_RGBA8, GL_COMPRESSED_RGBA8_ETC2_EAC, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-  {VIRGL_FORMAT_ETC2_SRGBA8, GL_COMPRESSED_SRGB8_ALPHA8_ETC2_EAC, GL_RGBA, GL_BYTE, NO_SWIZZLE },
-  {VIRGL_FORMAT_ETC2_R11_UNORM, GL_COMPRESSED_R11_EAC, GL_RED, GL_UNSIGNED_BYTE, NO_SWIZZLE},
-  {VIRGL_FORMAT_ETC2_R11_SNORM, GL_COMPRESSED_SIGNED_R11_EAC, GL_RED, GL_BYTE, NO_SWIZZLE},
-  {VIRGL_FORMAT_ETC2_RG11_UNORM, GL_COMPRESSED_RG11_EAC, GL_RG, GL_UNSIGNED_BYTE, NO_SWIZZLE},
-  {VIRGL_FORMAT_ETC2_RG11_SNORM, GL_COMPRESSED_SIGNED_RG11_EAC, GL_RG, GL_BYTE, NO_SWIZZLE},
+  {VIRGL_FORMAT_ETC2_RGB8, GL_COMPRESSED_RGB8_ETC2, GL_RGB, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_etc2_rgb },
+  {VIRGL_FORMAT_ETC2_SRGB8, GL_COMPRESSED_SRGB8_ETC2, GL_RGB, GL_BYTE, NO_SWIZZLE, view_class_etc2_rgb },
+  {VIRGL_FORMAT_ETC2_RGB8A1, GL_COMPRESSED_RGB8_PUNCHTHROUGH_ALPHA1_ETC2, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_etc2_rgba },
+  {VIRGL_FORMAT_ETC2_SRGB8A1, GL_COMPRESSED_SRGB8_PUNCHTHROUGH_ALPHA1_ETC2, GL_RGBA, GL_BYTE, NO_SWIZZLE, view_class_etc2_rgba },
+  {VIRGL_FORMAT_ETC2_RGBA8, GL_COMPRESSED_RGBA8_ETC2_EAC, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_etc2_eac_rgba },
+  {VIRGL_FORMAT_ETC2_SRGBA8, GL_COMPRESSED_SRGB8_ALPHA8_ETC2_EAC, GL_RGBA, GL_BYTE, NO_SWIZZLE, view_class_etc2_eac_rgba },
+  {VIRGL_FORMAT_ETC2_R11_UNORM, GL_COMPRESSED_R11_EAC, GL_RED, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_unsupported},
+  {VIRGL_FORMAT_ETC2_R11_SNORM, GL_COMPRESSED_SIGNED_R11_EAC, GL_RED, GL_BYTE, NO_SWIZZLE, view_class_unsupported},
+  {VIRGL_FORMAT_ETC2_RG11_UNORM, GL_COMPRESSED_RG11_EAC, GL_RG, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_unsupported},
+  {VIRGL_FORMAT_ETC2_RG11_SNORM, GL_COMPRESSED_SIGNED_RG11_EAC, GL_RG, GL_BYTE, NO_SWIZZLE, view_class_unsupported},
 };
+
+#define ASTC_FORMAT(size) \
+  {VIRGL_FORMAT_ASTC_ ## size, GL_COMPRESSED_RGBA_ASTC_ ## size, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_astc_## size ##_rgba}, \
+  {VIRGL_FORMAT_ASTC_ ## size, GL_COMPRESSED_SRGB8_ALPHA8_ASTC_ ## size, GL_RGBA, GL_BYTE, NO_SWIZZLE, view_class_astc_## size ##_rgba}
+
 static struct vrend_format_table astc_formats[] = {
-   {VIRGL_FORMAT_ASTC_4x4, GL_COMPRESSED_RGBA_ASTC_4x4, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-   {VIRGL_FORMAT_ASTC_5x4, GL_COMPRESSED_RGBA_ASTC_5x4, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-   {VIRGL_FORMAT_ASTC_5x5, GL_COMPRESSED_RGBA_ASTC_5x5, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-   {VIRGL_FORMAT_ASTC_6x5, GL_COMPRESSED_RGBA_ASTC_6x5, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-   {VIRGL_FORMAT_ASTC_6x6, GL_COMPRESSED_RGBA_ASTC_6x6, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-   {VIRGL_FORMAT_ASTC_8x5, GL_COMPRESSED_RGBA_ASTC_8x5, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-   {VIRGL_FORMAT_ASTC_8x6, GL_COMPRESSED_RGBA_ASTC_8x6, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-   {VIRGL_FORMAT_ASTC_8x8, GL_COMPRESSED_RGBA_ASTC_8x8, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-   {VIRGL_FORMAT_ASTC_10x5, GL_COMPRESSED_RGBA_ASTC_10x5, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-   {VIRGL_FORMAT_ASTC_10x6, GL_COMPRESSED_RGBA_ASTC_10x6, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-   {VIRGL_FORMAT_ASTC_10x8, GL_COMPRESSED_RGBA_ASTC_10x8, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-   {VIRGL_FORMAT_ASTC_10x10, GL_COMPRESSED_RGBA_ASTC_10x10, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-   {VIRGL_FORMAT_ASTC_12x10, GL_COMPRESSED_RGBA_ASTC_12x10, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-   {VIRGL_FORMAT_ASTC_12x12, GL_COMPRESSED_RGBA_ASTC_12x12, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-   {VIRGL_FORMAT_ASTC_4x4_SRGB, GL_COMPRESSED_SRGB8_ALPHA8_ASTC_4x4, GL_RGBA, GL_BYTE, NO_SWIZZLE },
-   {VIRGL_FORMAT_ASTC_5x4_SRGB, GL_COMPRESSED_SRGB8_ALPHA8_ASTC_5x4, GL_RGBA, GL_BYTE, NO_SWIZZLE },
-   {VIRGL_FORMAT_ASTC_5x5_SRGB, GL_COMPRESSED_SRGB8_ALPHA8_ASTC_5x5, GL_RGBA, GL_BYTE, NO_SWIZZLE },
-   {VIRGL_FORMAT_ASTC_6x5_SRGB, GL_COMPRESSED_SRGB8_ALPHA8_ASTC_6x5, GL_RGBA, GL_BYTE, NO_SWIZZLE },
-   {VIRGL_FORMAT_ASTC_6x6_SRGB, GL_COMPRESSED_SRGB8_ALPHA8_ASTC_6x6, GL_RGBA, GL_BYTE, NO_SWIZZLE },
-   {VIRGL_FORMAT_ASTC_8x5_SRGB, GL_COMPRESSED_SRGB8_ALPHA8_ASTC_8x5, GL_RGBA, GL_BYTE, NO_SWIZZLE },
-   {VIRGL_FORMAT_ASTC_8x6_SRGB, GL_COMPRESSED_SRGB8_ALPHA8_ASTC_8x6, GL_RGBA, GL_BYTE, NO_SWIZZLE },
-   {VIRGL_FORMAT_ASTC_8x8_SRGB, GL_COMPRESSED_SRGB8_ALPHA8_ASTC_8x8, GL_RGBA, GL_BYTE, NO_SWIZZLE },
-   {VIRGL_FORMAT_ASTC_10x5_SRGB, GL_COMPRESSED_SRGB8_ALPHA8_ASTC_10x5, GL_RGBA, GL_BYTE, NO_SWIZZLE },
-   {VIRGL_FORMAT_ASTC_10x6_SRGB, GL_COMPRESSED_SRGB8_ALPHA8_ASTC_10x6, GL_RGBA, GL_BYTE, NO_SWIZZLE },
-   {VIRGL_FORMAT_ASTC_10x8_SRGB, GL_COMPRESSED_SRGB8_ALPHA8_ASTC_10x8, GL_RGBA, GL_BYTE, NO_SWIZZLE },
-   {VIRGL_FORMAT_ASTC_10x10_SRGB, GL_COMPRESSED_SRGB8_ALPHA8_ASTC_10x10, GL_RGBA, GL_BYTE, NO_SWIZZLE },
-   {VIRGL_FORMAT_ASTC_12x10_SRGB, GL_COMPRESSED_SRGB8_ALPHA8_ASTC_12x10, GL_RGBA, GL_BYTE, NO_SWIZZLE },
-   {VIRGL_FORMAT_ASTC_12x12_SRGB, GL_COMPRESSED_SRGB8_ALPHA8_ASTC_12x12, GL_RGBA, GL_BYTE, NO_SWIZZLE },
+   ASTC_FORMAT(4x4),
+   ASTC_FORMAT(5x4),
+   ASTC_FORMAT(5x5),
+   ASTC_FORMAT(6x5),
+   ASTC_FORMAT(6x6),
+   ASTC_FORMAT(8x5),
+   ASTC_FORMAT(8x6),
+   ASTC_FORMAT(8x8),
+   ASTC_FORMAT(10x5),
+   ASTC_FORMAT(10x6),
+   ASTC_FORMAT(10x8),
+   ASTC_FORMAT(10x10),
+   ASTC_FORMAT(12x10),
+   ASTC_FORMAT(12x12),
 };
 
 static struct vrend_format_table rgtc_formats[] = {
-  { VIRGL_FORMAT_RGTC1_UNORM, GL_COMPRESSED_RED_RGTC1, GL_RED, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-  { VIRGL_FORMAT_RGTC1_SNORM, GL_COMPRESSED_SIGNED_RED_RGTC1, GL_RED, GL_BYTE, NO_SWIZZLE },
+  { VIRGL_FORMAT_RGTC1_UNORM, GL_COMPRESSED_RED_RGTC1, GL_RED, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_rgtc1_red },
+  { VIRGL_FORMAT_RGTC1_SNORM, GL_COMPRESSED_SIGNED_RED_RGTC1, GL_RED, GL_BYTE, NO_SWIZZLE, view_class_rgtc1_red },
 
-  { VIRGL_FORMAT_RGTC2_UNORM, GL_COMPRESSED_RG_RGTC2, GL_RG, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-  { VIRGL_FORMAT_RGTC2_SNORM, GL_COMPRESSED_SIGNED_RG_RGTC2, GL_RG, GL_BYTE, NO_SWIZZLE },
+  { VIRGL_FORMAT_RGTC2_UNORM, GL_COMPRESSED_RG_RGTC2, GL_RG, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_rgtc2_rg },
+  { VIRGL_FORMAT_RGTC2_SNORM, GL_COMPRESSED_SIGNED_RG_RGTC2, GL_RG, GL_BYTE, NO_SWIZZLE, view_class_rgtc2_rg },
 };
 
 static struct vrend_format_table srgb_formats[] = {
-  { VIRGL_FORMAT_R8G8B8X8_SRGB, GL_SRGB8_ALPHA8, GL_RGBA, GL_UNSIGNED_BYTE, RGB1_SWIZZLE },
-  { VIRGL_FORMAT_R8G8B8A8_SRGB, GL_SRGB8_ALPHA8, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
+  { VIRGL_FORMAT_R8G8B8X8_SRGB, GL_SRGB8_ALPHA8, GL_RGBA, GL_UNSIGNED_BYTE, RGB1_SWIZZLE, view_class_32 },
+  { VIRGL_FORMAT_R8G8B8A8_SRGB, GL_SRGB8_ALPHA8, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_32 },
 
-  { VIRGL_FORMAT_L8_SRGB, GL_SR8_EXT, GL_RED, GL_UNSIGNED_BYTE, RRR1_SWIZZLE },
-  { VIRGL_FORMAT_R8_SRGB, GL_SR8_EXT, GL_RED, GL_UNSIGNED_BYTE, NO_SWIZZLE },
+  { VIRGL_FORMAT_L8_SRGB, GL_SR8_EXT, GL_RED, GL_UNSIGNED_BYTE, RRR1_SWIZZLE, view_class_unsupported },
+  { VIRGL_FORMAT_R8_SRGB, GL_SR8_EXT, GL_RED, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_unsupported },
 
-  { VIRGL_FORMAT_R8G8_SRGB, GL_SRG8_EXT, GL_RG, GL_UNSIGNED_BYTE, NO_SWIZZLE },
+  { VIRGL_FORMAT_R8G8_SRGB, GL_SRG8_EXT, GL_RG, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_16 },
 };
 
 static struct vrend_format_table bit10_formats[] = {
-  { VIRGL_FORMAT_B10G10R10X2_UNORM, GL_RGB10_A2, GL_BGRA, GL_UNSIGNED_INT_2_10_10_10_REV, RGB1_SWIZZLE },
-  { VIRGL_FORMAT_B10G10R10A2_UNORM, GL_RGB10_A2, GL_BGRA, GL_UNSIGNED_INT_2_10_10_10_REV, NO_SWIZZLE },
-  { VIRGL_FORMAT_B10G10R10A2_UINT, GL_RGB10_A2UI, GL_BGRA_INTEGER, GL_UNSIGNED_INT_2_10_10_10_REV, NO_SWIZZLE },
-  { VIRGL_FORMAT_R10G10B10X2_UNORM, GL_RGB10_A2, GL_RGBA, GL_UNSIGNED_INT_2_10_10_10_REV, RGB1_SWIZZLE },
-  { VIRGL_FORMAT_R10G10B10A2_UNORM, GL_RGB10_A2, GL_RGBA, GL_UNSIGNED_INT_2_10_10_10_REV, NO_SWIZZLE },
-  { VIRGL_FORMAT_R10G10B10A2_UINT, GL_RGB10_A2UI, GL_RGBA_INTEGER, GL_UNSIGNED_INT_2_10_10_10_REV, NO_SWIZZLE },
+  { VIRGL_FORMAT_B10G10R10A2_UINT, GL_RGB10_A2UI, GL_BGRA_INTEGER, GL_UNSIGNED_INT_2_10_10_10_REV, NO_SWIZZLE, view_class_32 },
+  { VIRGL_FORMAT_R10G10B10X2_UNORM, GL_RGB10_A2, GL_RGBA, GL_UNSIGNED_INT_2_10_10_10_REV, RGB1_SWIZZLE, view_class_32 },
+  { VIRGL_FORMAT_R10G10B10A2_UNORM, GL_RGB10_A2, GL_RGBA, GL_UNSIGNED_INT_2_10_10_10_REV, NO_SWIZZLE, view_class_32 },
+  { VIRGL_FORMAT_R10G10B10A2_UINT, GL_RGB10_A2UI, GL_RGBA_INTEGER, GL_UNSIGNED_INT_2_10_10_10_REV, NO_SWIZZLE, view_class_32 },
+};
+
+static struct vrend_format_table gl_bit10_formats[] = {
+   { VIRGL_FORMAT_B10G10R10X2_UNORM, GL_RGB10_A2, GL_BGRA, GL_UNSIGNED_INT_2_10_10_10_REV, RGB1_SWIZZLE, view_class_32 },
+   { VIRGL_FORMAT_B10G10R10A2_UNORM, GL_RGB10_A2, GL_BGRA, GL_UNSIGNED_INT_2_10_10_10_REV, NO_SWIZZLE, view_class_32 },
+};
+
+static struct vrend_format_table gles_bit10_formats[] = {
+   { VIRGL_FORMAT_B10G10R10X2_UNORM, GL_RGB10_A2, GL_RGBA, GL_UNSIGNED_INT_2_10_10_10_REV, RGB1_SWIZZLE, view_class_32 },
+   { VIRGL_FORMAT_B10G10R10A2_UNORM, GL_RGB10_A2, GL_RGBA, GL_UNSIGNED_INT_2_10_10_10_REV, NO_SWIZZLE, view_class_32 },
 };
 
 static struct vrend_format_table packed_float_formats[] = {
-  { VIRGL_FORMAT_R11G11B10_FLOAT, GL_R11F_G11F_B10F, GL_RGB, GL_UNSIGNED_INT_10F_11F_11F_REV, NO_SWIZZLE },
+  { VIRGL_FORMAT_R11G11B10_FLOAT, GL_R11F_G11F_B10F, GL_RGB, GL_UNSIGNED_INT_10F_11F_11F_REV, NO_SWIZZLE, view_class_32 },
 };
 
 static struct vrend_format_table exponent_float_formats[] = {
-  { VIRGL_FORMAT_R9G9B9E5_FLOAT, GL_RGB9_E5, GL_RGB, GL_UNSIGNED_INT_5_9_9_9_REV, NO_SWIZZLE },
+  { VIRGL_FORMAT_R9G9B9E5_FLOAT, GL_RGB9_E5, GL_RGB, GL_UNSIGNED_INT_5_9_9_9_REV, NO_SWIZZLE, view_class_32 },
 };
 
 static struct vrend_format_table bptc_formats[] = {
-   { VIRGL_FORMAT_BPTC_RGBA_UNORM, GL_COMPRESSED_RGBA_BPTC_UNORM, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-   { VIRGL_FORMAT_BPTC_SRGBA, GL_COMPRESSED_SRGB_ALPHA_BPTC_UNORM, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-   { VIRGL_FORMAT_BPTC_RGB_FLOAT, GL_COMPRESSED_RGB_BPTC_SIGNED_FLOAT, GL_RGB, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-   { VIRGL_FORMAT_BPTC_RGB_UFLOAT, GL_COMPRESSED_RGB_BPTC_UNSIGNED_FLOAT, GL_RGB, GL_UNSIGNED_BYTE, NO_SWIZZLE },
+   { VIRGL_FORMAT_BPTC_RGBA_UNORM, GL_COMPRESSED_RGBA_BPTC_UNORM, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_bptc_unorm },
+   { VIRGL_FORMAT_BPTC_SRGBA, GL_COMPRESSED_SRGB_ALPHA_BPTC_UNORM, GL_RGBA, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_bptc_unorm },
+   { VIRGL_FORMAT_BPTC_RGB_FLOAT, GL_COMPRESSED_RGB_BPTC_SIGNED_FLOAT, GL_RGB, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_bptc_float },
+   { VIRGL_FORMAT_BPTC_RGB_UFLOAT, GL_COMPRESSED_RGB_BPTC_UNSIGNED_FLOAT, GL_RGB, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_bptc_float },
 };
 
 static struct vrend_format_table gl_bgra_formats[] = {
-  { VIRGL_FORMAT_B8G8R8X8_UNORM, GL_RGBA8, GL_BGRA, GL_UNSIGNED_BYTE, RGB1_SWIZZLE },
-  { VIRGL_FORMAT_B8G8R8A8_UNORM, GL_RGBA8, GL_BGRA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
-  { VIRGL_FORMAT_B8G8R8X8_SRGB, GL_SRGB8_ALPHA8, GL_BGRA, GL_UNSIGNED_BYTE, RGB1_SWIZZLE },
-  { VIRGL_FORMAT_B8G8R8A8_SRGB, GL_SRGB8_ALPHA8, GL_BGRA, GL_UNSIGNED_BYTE, NO_SWIZZLE },
+  { VIRGL_FORMAT_B8G8R8X8_UNORM, GL_RGBA8, GL_BGRA, GL_UNSIGNED_BYTE, RGB1_SWIZZLE, view_class_32 },
+  { VIRGL_FORMAT_B8G8R8A8_UNORM, GL_RGBA8, GL_BGRA, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_32 },
+  { VIRGL_FORMAT_B8G8R8X8_SRGB, GL_SRGB8_ALPHA8, GL_BGRA, GL_UNSIGNED_BYTE, RGB1_SWIZZLE, view_class_32 },
+  { VIRGL_FORMAT_B8G8R8A8_SRGB, GL_SRGB8_ALPHA8, GL_BGRA, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_32 },
 };
 
 static struct vrend_format_table gles_bgra_formats[] = {
-  { VIRGL_FORMAT_B8G8R8X8_UNORM, GL_RGBA8,        GL_RGBA,     GL_UNSIGNED_BYTE, RGB1_SWIZZLE },
-  { VIRGL_FORMAT_B8G8R8A8_UNORM, GL_RGBA8,        GL_RGBA,     GL_UNSIGNED_BYTE, NO_SWIZZLE },
-  { VIRGL_FORMAT_B8G8R8X8_SRGB,  GL_SRGB8_ALPHA8, GL_RGBA,     GL_UNSIGNED_BYTE, RGB1_SWIZZLE },
-  { VIRGL_FORMAT_B8G8R8A8_SRGB,  GL_SRGB8_ALPHA8, GL_RGBA,     GL_UNSIGNED_BYTE, NO_SWIZZLE },
-};
-
-
-
-static struct vrend_format_table gles_z32_format[] = {
-  { VIRGL_FORMAT_Z32_UNORM, GL_DEPTH_COMPONENT24, GL_DEPTH_COMPONENT, GL_UNSIGNED_INT, NO_SWIZZLE },
-};
-
-static struct vrend_format_table gles_bit10_formats[] = {
-  { VIRGL_FORMAT_B10G10R10X2_UNORM, GL_RGB10_A2, GL_RGBA, GL_UNSIGNED_INT_2_10_10_10_REV, RGB1_SWIZZLE },
-  { VIRGL_FORMAT_B10G10R10A2_UNORM, GL_RGB10_A2, GL_RGBA, GL_UNSIGNED_INT_2_10_10_10_REV, NO_SWIZZLE },
+  { VIRGL_FORMAT_B8G8R8X8_UNORM, GL_RGBA8,        GL_RGBA,     GL_UNSIGNED_BYTE, RGB1_SWIZZLE, view_class_32 },
+  { VIRGL_FORMAT_B8G8R8A8_UNORM, GL_RGBA8,        GL_RGBA,     GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_32 },
+  { VIRGL_FORMAT_B8G8R8X8_SRGB,  GL_SRGB8_ALPHA8, GL_RGBA,     GL_UNSIGNED_BYTE, RGB1_SWIZZLE, view_class_32 },
+  { VIRGL_FORMAT_B8G8R8A8_SRGB,  GL_SRGB8_ALPHA8, GL_RGBA,     GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_32 },
 };
 
 static bool color_format_can_readback(struct vrend_format_table *virgl_format, int gles_ver)
@@ -479,12 +475,13 @@ static void vrend_add_formats(struct vrend_format_table *table, int num_entries)
     }
 
     if (table[i].format < VIRGL_FORMAT_MAX  && util_format_is_depth_or_stencil(table[i].format)) {
+      const struct util_format_description *desc = util_format_description(table[i].format);
       GLenum attachment;
 
-      if (table[i].format == VIRGL_FORMAT_Z24X8_UNORM || table[i].format == VIRGL_FORMAT_Z32_UNORM || table[i].format == VIRGL_FORMAT_Z16_UNORM || table[i].format == VIRGL_FORMAT_Z32_FLOAT)
-        attachment = GL_DEPTH_ATTACHMENT;
-      else
+      if (util_format_has_stencil(desc))
         attachment = GL_DEPTH_STENCIL_ATTACHMENT;
+      else
+        attachment = GL_DEPTH_ATTACHMENT;
       glFramebufferTexture2D(GL_FRAMEBUFFER, attachment, GL_TEXTURE_2D, tex_id, 0);
 
       is_depth = true;
@@ -591,11 +588,13 @@ void vrend_build_format_list_common(void)
 
 void vrend_build_format_list_gl(void)
 {
+  add_formats(gl_z32_format);
   /* GL_BGRA formats aren't as well supported in GLES as in GL, specially in
    * transfer operations. So we only register support for it in GL.
    */
   add_formats(gl_base_rgba_formats);
   add_formats(gl_bgra_formats);
+  add_formats(gl_bit10_formats);
 }
 
 void vrend_build_format_list_gles(void)
@@ -709,56 +708,191 @@ unsigned vrend_renderer_query_multisample_caps(unsigned max_samples, struct virg
    GLuint fbo;
    GLenum status;
 
-   uint max_samples_confirmed = 1;
-   uint test_num_samples[4] = {2,4,8,16};
+   unsigned max_samples_confirmed = 1;
+   unsigned test_num_samples[4] = {2,4,8,16};
    int out_buf_offsets[4] = {0,1,2,4};
    int lowest_working_ms_count_idx = -1;
 
    assert(glGetError() == GL_NO_ERROR &&
           "Stale error state detected, please check for failures in initialization");
 
+   /* glTexStorage2DMultisample availability check with graceful downgrade:
+    * 
+    * glTexStorage2DMultisample requires:
+    *   - OpenGL 4.3+ or GL_ARB_texture_storage_multisample (desktop GL)
+    *   - OpenGL ES 3.1+ (mobile/ANGLE)
+    * 
+    * Fallback alternatives available on older versions:
+    *   - glTexImage2DMultisample: GL 3.2+ / ES 3.1+ (works on GL 4.1 Core)
+    *   - glRenderbufferStorageMultisample: GL 3.0+ / ES 3.0+ (works on ANGLE)
+    * 
+    * We'll use glTexStorage2DMultisample if available, otherwise fall back to
+    * glTexImage2DMultisample for proper MSAA capability testing. */
+   
+   const char *renderer = (const char *)glGetString(GL_RENDERER);
+   const char *version_str = (const char *)glGetString(GL_VERSION);
+   
+   /* Check multisample function availability with multiple fallback options */
+   bool has_tex_storage_ms = false;
+   bool has_tex_image_ms = false;
+   bool has_rbo_storage_ms = false;
+   
+   if (epoxy_is_desktop_gl()) {
+      /* Desktop OpenGL path */
+      if (epoxy_gl_version() >= 43) {
+         has_tex_storage_ms = true;
+      } else if (epoxy_has_gl_extension("GL_ARB_texture_storage_multisample")) {
+         has_tex_storage_ms = true;
+      }
+      /* glTexImage2DMultisample available since GL 3.2 */
+      if (epoxy_gl_version() >= 32) {
+         has_tex_image_ms = true;
+      }
+      /* glRenderbufferStorageMultisample available since GL 3.0 */
+      if (epoxy_gl_version() >= 30) {
+         has_rbo_storage_ms = true;
+      }
+   } else {
+      /* OpenGL ES path */
+      if (epoxy_gl_version() >= 31) {
+         has_tex_storage_ms = true;
+         has_tex_image_ms = true;
+      }
+      /* glRenderbufferStorageMultisample available since ES 3.0 (ANGLE/Metal) */
+      if (epoxy_gl_version() >= 30) {
+         has_rbo_storage_ms = true;
+      }
+   }
+   
+   /* If no multisample functions available at all, disable MSAA */
+   if (!has_tex_storage_ms && !has_tex_image_ms && !has_rbo_storage_ms) {
+      fprintf(stderr, "[VREND FORMATS] No multisample functions available "
+                      "(GL version: %s, renderer: %s, is_desktop: %d). "
+                      "Disabling MSAA support.\n", 
+                      version_str ? version_str : "unknown",
+                      renderer ? renderer : "unknown",
+                      epoxy_is_desktop_gl());
+      memset(caps->sample_locations, 0, 8 * sizeof(uint32_t));
+      return 0;  /* Return 0 to indicate MSAA not supported */
+   }
+   
+   /* Log which multisample method we're using */
+   if (has_tex_storage_ms) {
+      fprintf(stderr, "[VREND FORMATS] Testing MSAA with glTexStorage2DMultisample\n");
+   } else if (has_tex_image_ms) {
+      fprintf(stderr, "[VREND FORMATS] Testing MSAA with glTexImage2DMultisample fallback\n");
+   } else if (has_rbo_storage_ms) {
+      fprintf(stderr, "[VREND FORMATS] Testing MSAA with glRenderbufferStorageMultisample fallback "
+                      "(GL version: %s, renderer: %s)\n",
+                      version_str ? version_str : "unknown",
+                      renderer ? renderer : "unknown");
+   }
+
+   fprintf(stderr, "[VREND FORMATS] Starting MSAA capability test with max_samples=%u\n", max_samples);
+   
    glGenFramebuffers( 1, &fbo );
    memset(caps->sample_locations, 0, 8 * sizeof(uint32_t));
 
    for (int i = 3; i >= 0; i--) {
-      if (test_num_samples[i] > max_samples)
+      if (test_num_samples[i] > max_samples) {
+         fprintf(stderr, "[VREND FORMATS] Skipping %u samples (exceeds max %u)\n", 
+                 test_num_samples[i], max_samples);
          continue;
-      glGenTextures(1, &tex);
-      glBindTexture(GL_TEXTURE_2D_MULTISAMPLE, tex);
-      glTexStorage2DMultisample(GL_TEXTURE_2D_MULTISAMPLE, test_num_samples[i], GL_RGBA32F, 64, 64, GL_TRUE);
+      }
+      
+      fprintf(stderr, "[VREND FORMATS] Testing %u samples...\n", test_num_samples[i]);
+      
+      /* Clear any stale errors before testing */
+      while (glGetError() != GL_NO_ERROR);
+      
+      if (has_tex_storage_ms || has_tex_image_ms) {
+         /* Texture-based MSAA testing - use GL_RGBA8 for better compatibility */
+         glGenTextures(1, &tex);
+         GLenum err1 = glGetError();
+         
+         glBindTexture(GL_TEXTURE_2D_MULTISAMPLE, tex);
+         GLenum err2 = glGetError();
+         
+         if (has_tex_storage_ms) {
+            glTexStorage2DMultisample(GL_TEXTURE_2D_MULTISAMPLE, test_num_samples[i], GL_RGBA8, 64, 64, GL_TRUE);
+         } else {
+            glTexImage2DMultisample(GL_TEXTURE_2D_MULTISAMPLE, test_num_samples[i], GL_RGBA8, 64, 64, GL_TRUE);
+         }
+         GLenum err3 = glGetError();
+         
+         if (err1 != GL_NO_ERROR || err2 != GL_NO_ERROR || err3 != GL_NO_ERROR) {
+            fprintf(stderr, "[VREND FORMATS]   glGenTextures err=0x%x, glBindTexture err=0x%x, glTex*Multisample err=0x%x\n",
+                    err1, err2, err3);
+         }
+      } else {
+         /* Renderbuffer-based MSAA testing (fallback for ES 3.0 / ANGLE) */
+         GLuint rbo;
+         glGenRenderbuffers(1, &rbo);
+         glBindRenderbuffer(GL_RENDERBUFFER, rbo);
+         glRenderbufferStorageMultisample(GL_RENDERBUFFER, test_num_samples[i], GL_RGBA8, 64, 64);
+         tex = rbo;  /* Store RBO handle in tex variable for cleanup */
+      }
+      
       status = glGetError();
       if (status == GL_NO_ERROR) {
          glBindFramebuffer(GL_FRAMEBUFFER, fbo);
-         glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D_MULTISAMPLE, tex, 0);
+         
+         if (has_tex_storage_ms || has_tex_image_ms) {
+            glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D_MULTISAMPLE, tex, 0);
+         } else {
+            /* For renderbuffer fallback */
+            glFramebufferRenderbuffer(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_RENDERBUFFER, tex);
+         }
+         
          status = glCheckFramebufferStatus(GL_FRAMEBUFFER);
          if (status == GL_FRAMEBUFFER_COMPLETE) {
+            fprintf(stderr, "[VREND FORMATS]  %u samples COMPLETE\n", test_num_samples[i]);
             if (max_samples_confirmed < test_num_samples[i])
                max_samples_confirmed = test_num_samples[i];
 
-            for (uint k = 0; k < test_num_samples[i]; ++k) {
-               float msp[2];
-               uint32_t compressed;
-               glGetMultisamplefv(GL_SAMPLE_POSITION, k, msp);
-               compressed = ((unsigned)(floor(msp[0] * 16.0f)) & 0xf) << 4;
-               compressed |= ((unsigned)(floor(msp[1] * 16.0f)) & 0xf);
-               caps->sample_locations[out_buf_offsets[i] + (k >> 2)] |= compressed  << (8 * (k & 3));
+            /* glGetMultisamplefv only available in desktop GL (since 3.2), not in GL ES */
+            if (epoxy_is_desktop_gl()) {
+               for (unsigned k = 0; k < test_num_samples[i]; ++k) {
+                  float msp[2];
+                  uint32_t compressed;
+                  glGetMultisamplefv(GL_SAMPLE_POSITION, k, msp);
+                  compressed = ((unsigned)(floor(msp[0] * 16.0f)) & 0xf) << 4;
+                  compressed |= ((unsigned)(floor(msp[1] * 16.0f)) & 0xf);
+                  caps->sample_locations[out_buf_offsets[i] + (k >> 2)] |= compressed  << (8 * (k & 3));
+               }
+            } else {
+               /* OpenGL ES: sample locations not available, leave them zero-initialized */
+               fprintf(stderr, "[VREND FORMATS]   (OpenGL ES: sample locations not available)\n");
             }
             lowest_working_ms_count_idx = i;
          } else {
+            fprintf(stderr, "[VREND FORMATS]  %u samples INCOMPLETE (status=0x%x)\n", 
+                    test_num_samples[i], status);
             /* If a framebuffer doesn't support low sample counts,
              * use the sample position from the last working larger count. */
             if (lowest_working_ms_count_idx > 0) {
-               for (uint k = 0; k < test_num_samples[i]; ++k) {
+               for (unsigned k = 0; k < test_num_samples[i]; ++k) {
                   caps->sample_locations[out_buf_offsets[i] + (k >> 2)] =
-                        caps->sample_locations[out_buf_offsets[lowest_working_ms_count_idx]  + (k >> 2)];
+                     caps->sample_locations[out_buf_offsets[lowest_working_ms_count_idx]  + (k >> 2)];
                }
             }
          }
          glBindFramebuffer(GL_FRAMEBUFFER, 0);
+      } else {
+         fprintf(stderr, "[VREND FORMATS]  %u samples GL_ERROR=0x%x\n", test_num_samples[i], status);
+      }
+      
+      /* Cleanup - delete texture or renderbuffer */
+      if (has_tex_storage_ms || has_tex_image_ms) {
+         glDeleteTextures(1, &tex);
+      } else {
+         glDeleteRenderbuffers(1, &tex);
       }
-      glDeleteTextures(1, &tex);
    }
    glDeleteFramebuffers(1, &fbo);
+   
+   fprintf(stderr, "[VREND FORMATS] MSAA test complete: returning max_samples_confirmed=%u\n", 
+           max_samples_confirmed);
    return max_samples_confirmed;
 }
 
@@ -853,7 +987,7 @@ static int format_uncompressed_compressed_copy_compatible(enum virgl_formats src
    }
 }
 
-static boolean format_compressed_compressed_copy_compatible(enum virgl_formats src, enum virgl_formats dst)
+static bool format_compressed_compressed_copy_compatible(enum virgl_formats src, enum virgl_formats dst)
 {
    const bool is_desktop_gl = epoxy_is_desktop_gl();
 
@@ -889,8 +1023,8 @@ static boolean format_compressed_compressed_copy_compatible(enum virgl_formats s
    return false;
 }
 
-boolean format_is_copy_compatible(enum virgl_formats src, enum virgl_formats dst,
-                                  unsigned int flags)
+bool format_is_copy_compatible(enum virgl_formats src, enum virgl_formats dst,
+                               unsigned int flags)
 {
    int r;
 
diff --git a/src/vrend_renderer.c b/src/vrend_renderer.c
index 03d2480..79f552b 100644
--- a/src/vrend_renderer.c
+++ b/src/vrend_renderer.c
@@ -28,6 +28,7 @@
 #include <unistd.h>
 #include <stdatomic.h>
 #include <stdio.h>
+#include <string.h>
 #include <errno.h>
 #include "pipe/p_shader_tokens.h"
 
@@ -42,6 +43,7 @@
 
 #include "util/u_thread.h"
 #include "util/u_format.h"
+#include "tgsi/tgsi_dump.h"
 #include "tgsi/tgsi_parse.h"
 
 #include "vrend_object.h"
@@ -58,8 +60,9 @@
 #include "virgl_hw.h"
 #include "virgl_resource.h"
 #include "virglrenderer.h"
-#include "virglrenderer_hw.h"
 #include "virgl_protocol.h"
+#include "virgl_fence.h"
+#include "virtgpu_drm.h"
 
 #include "tgsi/tgsi_text.h"
 
@@ -68,7 +71,7 @@
 #endif
 
 #ifdef ENABLE_VIDEO
-#include <vrend_video.h>
+#include "vrend_video.h"
 #endif
 
 #ifdef WIN32
@@ -76,10 +79,14 @@
 #endif
 
 /*
- * VIRGL_RENDERER_CAPSET_VIRGL has version 0 and 1, but they are both
+ * VIRTGPU_DRM_CAPSET_VIRGL has version 0 and 1, but they are both
  * virgl_caps_v1 and are exactly the same.
+bool vrend_renderer_video_available(void)
+{
+   return vrend_state.video_available;
+}
  *
- * VIRGL_RENDERER_CAPSET_VIRGL2 has version 0, 1, and 2, but they are
+ * VIRTGPU_DRM_CAPSET_VIRGL2 has version 0, 1, and 2, but they are
  * all virgl_caps_v2 and are exactly the same.
  *
  * Since virgl_caps_v2 is growable and no backward-incompatible change is
@@ -205,6 +212,9 @@ enum features_id
    feat_texture_barrier,
    feat_texture_buffer_range,
    feat_texture_gather,
+   feat_texture_mirror_clamp_to_edge,
+   feat_texture_mirror_clamp,
+   feat_texture_mirror_clamp_to_border,
    feat_texture_multisample,
    feat_texture_query_lod,
    feat_texture_shadow_lod,
@@ -260,7 +270,7 @@ static const  struct {
    FEAT(cull_distance, 45, UNAVAIL, "GL_ARB_cull_distance", "GL_EXT_clip_cull_distance" ),
    FEAT(debug_cb, UNAVAIL, UNAVAIL, NULL), /* special case */
    FEAT(draw_instance, 31, 30,  "GL_ARB_draw_instanced" ),
-   FEAT(draw_parameters, 46, 0, "ARB_shader_draw_parameters"),
+   FEAT(draw_parameters, 46, UNAVAIL, "ARB_shader_draw_parameters"),
    FEAT(dual_src_blend, 33, UNAVAIL,  "GL_ARB_blend_func_extended", "GL_EXT_blend_func_extended" ),
    FEAT(depth_clamp, 32, UNAVAIL, "GL_ARB_depth_clamp", "GL_EXT_depth_clamp", "GL_NV_depth_clamp"),
    FEAT(enhanced_layouts, 44, UNAVAIL, "GL_ARB_enhanced_layouts"),
@@ -315,7 +325,10 @@ static const  struct {
    FEAT(texture_barrier, 45, UNAVAIL,  "GL_ARB_texture_barrier" ),
    FEAT(texture_buffer_range, 43, 32,  "GL_ARB_texture_buffer_range" ),
    FEAT(texture_gather, 40, 31,  "GL_ARB_texture_gather" ),
-   FEAT(texture_multisample, 32, 31,  "GL_ARB_texture_multisample" ),
+   FEAT(texture_mirror_clamp_to_edge, UNAVAIL, UNAVAIL, "GL_ATI_texture_mirror_once", "GL_EXT_texture_mirror_clamp", "GL_ARB_texture_mirror_clamp_to_edge", "GL_EXT_texture_mirror_clamp_to_edge"),
+   FEAT(texture_mirror_clamp, UNAVAIL, UNAVAIL, "GL_ATI_texture_mirror_once", "GL_EXT_texture_mirror_clamp"),
+   FEAT(texture_mirror_clamp_to_border, UNAVAIL, UNAVAIL, "GL_EXT_texture_mirror_clamp"),
+   FEAT(texture_multisample, 32, 30,  "GL_ARB_texture_multisample" ),
    FEAT(texture_query_lod, 40, UNAVAIL, "GL_ARB_texture_query_lod", "GL_EXT_texture_query_lod"),
    FEAT(texture_shadow_lod, UNAVAIL, UNAVAIL, "GL_EXT_texture_shadow_lod"),
    FEAT(texture_srgb_decode, UNAVAIL, UNAVAIL,  "GL_EXT_texture_sRGB_decode" ),
@@ -370,6 +383,8 @@ struct global_renderer_state {
    uint32_t max_texture_3d_size;
    uint32_t max_texture_cube_size;
    uint32_t max_shader_patch_varyings;
+   uint32_t max_vertex_attributes;
+   uint32_t max_texture_units;
 
    /* inferred GL caching type */
    uint32_t inferred_gl_caching_type;
@@ -392,6 +407,9 @@ struct global_renderer_state {
    bool use_egl_fence : 1;
 #endif
    bool d3d_share_texture : 1;
+
+   /* host-side video acceleration availability */
+   bool video_available;
 };
 
 struct sysval_uniform_block {
@@ -447,19 +465,23 @@ struct vrend_linked_shader_program {
 
    uint32_t ubo_used_mask[PIPE_SHADER_TYPES];
    uint32_t samplers_used_mask[PIPE_SHADER_TYPES];
+   // a subset of samplers_used_mask
+   uint32_t shadow_samp_mask[PIPE_SHADER_TYPES];
 
+   // compact arrays of uniform locations for each set bit in samplers_used_mask
+   GLuint *sampler_locs[PIPE_SHADER_TYPES];
+   // ignore elements corresponding to unset bits of shadow_samp_mask
    GLuint *shadow_samp_mask_locs[PIPE_SHADER_TYPES];
    GLuint *shadow_samp_add_locs[PIPE_SHADER_TYPES];
 
    GLint const_location[PIPE_SHADER_TYPES];
 
    GLuint *attrib_locs;
-   uint32_t shadow_samp_mask[PIPE_SHADER_TYPES];
 
    GLuint separate_virgl_block_id[PIPE_SHADER_TYPES];
-   GLint virgl_block_bind;
+   GLuint virgl_block_bind;
    uint32_t sysvalue_data_cookie;
-   GLint ubo_sysval_buffer_id;
+   GLuint ubo_sysval_buffer_id;
 
    uint32_t images_used_mask[PIPE_SHADER_TYPES];
    GLint *img_locs[PIPE_SHADER_TYPES];
@@ -485,7 +507,6 @@ struct vrend_shader {
    GLuint id;
    GLuint program_id; /* only used for separable shaders */
    GLuint last_pipeline_id;
-   uint32_t uid;
    bool is_compiled;
    bool is_linked; /* only used for separable shaders */
    struct vrend_shader_key key;
@@ -502,9 +523,15 @@ struct vrend_shader_selector {
    struct tgsi_token *tokens;
 
    uint32_t req_local_mem;
+};
+
+struct vrend_long_shader_buffer {
+   uint32_t handle;
+   struct vrend_shader_selector *sel;
+
    char *tmp_buf;
-   uint32_t buf_len;
-   uint32_t buf_offset;
+   uint32_t total_length;
+   uint32_t current_length;
 };
 
 struct vrend_texture {
@@ -517,10 +544,11 @@ struct vrend_texture {
 
 struct vrend_surface {
    struct pipe_reference reference;
-   GLuint id;
-   GLuint res_handle;
+   GLuint gl_id;
    GLuint format;
-   GLuint val0, val1;
+   GLuint level;
+   GLuint first_layer;
+   GLuint last_layer;
    GLuint nr_samples;
    struct vrend_resource *texture;
 };
@@ -547,10 +575,21 @@ struct vrend_so_target {
 
 struct vrend_sampler_view {
    struct pipe_reference reference;
-   GLuint id;
+   GLuint gl_id;
    enum virgl_formats format;
    GLenum target;
-   GLuint val0, val1;
+   union {
+       struct {
+           GLuint first_layer:16;     /**< first layer to use for array textures */
+           GLuint last_layer:16;      /**< last layer to use for array textures */
+           GLuint first_level:8;      /**< first mipmap level to use */
+           GLuint last_level:8;       /**< last mipmap level to use */
+       } tex;
+       struct {
+           GLuint first_element;
+           GLuint last_element;
+       } buf;
+   } u;
    GLint gl_swizzle[4];
    GLuint srgb_decode;
    GLuint levels;
@@ -613,11 +652,24 @@ struct vrend_constants {
    uint32_t num_allocated_consts;
 };
 
+// bound sampler view (texture) state associated with the current
+// program or pipeline stage
 struct vrend_shader_view {
-   int num_views;
+   // a possibly sparse array of sampler views, with upper limit `max_num_views`
+   // on the length. must test each element for non-null.
+   int max_num_views;
    struct vrend_sampler_view *views[PIPE_MAX_SHADER_SAMPLER_VIEWS];
-   uint32_t res_id[PIPE_MAX_SHADER_SAMPLER_VIEWS];
-   uint32_t old_ids[PIPE_MAX_SHADER_SAMPLER_VIEWS];
+   struct vrend_sampler_state *samplers[PIPE_MAX_SAMPLERS];
+
+   // for elements of `views` and 'samplers' indicated by set dirty bits:
+   // texture-to-texture-unit binding is needed before next draw
+   uint32_t dirty_mask;
+
+   // only used for gles host contexts to emulate textureQueryLevels().
+   // compact array corresponding only to valid elements of `views`, with length
+   // equal to `num_used_views`
+   int32_t texture_levels[PIPE_MAX_SHADER_SAMPLER_VIEWS];
+   uint32_t num_used_views;
 };
 
 struct vrend_viewport {
@@ -673,7 +725,6 @@ struct vrend_sub_context {
    struct vrend_vertex_buffer vbo[PIPE_MAX_ATTRIBS];
 
    struct pipe_index_buffer ib;
-   uint32_t index_buffer_res_id;
 
    bool vbo_dirty;
    bool shader_dirty;
@@ -682,7 +733,7 @@ struct vrend_sub_context {
    bool image_state_dirty;
    bool blend_state_dirty;
 
-   uint32_t long_shader_in_progress_handle[PIPE_SHADER_TYPES];
+   struct vrend_long_shader_buffer *long_shader_in_progress[PIPE_SHADER_TYPES];
    struct vrend_shader_selector *shaders[PIPE_SHADER_TYPES];
    struct vrend_linked_shader_program *prog;
 
@@ -691,20 +742,13 @@ struct vrend_sub_context {
 
    struct vrend_constants consts[PIPE_SHADER_TYPES];
    bool const_dirty[PIPE_SHADER_TYPES];
-   struct vrend_sampler_state *sampler_state[PIPE_SHADER_TYPES][PIPE_MAX_SAMPLERS];
 
    struct pipe_constant_buffer cbs[PIPE_SHADER_TYPES][PIPE_MAX_CONSTANT_BUFFERS];
    uint32_t const_bufs_used_mask[PIPE_SHADER_TYPES];
    uint32_t const_bufs_dirty[PIPE_SHADER_TYPES];
 
-   int num_sampler_states[PIPE_SHADER_TYPES];
-
-   uint32_t sampler_views_dirty[PIPE_SHADER_TYPES];
-   int32_t texture_levels[PIPE_SHADER_TYPES][PIPE_MAX_SAMPLERS];
-   int32_t n_samplers[PIPE_SHADER_TYPES];
-
    uint32_t fb_id;
-   int nr_cbufs;
+   uint32_t nr_cbufs;
    struct vrend_surface *zsurf;
    struct vrend_surface *surf[PIPE_MAX_COLOR_BUFS];
 
@@ -741,10 +785,6 @@ struct vrend_sub_context {
 
    int last_shader_idx;
 
-   GLint draw_indirect_buffer;
-
-   GLint draw_indirect_params_buffer;
-
    struct pipe_rasterizer_state hw_rs_state;
    struct pipe_blend_state hw_blend_state;
 
@@ -840,6 +880,7 @@ struct vrend_context {
 #endif
 };
 
+static int get_glsl_version(void);
 static void vrend_pause_render_condition(struct vrend_context *ctx, bool pause);
 static void vrend_update_viewport_state(struct vrend_sub_context *sub_ctx);
 static void vrend_update_scissor_state(struct vrend_sub_context *sub_ctx);
@@ -847,13 +888,14 @@ static void vrend_destroy_query_object(void *obj_ptr);
 static void vrend_finish_context_switch(struct vrend_context *ctx);
 static void vrend_patch_blend_state(struct vrend_sub_context *sub_ctx);
 static void vrend_update_frontface_state(struct vrend_sub_context *ctx);
-static int vrender_get_glsl_version(void);
 static void vrend_destroy_program(struct vrend_linked_shader_program *ent);
 static void vrend_apply_sampler_state(struct vrend_sub_context *sub_ctx,
                                       struct vrend_resource *res,
-                                      uint32_t shader_type,
-                                      int id, int sampler_id,
+                                      struct vrend_sampler_state *sampler_state,
+                                      GLuint sampler_id,
                                       struct vrend_sampler_view *tview);
+static void vrend_object_bind_dsa_to_sub_context(struct vrend_sub_context *sub_ctx,
+                                                 uint32_t handle);
 static GLenum tgsitargettogltarget(const enum pipe_texture_target target, int nr_samples);
 
 void vrend_update_stencil_state(struct vrend_sub_context *sub_ctx);
@@ -867,7 +909,7 @@ static inline bool vrend_format_can_sample(enum virgl_formats format)
    if (tex_conv_table[format].bindings & VIRGL_BIND_SAMPLER_VIEW)
       return true;
 
-#ifdef ENABLE_MINIGBM_ALLOCATION
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
    uint32_t gbm_format = 0;
    if (virgl_gbm_convert_format(&format, &gbm_format))
       return false;
@@ -904,7 +946,7 @@ static inline bool vrend_format_is_ds(enum virgl_formats format)
 
 static inline bool vrend_format_can_scanout(enum virgl_formats format)
 {
-#ifdef ENABLE_MINIGBM_ALLOCATION
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
    uint32_t gbm_format = 0;
    if (virgl_gbm_convert_format(&format, &gbm_format))
       return false;
@@ -919,7 +961,7 @@ static inline bool vrend_format_can_scanout(enum virgl_formats format)
 #endif
 }
 
-#ifdef ENABLE_MINIGBM_ALLOCATION
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
 static inline bool vrend_format_can_texture_view(enum virgl_formats format)
 {
    return has_feature(feat_texture_view) &&
@@ -947,12 +989,29 @@ bool vrend_format_is_bgra(enum virgl_formats format) {
            format == VIRGL_FORMAT_B8G8R8A8_SRGB);
 }
 
-static bool vrend_resource_has_24bpp_internal_format(const struct vrend_resource *res)
+static GLuint vrend_resource_get_internal_format_override(const struct vrend_resource *res)
 {
-   /* Some shared resources imported to guest mesa as EGL images occupy 24bpp instead of more common 32bpp. */
-   return (has_bit(res->storage_bits, VREND_STORAGE_EGL_IMAGE) &&
-           (res->base.format == VIRGL_FORMAT_B8G8R8X8_UNORM ||
-            res->base.format == VIRGL_FORMAT_R8G8B8X8_UNORM));
+   /* Some shared resources imported to guest mesa as EGL images occupy 24bpp instead of more common 32bpp.
+    *
+    * TODO: perhaps this can be generalized to all alpha-less formats?
+    */
+   const enum virgl_formats format = res->base.format;
+   if (has_bit(res->storage_bits, VREND_STORAGE_EGL_IMAGE)) {
+      switch (format) {
+      case VIRGL_FORMAT_B8G8R8X8_UNORM:
+      case VIRGL_FORMAT_B8G8R8X8_SRGB:
+      case VIRGL_FORMAT_R8G8B8X8_UNORM:
+      case VIRGL_FORMAT_R8G8B8X8_SRGB:
+         return GL_RGB8;
+      case VIRGL_FORMAT_R16G16B16X16_FLOAT:
+         return GL_RGB16F;
+      case VIRGL_FORMAT_R16G16B16X16_UNORM:
+         return GL_RGB16;
+      default:
+         ;
+      }
+   }
+   return GL_NONE;
 }
 
 static bool vrend_resource_supports_view(const struct vrend_resource *res,
@@ -968,7 +1027,7 @@ static bool vrend_resource_supports_view(const struct vrend_resource *res,
     * decode/encode is required. */
    return !(vrend_format_is_bgra(res->base.format) &&
             has_bit(res->storage_bits, VREND_STORAGE_EGL_IMAGE)) &&
-         !vrend_resource_has_24bpp_internal_format(res);
+         (vrend_resource_get_internal_format_override(res) == GL_NONE);
 }
 
 static inline bool
@@ -1062,6 +1121,12 @@ static const char *vrend_ctx_error_strings[] = {
    [VIRGL_ERROR_CTX_ILLEGAL_DUAL_SRC_BLEND]= "Dual source blend not supported",
    [VIRGL_ERROR_CTX_UNSUPPORTED_FUNCTION]  = "Unsupported host function called",
    [VIRGL_ERROR_CTX_ILLEGAL_PROGRAM_PIPELINE] = "Illegal shader program pipeline",
+   [VIRGL_ERROR_CTX_TOO_MANY_VERTEX_ATTRIBUTES] = "Too many vertex attributes are requested",
+   [VIRGL_ERROR_CTX_UNSUPPORTED_TEX_WRAP] = "Unsupported texture mirror wrapping, default to GL_MIRROR_REPEAT",
+   [VIRGL_ERROR_CTX_CUBE_MAP_FACE_OUT_OF_RANGE] = "Cube map face out of range:",
+   [VIRGL_ERROR_CTX_BLIT_AREA_OUT_OF_RANGE] = "Blit z-slices out of range;",
+   [VIRGL_ERROR_CTX_SSBO_BINDING_RANGE] = "SSBO binding out of range for resource",
+   [VIRGL_ERROR_CTX_RESOURCE_OUT_OF_RANGE] = "Resource copy out of range for resource",
 };
 
 void vrend_report_context_error_internal(const char *fname, struct vrend_context *ctx,
@@ -1069,9 +1134,9 @@ void vrend_report_context_error_internal(const char *fname, struct vrend_context
 {
    ctx->in_error = true;
    ctx->last_error = error;
-   vrend_printf("%s: context error reported %d \"%s\" %s %d\n", fname,
-                ctx->ctx_id, ctx->debug_name, vrend_ctx_error_strings[error],
-                value);
+   virgl_error("%s: context error reported %d \"%s\" %s %d\n", fname,
+               ctx->ctx_id, ctx->debug_name, vrend_ctx_error_strings[error],
+               value);
 }
 
 #define CORE_PROFILE_WARN_NONE 0
@@ -1093,9 +1158,9 @@ static const char *vrend_core_profile_warn_strings[] = {
 static void __report_core_warn(const char *fname, struct vrend_context *ctx,
                                enum virgl_ctx_errors error)
 {
-   vrend_printf("%s: core profile violation reported %d \"%s\" %s\n", fname,
-                ctx->ctx_id, ctx->debug_name,
-                vrend_core_profile_warn_strings[error]);
+   virgl_warn("%s: core profile violation reported %d \"%s\" %s\n", fname,
+              ctx->ctx_id, ctx->debug_name,
+              vrend_core_profile_warn_strings[error]);
 }
 #define report_core_warn(ctx, error) __report_core_warn(__func__, ctx, error)
 
@@ -1155,6 +1220,25 @@ static void __report_gles_missing_func(ASSERTED const char *fname,
 
 #define report_gles_missing_func(ctx, missf) __report_gles_missing_func(__func__, ctx, missf)
 
+static void buffered_logger(const char *fmt,
+                            va_list ap,
+                            void *user_data)
+{
+   struct vrend_strbuf *logger_buffer = user_data;
+   strbuf_vappendf(logger_buffer, fmt, ap);
+}
+
+static void vrend_dump_tgsi(const struct tgsi_token *tokens,
+                            unsigned flags)
+{
+   struct vrend_strbuf logger_buffer = {0};
+   strbuf_alloc(&logger_buffer, STRBUF_MIN_MALLOC);
+
+   tgsi_dump_with_logger(tokens, flags, buffered_logger, &logger_buffer);
+   virgl_debug("%s", logger_buffer.buf);
+   strbuf_free(&logger_buffer);
+}
+
 static void init_features(int gl_ver, int gles_ver)
 {
    for (enum features_id id = 0; id < feat_last; id++) {
@@ -1182,8 +1266,8 @@ static void init_features(int gl_ver, int gles_ver)
 
 static void vrend_destroy_surface(struct vrend_surface *surf)
 {
-   if (surf->id != surf->texture->id)
-      glDeleteTextures(1, &surf->id);
+   if (surf->gl_id != surf->texture->gl_id)
+      glDeleteTextures(1, &surf->gl_id);
    vrend_resource_reference(&surf->texture, NULL);
    free(surf);
 }
@@ -1193,15 +1277,15 @@ vrend_surface_reference(struct vrend_surface **ptr, struct vrend_surface *surf)
 {
    struct vrend_surface *old_surf = *ptr;
 
-   if (pipe_reference(&(*ptr)->reference, &surf->reference))
+   if (pipe_reference((struct pipe_reference *)*ptr, (struct pipe_reference *)surf))
       vrend_destroy_surface(old_surf);
    *ptr = surf;
 }
 
 static void vrend_destroy_sampler_view(struct vrend_sampler_view *samp)
 {
-   if (samp->texture->id != samp->id)
-      glDeleteTextures(1, &samp->id);
+   if (samp->texture->gl_id != samp->gl_id)
+      glDeleteTextures(1, &samp->gl_id);
    vrend_resource_reference(&samp->texture, NULL);
    free(samp);
 }
@@ -1211,7 +1295,7 @@ vrend_sampler_view_reference(struct vrend_sampler_view **ptr, struct vrend_sampl
 {
    struct vrend_sampler_view *old_view = *ptr;
 
-   if (pipe_reference(&(*ptr)->reference, &view->reference))
+   if (pipe_reference((struct pipe_reference *)*ptr, (struct pipe_reference *)view))
       vrend_destroy_sampler_view(old_view);
    *ptr = view;
 }
@@ -1227,7 +1311,7 @@ vrend_so_target_reference(struct vrend_so_target **ptr, struct vrend_so_target *
 {
    struct vrend_so_target *old_target = *ptr;
 
-   if (pipe_reference(&(*ptr)->reference, &target->reference))
+   if (pipe_reference((struct pipe_reference *)*ptr, (struct pipe_reference *)target))
       vrend_destroy_so_target(old_target);
    *ptr = target;
 }
@@ -1235,21 +1319,20 @@ vrend_so_target_reference(struct vrend_so_target **ptr, struct vrend_so_target *
 static void vrend_shader_dump(struct vrend_shader *shader)
 {
    const char *prefix = pipe_shader_to_prefix(shader->sel->type);
-   if (shader->sel->tmp_buf)
-      vrend_printf("%s: %d TGSI:\n%s\n", prefix, shader->id, shader->sel->tmp_buf);
+   if (shader->sel->tokens) {
+      virgl_debug("%s: %d TGSI:\n", prefix, shader->id);
+      vrend_dump_tgsi(shader->sel->tokens, 0);
+   }
 
-   vrend_printf("%s: %d GLSL:\n", prefix, shader->id);
+   virgl_debug("%s: %d GLSL:\n", prefix, shader->id);
    strarray_dump_with_line_numbers(&shader->glsl_strings);
-   vrend_printf("\n");
+   virgl_debug("\n");
 }
 
 static void vrend_shader_destroy(struct vrend_shader *shader)
 {
-   struct vrend_linked_shader_program *ent, *tmp;
-
-   LIST_FOR_EACH_ENTRY_SAFE(ent, tmp, &shader->programs, sl[shader->sel->type]) {
+   list_for_each_entry_safe(struct vrend_linked_shader_program, ent, &shader->programs, sl[shader->sel->type])
       vrend_destroy_program(ent);
-   }
 
    if (shader->sel->sinfo.separable_program)
        glDeleteProgram(shader->program_id);
@@ -1270,7 +1353,6 @@ static void vrend_destroy_shader_selector(struct vrend_shader_selector *sel)
    if (sel->sinfo.so_names)
       for (i = 0; i < sel->sinfo.so_info.num_outputs; i++)
          free(sel->sinfo.so_names[i]);
-   free(sel->tmp_buf);
    free(sel->sinfo.so_names);
    free(sel->sinfo.sampler_arrays);
    free(sel->sinfo.image_arrays);
@@ -1278,6 +1360,23 @@ static void vrend_destroy_shader_selector(struct vrend_shader_selector *sel)
    free(sel);
 }
 
+static inline void
+vrend_shader_state_reference(struct vrend_shader_selector **ptr, struct vrend_shader_selector *shader)
+{
+   struct vrend_shader_selector *old_shader = *ptr;
+
+   if (pipe_reference((struct pipe_reference *)*ptr, (struct pipe_reference *)shader))
+      vrend_destroy_shader_selector(old_shader);
+   *ptr = shader;
+}
+
+static void vrend_destroy_long_shader_buffer(struct vrend_long_shader_buffer *lsbuf)
+{
+   vrend_shader_state_reference(&lsbuf->sel, NULL);
+   free(lsbuf->tmp_buf);
+   free(lsbuf);
+}
+
 static inline int conv_shader_type(int type)
 {
    switch (type) {
@@ -1297,12 +1396,60 @@ static bool vrend_compile_shader(struct vrend_sub_context *sub_ctx,
 {
    GLint param;
    const char *shader_parts[SHADER_MAX_STRINGS];
-
-   for (int i = 0; i < shader->glsl_strings.num_strings; i++)
-      shader_parts[i] = shader->glsl_strings.strings[i].buf;
+   char *modified_shaders[SHADER_MAX_STRINGS] = {NULL};
+
+   /* Firefox uses GL_EXT_shader_texture_lod (GLES), but we have GL_ARB_shader_texture_lod (desktop GL).
+    * Rewrite extension directives to use the ARB version. */
+   for (int i = 0; i < shader->glsl_strings.num_strings; i++) {
+      const char *src = shader->glsl_strings.strings[i].buf;
+      const char *ext_check = strstr(src, "GL_EXT_shader_texture_lod");
+      
+      if (ext_check) {
+         /* Found GL_EXT_shader_texture_lod - replace with GL_ARB_shader_texture_lod */
+         if (getenv("VIRGL_DEBUG_SHADERS")) {
+            fprintf(stderr, "DEBUG: Rewriting shader - replacing GL_EXT_shader_texture_lod with GL_ARB_shader_texture_lod\n");
+            fflush(stderr);
+         }
+         size_t src_len = strlen(src);
+         modified_shaders[i] = malloc(src_len + 16); /* Extra space for ARB vs EXT */
+         if (modified_shaders[i]) {
+            char *dst = modified_shaders[i];
+            const char *read_pos = src;
+            
+            while ((ext_check = strstr(read_pos, "GL_EXT_shader_texture_lod")) != NULL) {
+               /* Copy up to the extension name */
+               size_t prefix_len = ext_check - read_pos;
+               memcpy(dst, read_pos, prefix_len);
+               dst += prefix_len;
+               
+               /* Write ARB version instead */
+               memcpy(dst, "GL_ARB_shader_texture_lod", 25);
+               dst += 25;
+               
+               /* Skip past the EXT version */
+               read_pos = ext_check + 25;
+            }
+            
+            /* Copy remaining string */
+            strcpy(dst, read_pos);
+            shader_parts[i] = modified_shaders[i];
+         } else {
+            shader_parts[i] = src;
+         }
+      } else {
+         shader_parts[i] = src;
+      }
+   }
 
    shader->id = glCreateShader(conv_shader_type(shader->sel->type));
    glShaderSource(shader->id, shader->glsl_strings.num_strings, shader_parts, NULL);
+   
+   /* Free temporary modified shader strings */
+   for (int i = 0; i < shader->glsl_strings.num_strings; i++) {
+      if (modified_shaders[i])
+         free(modified_shaders[i]);
+   }
+   
    glCompileShader(shader->id);
    glGetShaderiv(shader->id, GL_COMPILE_STATUS, &param);
    if (param == GL_FALSE) {
@@ -1310,7 +1457,7 @@ static bool vrend_compile_shader(struct vrend_sub_context *sub_ctx,
       int len;
       glGetShaderInfoLog(shader->id, 65536, &len, infolog);
       vrend_report_context_error(sub_ctx->parent, VIRGL_ERROR_CTX_ILLEGAL_SHADER, 0);
-      vrend_printf("shader failed to compile\n%s\n", infolog);
+      virgl_error("Shader failed to compile\n%s\n", infolog);
       vrend_shader_dump(shader);
       return false;
    }
@@ -1326,16 +1473,6 @@ static bool vrend_compile_shader(struct vrend_sub_context *sub_ctx,
    return true;
 }
 
-static inline void
-vrend_shader_state_reference(struct vrend_shader_selector **ptr, struct vrend_shader_selector *shader)
-{
-   struct vrend_shader_selector *old_shader = *ptr;
-
-   if (pipe_reference(&(*ptr)->reference, &shader->reference))
-      vrend_destroy_shader_selector(old_shader);
-   *ptr = shader;
-}
-
 void
 vrend_insert_format(struct vrend_format_table *entry, uint32_t bindings, uint32_t flags)
 {
@@ -1368,42 +1505,25 @@ static bool vrend_is_timer_query(GLenum gltype)
       gltype == GL_TIME_ELAPSED;
 }
 
-static inline void use_program(struct vrend_sub_context *sub_ctx, uint32_t id)
-{
-      if (sub_ctx->current_program_id != id) {
-         sub_ctx->current_program_id = id;
-         glUseProgram(id);
-      }
-}
-
-static inline void bind_pipeline(struct vrend_sub_context *sub_ctx, uint32_t id)
-{
-      if (sub_ctx->current_pipeline_id != id) {
-         sub_ctx->current_pipeline_id = id;
-         glBindProgramPipeline(id);
-      }
-}
-
-static void vrend_use_program(struct vrend_sub_context *sub_ctx,
-                              struct vrend_linked_shader_program *program)
+static void vrend_use_program(struct vrend_linked_shader_program *program)
 {
    GLuint id = !program ? 0 :
                           program->is_pipeline ? program->id.pipeline :
                                                  program->id.program;
    if (program && program->is_pipeline) {
-      use_program(sub_ctx, 0);
-      bind_pipeline(sub_ctx, id);
+      glUseProgram(0);
+      glBindProgramPipeline(id);
    } else {
        if (has_feature(feat_separate_shader_objects))
-          bind_pipeline(sub_ctx, 0);
-       use_program(sub_ctx, id);
+          glBindProgramPipeline(0);
+       glUseProgram(id);
    }
 }
 
-static void vrend_depth_test_enable(struct vrend_context *ctx, bool depth_test_enable)
+static void vrend_depth_test_enable(struct vrend_sub_context *sub_ctx, bool depth_test_enable)
 {
-   if (ctx->sub->depth_test_enabled != depth_test_enable) {
-      ctx->sub->depth_test_enabled = depth_test_enable;
+   if (sub_ctx->depth_test_enabled != depth_test_enable) {
+      sub_ctx->depth_test_enabled = depth_test_enable;
       if (depth_test_enable)
          glEnable(GL_DEPTH_TEST);
       else
@@ -1411,14 +1531,14 @@ static void vrend_depth_test_enable(struct vrend_context *ctx, bool depth_test_e
    }
 }
 
-static void vrend_alpha_test_enable(struct vrend_context *ctx, bool alpha_test_enable)
+static void vrend_alpha_test_enable(struct vrend_sub_context *sub_ctx, bool alpha_test_enable)
 {
    if (vrend_state.use_core_profile) {
       /* handled in shaders */
       return;
    }
-   if (ctx->sub->alpha_test_enabled != alpha_test_enable) {
-      ctx->sub->alpha_test_enabled = alpha_test_enable;
+   if (sub_ctx->alpha_test_enabled != alpha_test_enable) {
+      sub_ctx->alpha_test_enabled = alpha_test_enable;
       if (alpha_test_enable)
          glEnable(GL_ALPHA_TEST);
       else
@@ -1443,21 +1563,21 @@ static void dump_stream_out(struct pipe_stream_output_info *so)
    unsigned i;
    if (!so)
       return;
-   vrend_printf("streamout: %d\n", so->num_outputs);
-   vrend_printf("strides: ");
+   virgl_debug("streamout: %d\n", so->num_outputs);
+   virgl_debug("strides: ");
    for (i = 0; i < 4; i++)
-      vrend_printf("%d ", so->stride[i]);
-   vrend_printf("\n");
-   vrend_printf("outputs:\n");
+      virgl_debug("%d ", so->stride[i]);
+   virgl_debug("\n");
+   virgl_debug("outputs:\n");
    for (i = 0; i < so->num_outputs; i++) {
-      vrend_printf("\t%d: reg: %d sc: %d, nc: %d ob: %d do: %d st: %d\n",
-                   i,
-                   so->output[i].register_index,
-                   so->output[i].start_component,
-                   so->output[i].num_components,
-                   so->output[i].output_buffer,
-                   so->output[i].dst_offset,
-                   so->output[i].stream);
+      virgl_debug("\t%d: reg: %d sc: %d, nc: %d ob: %d do: %d st: %d\n",
+                  i,
+                  so->output[i].register_index,
+                  so->output[i].start_component,
+                  so->output[i].num_components,
+                  so->output[i].output_buffer,
+                  so->output[i].dst_offset,
+                  so->output[i].stream);
    }
 }
 
@@ -1492,7 +1612,7 @@ static void set_stream_out_varyings(ASSERTED struct vrend_sub_context *sub_ctx,
    struct pipe_stream_output_info *so = &sinfo->so_info;
    char *varyings[PIPE_MAX_SHADER_OUTPUTS*2];
    int j;
-   uint i, n_outputs = 0;
+   unsigned i, n_outputs = 0;
    int last_buffer = 0;
    char *start_skip;
    int buf_offset = 0;
@@ -1571,17 +1691,18 @@ static int bind_sampler_locs(struct vrend_linked_shader_program *sprog,
    const struct vrend_shader_info *sinfo = &sprog->ss[shader_type]->sel->sinfo;
 
    if (sinfo->samplers_used_mask) {
-      uint32_t mask = sinfo->samplers_used_mask;
-      sprog->shadow_samp_mask[shader_type] = sinfo->shadow_samp_mask;
+      unsigned nsamp = util_bitcount(sinfo->samplers_used_mask);
+      sprog->sampler_locs[shader_type] = calloc(nsamp, sizeof(GLuint));
       if (sinfo->shadow_samp_mask) {
-         unsigned nsamp = util_bitcount(sinfo->samplers_used_mask);
-         sprog->shadow_samp_mask_locs[shader_type] = calloc(nsamp, sizeof(uint32_t));
-         sprog->shadow_samp_add_locs[shader_type] = calloc(nsamp, sizeof(uint32_t));
+         sprog->shadow_samp_mask_locs[shader_type] = calloc(nsamp, sizeof(GLuint));
+         sprog->shadow_samp_add_locs[shader_type] = calloc(nsamp, sizeof(GLuint));
       } else {
          sprog->shadow_samp_mask_locs[shader_type] = sprog->shadow_samp_add_locs[shader_type] = NULL;
       }
+
       const char *prefix = pipe_shader_to_prefix(shader_type);
       int sampler_index = 0;
+      uint32_t mask = sinfo->samplers_used_mask;
       while(mask) {
          uint32_t i = u_bit_scan(&mask);
          char name[64];
@@ -1592,8 +1713,8 @@ static int bind_sampler_locs(struct vrend_linked_shader_program *sprog,
             snprintf(name, 32, "%ssamp%d", prefix, i);
 
          vrend_set_active_pipeline_stage(sprog, shader_type);
-         glUniform1i(vrend_get_uniform_location(sprog, name, shader_type),
-                     next_sampler_id++);
+         sprog->sampler_locs[shader_type][sampler_index] =
+            vrend_get_uniform_location(sprog, name, shader_type);
 
          if (sinfo->shadow_samp_mask & (1 << i)) {
             snprintf(name, 32, "%sshadmask%d", prefix, i);
@@ -1606,11 +1727,13 @@ static int bind_sampler_locs(struct vrend_linked_shader_program *sprog,
          sampler_index++;
       }
    } else {
+      sprog->sampler_locs[shader_type] = NULL;
       sprog->shadow_samp_mask_locs[shader_type] = NULL;
       sprog->shadow_samp_add_locs[shader_type] = NULL;
-      sprog->shadow_samp_mask[shader_type] = 0;
+      assert(!sinfo->shadow_samp_mask);
    }
    sprog->samplers_used_mask[shader_type] = sinfo->samplers_used_mask;
+   sprog->shadow_samp_mask[shader_type] = sinfo->shadow_samp_mask;
 
    return next_sampler_id;
 }
@@ -1642,7 +1765,7 @@ vrend_get_uniform_block_index(struct vrend_linked_shader_program *sprog,
 
 static inline void
 vrend_uniform_block_binding(struct vrend_linked_shader_program *sprog,
-                            int shader_type, int loc, int value)
+                            int shader_type, GLuint loc, GLuint value)
 {
     assert(!sprog->is_pipeline || sprog->ss[shader_type]->sel->sinfo.separable_program);
 
@@ -1653,8 +1776,8 @@ vrend_uniform_block_binding(struct vrend_linked_shader_program *sprog,
     glUniformBlockBinding(id, loc, value);
 }
 
-static int bind_ubo_locs(struct vrend_linked_shader_program *sprog,
-                         enum pipe_shader_type shader_type, int next_ubo_id)
+static unsigned bind_ubo_locs(struct vrend_linked_shader_program *sprog,
+                              enum pipe_shader_type shader_type, GLuint next_ubo_id)
 {
    const struct vrend_shader_info *sinfo = &sprog->ss[shader_type]->sel->sinfo;
    if (sinfo->ubo_used_mask) {
@@ -1681,32 +1804,32 @@ static int bind_ubo_locs(struct vrend_linked_shader_program *sprog,
 
 static void bind_virgl_block_loc(struct vrend_linked_shader_program *sprog,
                                  enum pipe_shader_type shader_type,
-                                 int virgl_block_ubo_id)
+                                 GLuint virgl_block_ubo_id)
 {
    sprog->separate_virgl_block_id[shader_type] =
-	 vrend_get_uniform_block_index(sprog, "VirglBlock", shader_type);
+      vrend_get_uniform_block_index(sprog, "VirglBlock", shader_type);
 
    if (sprog->separate_virgl_block_id[shader_type] != GL_INVALID_INDEX) {
       bool created_virgl_block_buffer = false;
 
-      if (sprog->virgl_block_bind == -1) {
+      if (sprog->virgl_block_bind == GL_INVALID_INDEX) {
          sprog->virgl_block_bind = virgl_block_ubo_id;
-         if (sprog->ubo_sysval_buffer_id == -1) {
-             glGenBuffers(1, (GLuint *) &sprog->ubo_sysval_buffer_id);
+         if (sprog->ubo_sysval_buffer_id == GL_INVALID_INDEX) {
+             glGenBuffers(1, &sprog->ubo_sysval_buffer_id);
              created_virgl_block_buffer = true;
          }
       }
 
       vrend_set_active_pipeline_stage(sprog, shader_type);
       vrend_uniform_block_binding(sprog, shader_type,
-		                  sprog->separate_virgl_block_id[shader_type],
-				  sprog->virgl_block_bind);
+                                  sprog->separate_virgl_block_id[shader_type],
+                                  sprog->virgl_block_bind);
 
       GLint virgl_block_size;
       int prog_id = sprog->is_pipeline ? sprog->ss[shader_type]->program_id :
                                          sprog->id.program;
       glGetActiveUniformBlockiv(prog_id, sprog->separate_virgl_block_id[shader_type],
-				GL_UNIFORM_BLOCK_DATA_SIZE, &virgl_block_size);
+                                GL_UNIFORM_BLOCK_DATA_SIZE, &virgl_block_size);
       assert((size_t) virgl_block_size >= sizeof(struct sysval_uniform_block));
 
       if (created_virgl_block_buffer) {
@@ -1721,7 +1844,7 @@ static void rebind_ubo_and_sampler_locs(struct vrend_linked_shader_program *spro
                                         enum pipe_shader_type last_shader)
 {
    int next_sampler_id = 0;
-   int next_ubo_id = 0;
+   GLuint next_ubo_id = 0;
 
    for (enum pipe_shader_type shader_type = PIPE_SHADER_VERTEX;
         shader_type <= last_shader;
@@ -1737,7 +1860,7 @@ static void rebind_ubo_and_sampler_locs(struct vrend_linked_shader_program *spro
    }
 
    /* Now `next_ubo_id` is the last ubo id, which is used for the VirglBlock. */
-   sprog->virgl_block_bind = -1;
+   sprog->virgl_block_bind = GL_INVALID_INDEX;
    for (enum pipe_shader_type shader_type = PIPE_SHADER_VERTEX;
         shader_type <= last_shader;
         shader_type++) {
@@ -1787,7 +1910,7 @@ static void bind_image_locs(struct vrend_linked_shader_program *sprog,
             sprog->img_locs[shader_type][img_array->first + j] =
                vrend_get_uniform_location(sprog, name, shader_type);
             if (sprog->img_locs[shader_type][img_array->first + j] == -1)
-               vrend_printf( "failed to get uniform loc for image %s\n", name);
+               virgl_error("Failed to get uniform loc for image %s\n", name);
          }
       }
    } else if (mask) {
@@ -1797,7 +1920,7 @@ static void bind_image_locs(struct vrend_linked_shader_program *sprog,
             sprog->img_locs[shader_type][i] =
                vrend_get_uniform_location(sprog, name, shader_type);
             if (sprog->img_locs[shader_type][i] == -1)
-               vrend_printf( "failed to get uniform loc for image %s\n", name);
+               virgl_error("Failed to get uniform loc for image %s\n", name);
          } else {
             sprog->img_locs[shader_type][i] = -1;
          }
@@ -1815,7 +1938,7 @@ static bool vrend_link(GLuint id)
       char infolog[65536];
       int len;
       glGetProgramInfoLog(id, 65536, &len, infolog);
-      vrend_printf("Error linking program:\n%s\n", infolog);
+      virgl_error("Error linking program:\n%s\n", infolog);
       return false;
    }
    return true;
@@ -1903,7 +2026,7 @@ static struct vrend_linked_shader_program *add_cs_shader_program(struct vrend_co
    sprog->id.program = prog_id;
    list_addtail(&sprog->head, &ctx->sub->cs_programs);
 
-   vrend_use_program(ctx->sub, sprog);
+   vrend_use_program(sprog);
 
    bind_sampler_locs(sprog, PIPE_SHADER_COMPUTE, 0);
    bind_ubo_locs(sprog, PIPE_SHADER_COMPUTE, 0);
@@ -1930,7 +2053,6 @@ static struct vrend_linked_shader_program *add_shader_program(struct vrend_sub_c
 {
    struct vrend_linked_shader_program *sprog = CALLOC_STRUCT(vrend_linked_shader_program);
    char name[64];
-   int i;
    GLuint prog_id = 0;
    GLuint pipeline_id = 0;
    GLuint vs_id, fs_id, gs_id, tes_id = 0;
@@ -2006,7 +2128,7 @@ static struct vrend_linked_shader_program *add_shader_program(struct vrend_sub_c
    if (has_feature(feat_gles31_vertex_attrib_binding)) {
       uint32_t mask = vs->sel->sinfo.attrib_input_mask;
       while (mask) {
-         i = u_bit_scan(&mask);
+         int i = u_bit_scan(&mask);
          snprintf(name, 32, "in_%d", i);
          glBindAttribLocation(vs_id, i, name);
       }
@@ -2088,11 +2210,11 @@ static struct vrend_linked_shader_program *add_shader_program(struct vrend_sub_c
 
    list_addtail(&sprog->head, &sub_ctx->gl_programs[vs->id & VREND_PROGRAM_NQUEUE_MASK]);
 
-   sprog->virgl_block_bind = -1;
-   sprog->ubo_sysval_buffer_id = -1;
+   sprog->virgl_block_bind = GL_INVALID_INDEX;
+   sprog->ubo_sysval_buffer_id = GL_INVALID_INDEX;
    sprog->sysvalue_data_cookie = UINT32_MAX;
 
-   vrend_use_program(sub_ctx, sprog);
+   vrend_use_program(sprog);
 
    for (enum pipe_shader_type shader_type = PIPE_SHADER_VERTEX;
         shader_type <= last_shader;
@@ -2113,7 +2235,7 @@ static struct vrend_linked_shader_program *add_shader_program(struct vrend_sub_c
       if (vs->sel->sinfo.num_inputs) {
          sprog->attrib_locs = calloc(vs->sel->sinfo.num_inputs, sizeof(uint32_t));
          if (sprog->attrib_locs) {
-            for (i = 0; i < vs->sel->sinfo.num_inputs; i++) {
+            for (int i = 0; i < vs->sel->sinfo.num_inputs; i++) {
                snprintf(name, 32, "in_%d", i);
                sprog->attrib_locs[i] = glGetAttribLocation(vs_id, name);
             }
@@ -2128,8 +2250,7 @@ static struct vrend_linked_shader_program *add_shader_program(struct vrend_sub_c
 static struct vrend_linked_shader_program *lookup_cs_shader_program(struct vrend_context *ctx,
                                                                     GLuint cs_id)
 {
-   struct vrend_linked_shader_program *ent;
-   LIST_FOR_EACH_ENTRY(ent, &ctx->sub->cs_programs, head) {
+   list_for_each_entry(struct vrend_linked_shader_program, ent, &ctx->sub->cs_programs, head) {
       if (ent->ss[PIPE_SHADER_COMPUTE]->id == cs_id) {
          list_del(&ent->head);
          list_add(&ent->head, &ctx->sub->cs_programs);
@@ -2150,10 +2271,8 @@ static struct vrend_linked_shader_program *lookup_shader_program(struct vrend_su
    uint64_t vs_fs_key = (((uint64_t)fs_id) << 32) | (vs_id & ~VREND_PROGRAM_NQUEUE_MASK) |
                         (dual_src ? 1 : 0);
 
-   struct vrend_linked_shader_program *ent;
-
    struct list_head *programs = &sub_ctx->gl_programs[vs_id & VREND_PROGRAM_NQUEUE_MASK];
-   LIST_FOR_EACH_ENTRY(ent, programs, head) {
+   list_for_each_entry(struct vrend_linked_shader_program, ent, programs, head) {
       if (likely(ent->vs_fs_key != vs_fs_key))
          continue;
       if (ent->ss[PIPE_SHADER_GEOMETRY] &&
@@ -2182,8 +2301,8 @@ static void vrend_destroy_program(struct vrend_linked_shader_program *ent)
    if (ent->ref_context && ent->ref_context->prog == ent)
       ent->ref_context->prog = NULL;
 
-   if (ent->ubo_sysval_buffer_id != -1) {
-       glDeleteBuffers(1, (GLuint *) &ent->ubo_sysval_buffer_id);
+   if (ent->ubo_sysval_buffer_id != GL_INVALID_INDEX) {
+       glDeleteBuffers(1, &ent->ubo_sysval_buffer_id);
    }
 
    if (ent->is_pipeline)
@@ -2194,11 +2313,16 @@ static void vrend_destroy_program(struct vrend_linked_shader_program *ent)
    list_del(&ent->head);
 
    for (i = PIPE_SHADER_VERTEX; i <= PIPE_SHADER_COMPUTE; i++) {
-      if (ent->ss[i])
+      if (ent->ss[i]) {
          list_del(&ent->sl[i]);
+         if (ent->ss[i]->last_pipeline_id == ent->id.pipeline)
+            ent->ss[i]->last_pipeline_id = 0xffffffff;
+      }
+      free(ent->sampler_locs[i]);
       free(ent->shadow_samp_mask_locs[i]);
       free(ent->shadow_samp_add_locs[i]);
       free(ent->img_locs[i]);
+
    }
    free(ent->attrib_locs);
    free(ent);
@@ -2206,16 +2330,14 @@ static void vrend_destroy_program(struct vrend_linked_shader_program *ent)
 
 static void vrend_free_programs(struct vrend_sub_context *sub)
 {
-   struct vrend_linked_shader_program *ent, *tmp;
-
-   if (!LIST_IS_EMPTY(&sub->cs_programs)) {
-      LIST_FOR_EACH_ENTRY_SAFE(ent, tmp, &sub->cs_programs, head)
+   if (!list_is_empty(&sub->cs_programs)) {
+      list_for_each_entry_safe(struct vrend_linked_shader_program, ent, &sub->cs_programs, head)
          vrend_destroy_program(ent);
    }
 
    for (unsigned i = 0; i < VREND_PROGRAM_NQUEUES; ++i) {
-      if (!LIST_IS_EMPTY(&sub->gl_programs[i])) {
-         LIST_FOR_EACH_ENTRY_SAFE(ent, tmp, &sub->gl_programs[i], head)
+      if (!list_is_empty(&sub->gl_programs[i])) {
+         list_for_each_entry_safe(struct vrend_linked_shader_program, ent, &sub->gl_programs[i], head)
             vrend_destroy_program(ent);
       }
    }
@@ -2240,35 +2362,24 @@ void vrend_sync_make_current(virgl_gl_context gl_cxt) {
 }
 
 int vrend_create_surface(struct vrend_context *ctx,
-                         uint32_t handle,
-                         uint32_t res_handle, uint32_t format,
-                         uint32_t val0, uint32_t val1,
+                         uint32_t handle, struct vrend_resource *res,
+                         enum virgl_formats format, uint32_t level,
+                         uint32_t first_layer, uint32_t last_layer,
                          uint32_t nr_samples)
 {
    struct vrend_surface *surf;
-   struct vrend_resource *res;
    uint32_t ret_handle;
 
-   if (format >= PIPE_FORMAT_COUNT) {
-      return EINVAL;
-   }
-
-   res = vrend_renderer_ctx_res_lookup(ctx, res_handle);
-   if (!res) {
-      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, res_handle);
-      return EINVAL;
-   }
-
    surf = CALLOC_STRUCT(vrend_surface);
    if (!surf)
       return ENOMEM;
 
-   surf->res_handle = res_handle;
    surf->format = format;
 
-   surf->val0 = val0;
-   surf->val1 = val1;
-   surf->id = res->id;
+   surf->level = level;
+   surf->first_layer = first_layer;
+   surf->last_layer = last_layer;
+   surf->gl_id = res->gl_id;
    surf->nr_samples = nr_samples;
 
    if (!has_bit(res->storage_bits, VREND_STORAGE_GL_BUFFER) &&
@@ -2282,11 +2393,11 @@ int vrend_create_surface(struct vrend_context *ctx,
        * can map the whole texure fine. In those cases we don't
        * create a texture view.
        */
-      int first_layer = surf->val1 & 0xffff;
-      int last_layer = (surf->val1 >> 16) & 0xffff;
+      int first_layer = surf->first_layer;
+      int last_layer = surf->last_layer;
 
       bool needs_view = first_layer != last_layer &&
-         (first_layer != 0 || (last_layer != (int)util_max_layer(&res->base, surf->val0)));
+         (first_layer != 0 || (last_layer != (int)util_max_layer(&res->base, surf->level)));
       if (!needs_view && surf->format != res->base.format)
          needs_view = true;
 
@@ -2299,11 +2410,13 @@ int vrend_create_surface(struct vrend_context *ctx,
             last_layer = 5;
          }
 
+         int num_layers = last_layer - first_layer + 1;
+
          VREND_DEBUG(dbg_tex, ctx, "Create texture view from %s for %s\n",
                      util_format_name(res->base.format),
                      util_format_name(surf->format));
 
-         glGenTextures(1, &surf->id);
+         glGenTextures(1, &surf->gl_id);
          if (vrend_state.use_gles) {
             if (target == GL_TEXTURE_1D)
                target = GL_TEXTURE_2D;
@@ -2316,9 +2429,9 @@ int vrend_create_surface(struct vrend_context *ctx,
             target = GL_TEXTURE_2D;
          }
 
-         glTextureView(surf->id, target, res->id, internalformat,
+         glTextureView(surf->gl_id, target, res->gl_id, internalformat,
                        0, res->base.last_level + 1,
-                       first_layer, last_layer - first_layer + 1);
+                       first_layer, num_layers);
       }
    }
 
@@ -2328,6 +2441,7 @@ int vrend_create_surface(struct vrend_context *ctx,
 
    ret_handle = vrend_renderer_object_insert(ctx, surf, handle, VIRGL_OBJECT_SURFACE);
    if (ret_handle == 0) {
+      vrend_resource_reference(&surf->texture, NULL);
       FREE(surf);
       return ENOMEM;
    }
@@ -2374,30 +2488,24 @@ static void vrend_destroy_so_target_object(void *obj_ptr)
 {
    struct vrend_so_target *target = obj_ptr;
    struct vrend_sub_context *sub_ctx = target->sub_ctx;
-   struct vrend_streamout_object *obj, *tmp;
-   bool found;
    unsigned i;
 
-   LIST_FOR_EACH_ENTRY_SAFE(obj, tmp, &sub_ctx->streamout_list, head) {
-      found = false;
+   list_for_each_entry_safe(struct vrend_streamout_object, obj, &sub_ctx->streamout_list, head) {
       for (i = 0; i < obj->num_targets; i++) {
          if (obj->so_targets[i] == target) {
-            found = true;
+            if (obj == sub_ctx->current_so)
+               sub_ctx->current_so = NULL;
+            if (obj->xfb_state == XFB_STATE_PAUSED) {
+                  if (has_feature(feat_transform_feedback2))
+                     glBindTransformFeedback(GL_TRANSFORM_FEEDBACK, obj->id);
+                  glEndTransformFeedback();
+               if (sub_ctx->current_so && has_feature(feat_transform_feedback2))
+                  glBindTransformFeedback(GL_TRANSFORM_FEEDBACK, sub_ctx->current_so->id);
+            }
+            vrend_destroy_streamout_object(obj);
             break;
          }
       }
-      if (found) {
-         if (obj == sub_ctx->current_so)
-            sub_ctx->current_so = NULL;
-         if (obj->xfb_state == XFB_STATE_PAUSED) {
-               if (has_feature(feat_transform_feedback2))
-                  glBindTransformFeedback(GL_TRANSFORM_FEEDBACK, obj->id);
-               glEndTransformFeedback();
-            if (sub_ctx->current_so && has_feature(feat_transform_feedback2))
-               glBindTransformFeedback(GL_TRANSFORM_FEEDBACK, sub_ctx->current_so->id);
-         }
-         vrend_destroy_streamout_object(obj);
-      }
    }
 
    vrend_so_target_reference(&target, NULL);
@@ -2428,17 +2536,19 @@ static void vrend_destroy_sampler_state_object(void *obj_ptr)
       for (enum pipe_shader_type shader_type = PIPE_SHADER_VERTEX;
            shader_type < PIPE_SHADER_TYPES;
            shader_type++) {
+         struct vrend_shader_view *shader_view = &sub_ctx->views[shader_type];
+
          int deleted_samplers = 0;
          for (uint32_t sampler = 0; sampler < PIPE_MAX_SAMPLERS; sampler++) {
-            if (sub_ctx->sampler_state[shader_type][sampler] == state) {
-               sub_ctx->sampler_state[shader_type][sampler] = NULL;
-               sub_ctx->num_sampler_states[shader_type]--;
-               sub_ctx->sampler_views_dirty[shader_type] |= (1u << sampler);
+            if (shader_view->samplers[sampler] == state) {
+               shader_view->samplers[sampler] = NULL;
+               shader_view->dirty_mask |= (1u << sampler);
                deleted_samplers++;
             } else if (deleted_samplers) {
-               sub_ctx->sampler_state[shader_type][sampler-deleted_samplers] = sub_ctx->sampler_state[shader_type][sampler];
-               sub_ctx->sampler_state[shader_type][sampler] = NULL;
-               sub_ctx->sampler_views_dirty[shader_type] |= (1u << sampler);
+               shader_view->samplers[sampler-deleted_samplers] =
+                  shader_view->samplers[sampler];
+               shader_view->samplers[sampler] = NULL;
+               shader_view->dirty_mask |= (1u << sampler);
             }
          }
       }
@@ -2452,24 +2562,44 @@ static void vrend_destroy_dsa_object(void *obj_ptr)
    struct vrend_depth_stencil_alpha_state *state = obj_ptr;
 
    if (state->owning_sub && state == state->owning_sub->dsa)
-      vrend_object_bind_dsa(state->owning_sub->parent, 0 /* unbind */);
+      vrend_object_bind_dsa_to_sub_context(state->owning_sub, 0 /* unbind */);
 
    FREE(state);
 }
 
-static GLuint convert_wrap(int wrap)
+static GLuint convert_wrap(struct vrend_context *ctx, int wrap)
 {
    switch(wrap){
    case PIPE_TEX_WRAP_REPEAT: return GL_REPEAT;
    case PIPE_TEX_WRAP_CLAMP: if (vrend_state.use_core_profile == false) return GL_CLAMP; else return GL_CLAMP_TO_EDGE;
 
    case PIPE_TEX_WRAP_CLAMP_TO_EDGE: return GL_CLAMP_TO_EDGE;
-   case PIPE_TEX_WRAP_CLAMP_TO_BORDER: return GL_CLAMP_TO_BORDER;
+   case PIPE_TEX_WRAP_CLAMP_TO_BORDER:
+      /* GLES (ANGLE/Metal) does not support clamp-to-border; fall back to edge
+       * to avoid GL_INVALID_ENUM on sampler parameter calls.
+       */
+      if (vrend_state.use_gles || !has_feature(feat_sampler_border_colors)) {
+         return GL_CLAMP_TO_EDGE;
+      }
+      return GL_CLAMP_TO_BORDER;
 
    case PIPE_TEX_WRAP_MIRROR_REPEAT: return GL_MIRRORED_REPEAT;
-   case PIPE_TEX_WRAP_MIRROR_CLAMP: return GL_MIRROR_CLAMP_EXT;
-   case PIPE_TEX_WRAP_MIRROR_CLAMP_TO_EDGE: return GL_MIRROR_CLAMP_TO_EDGE_EXT;
-   case PIPE_TEX_WRAP_MIRROR_CLAMP_TO_BORDER: return GL_MIRROR_CLAMP_TO_BORDER_EXT;
+   case PIPE_TEX_WRAP_MIRROR_CLAMP:
+      /* Not available on GLES; fall back to mirrored repeat without error. */
+      if (has_feature(feat_texture_mirror_clamp))
+         return GL_MIRROR_CLAMP_EXT;
+      return GL_MIRRORED_REPEAT;
+   case PIPE_TEX_WRAP_MIRROR_CLAMP_TO_EDGE:
+      /* ANGLE/Metal lacks this; fall back silently to clamp-to-edge. */
+      if (has_feature(feat_texture_mirror_clamp_to_edge))
+         return GL_MIRROR_CLAMP_TO_EDGE_EXT;
+      return GL_CLAMP_TO_EDGE;
+   case PIPE_TEX_WRAP_MIRROR_CLAMP_TO_BORDER:
+      if (has_feature(feat_texture_mirror_clamp_to_border)) {
+         return GL_MIRROR_CLAMP_TO_BORDER_EXT;
+      }
+      /* No host support: clamp to edge to avoid GL errors. */
+      return GL_CLAMP_TO_EDGE;
    default:
       assert(0);
       return -1;
@@ -2508,7 +2638,7 @@ static void apply_sampler_border_color(GLuint sampler,
    if (has_feature(feat_sampler_border_colors)) {
       glSamplerParameterIuiv(sampler, GL_TEXTURE_BORDER_COLOR, colors);
    } else if (colors[0] || colors[1] || colors[2] || colors[3]) {
-      vrend_printf("sampler border color setting requested but not supported\n");
+      virgl_warn("Sampler border color setting requested but not supported\n");
    }
 }
 
@@ -2528,9 +2658,9 @@ int vrend_create_sampler_state(struct vrend_context *ctx,
       glGenSamplers(2, state->ids);
 
       for (int i = 0; i < 2; ++i) {
-         glSamplerParameteri(state->ids[i], GL_TEXTURE_WRAP_S, convert_wrap(templ->wrap_s));
-         glSamplerParameteri(state->ids[i], GL_TEXTURE_WRAP_T, convert_wrap(templ->wrap_t));
-         glSamplerParameteri(state->ids[i], GL_TEXTURE_WRAP_R, convert_wrap(templ->wrap_r));
+         glSamplerParameteri(state->ids[i], GL_TEXTURE_WRAP_S, convert_wrap(ctx, templ->wrap_s));
+         glSamplerParameteri(state->ids[i], GL_TEXTURE_WRAP_T, convert_wrap(ctx, templ->wrap_t));
+         glSamplerParameteri(state->ids[i], GL_TEXTURE_WRAP_R, convert_wrap(ctx, templ->wrap_r));
          glSamplerParameterf(state->ids[i], GL_TEXTURE_MIN_FILTER, convert_min_filter(templ->min_img_filter, templ->min_mip_filter));
          glSamplerParameterf(state->ids[i], GL_TEXTURE_MAG_FILTER, convert_mag_filter(templ->mag_img_filter));
          glSamplerParameterf(state->ids[i], GL_TEXTURE_MIN_LOD, templ->min_lod);
@@ -2573,27 +2703,12 @@ int vrend_create_sampler_state(struct vrend_context *ctx,
 static inline GLenum to_gl_swizzle(enum pipe_swizzle swizzle)
 {
    switch (swizzle) {
-   case PIPE_SWIZZLE_RED: return GL_RED;
-   case PIPE_SWIZZLE_GREEN: return GL_GREEN;
-   case PIPE_SWIZZLE_BLUE: return GL_BLUE;
-   case PIPE_SWIZZLE_ALPHA: return GL_ALPHA;
-   case PIPE_SWIZZLE_ZERO: return GL_ZERO;
-   case PIPE_SWIZZLE_ONE: return GL_ONE;
-   default:
-      assert(0);
-      return 0;
-   }
-}
-
-static inline enum pipe_swizzle to_pipe_swizzle(GLenum swizzle)
-{
-   switch (swizzle) {
-   case GL_RED: return PIPE_SWIZZLE_RED;
-   case GL_GREEN: return PIPE_SWIZZLE_GREEN;
-   case GL_BLUE: return PIPE_SWIZZLE_BLUE;
-   case GL_ALPHA: return PIPE_SWIZZLE_ALPHA;
-   case GL_ZERO: return PIPE_SWIZZLE_ZERO;
-   case GL_ONE: return PIPE_SWIZZLE_ONE;
+   case PIPE_SWIZZLE_X: return GL_RED;
+   case PIPE_SWIZZLE_Y: return GL_GREEN;
+   case PIPE_SWIZZLE_Z: return GL_BLUE;
+   case PIPE_SWIZZLE_W: return GL_ALPHA;
+   case PIPE_SWIZZLE_0: return GL_ZERO;
+   case PIPE_SWIZZLE_1: return GL_ONE;
    default:
       assert(0);
       return 0;
@@ -2602,40 +2717,33 @@ static inline enum pipe_swizzle to_pipe_swizzle(GLenum swizzle)
 
 int vrend_create_sampler_view(struct vrend_context *ctx,
                               uint32_t handle,
-                              uint32_t res_handle, uint32_t format,
+                              struct vrend_resource *res,
+                              enum virgl_formats format, enum pipe_texture_target pipe_target,
                               uint32_t val0, uint32_t val1, uint32_t swizzle_packed)
 {
    struct vrend_sampler_view *view;
-   struct vrend_resource *res;
    int ret_handle;
    enum pipe_swizzle swizzle[4];
 
-   res = vrend_renderer_ctx_res_lookup(ctx, res_handle);
-   if (!res) {
-      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, res_handle);
+   assert(format < PIPE_FORMAT_COUNT);
+   if (unlikely(format >= PIPE_FORMAT_COUNT)) {
+      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_FORMAT, format);
       return EINVAL;
    }
 
+   for (int i = 0; i < 4; ++i) {
+      swizzle[i] = (swizzle_packed  >> (3 * i)) & 0x7;
+      if (swizzle[i] > PIPE_SWIZZLE_1) {
+         return EINVAL;
+      }
+   }
+
    view = CALLOC_STRUCT(vrend_sampler_view);
    if (!view)
       return ENOMEM;
 
    pipe_reference_init(&view->reference, 1);
-   view->format = format & 0xffffff;
-
-   if (!view->format || view->format >= VIRGL_FORMAT_MAX) {
-      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_FORMAT, view->format);
-      FREE(view);
-      return EINVAL;
-   }
-
-   uint32_t pipe_target = (format >> 24) & 0xff;
-   if (pipe_target >= PIPE_MAX_TEXTURE_TYPES) {
-      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_SAMPLER_VIEW_TARGET,
-                           view->format);
-      FREE(view);
-      return EINVAL;
-   }
+   view->format = format;
 
    view->target = tgsitargettogltarget(pipe_target, res->base.nr_samples);
 
@@ -2653,20 +2761,22 @@ int vrend_create_sampler_view(struct vrend_context *ctx,
       view->target = GL_TEXTURE_2D;
    }
 
-   view->val0 = val0;
-   view->val1 = val1;
-
-   swizzle[0] = swizzle_packed & 0x7;
-   swizzle[1] = (swizzle_packed >> 3) & 0x7;
-   swizzle[2] = (swizzle_packed >> 6) & 0x7;
-   swizzle[3] = (swizzle_packed >> 9) & 0x7;
-
    vrend_resource_reference(&view->texture, res);
 
-   view->id = view->texture->id;
-   if (view->target == PIPE_BUFFER)
+   view->gl_id = view->texture->gl_id;
+
+   if (view->target == PIPE_BUFFER) {
       view->target = view->texture->target;
 
+      view->u.buf.first_element = val0;
+      view->u.buf.last_element = val1;
+   } else {
+      view->u.tex.first_layer = val0 & 0xffff;
+      view->u.tex.last_layer = (val0 >> 16) & 0xffff;
+      view->u.tex.first_level = val1 & 0xff;
+      view->u.tex.last_level = (val1 >> 8) & 0xff;
+   }
+
    view->srgb_decode = GL_DECODE_EXT;
    if (view->format != view->texture->base.format) {
       if (util_format_is_srgb(view->texture->base.format) &&
@@ -2675,30 +2785,40 @@ int vrend_create_sampler_view(struct vrend_context *ctx,
    }
 
    if (!(util_format_has_alpha(view->format) || util_format_is_depth_or_stencil(view->format))) {
-      if (swizzle[0] == PIPE_SWIZZLE_ALPHA)
-          swizzle[0] = PIPE_SWIZZLE_ONE;
-      if (swizzle[1] == PIPE_SWIZZLE_ALPHA)
-          swizzle[1] = PIPE_SWIZZLE_ONE;
-      if (swizzle[2] == PIPE_SWIZZLE_ALPHA)
-          swizzle[2] = PIPE_SWIZZLE_ONE;
-      if (swizzle[3] == PIPE_SWIZZLE_ALPHA)
-          swizzle[3] = PIPE_SWIZZLE_ONE;
+      if (swizzle[0] == PIPE_SWIZZLE_W)
+          swizzle[0] = PIPE_SWIZZLE_1;
+      if (swizzle[1] == PIPE_SWIZZLE_W)
+          swizzle[1] = PIPE_SWIZZLE_1;
+      if (swizzle[2] == PIPE_SWIZZLE_W)
+          swizzle[2] = PIPE_SWIZZLE_1;
+      if (swizzle[3] == PIPE_SWIZZLE_W)
+          swizzle[3] = PIPE_SWIZZLE_1;
    }
 
    if (tex_conv_table[view->format].flags & VIRGL_TEXTURE_NEED_SWIZZLE) {
-      if (swizzle[0] <= PIPE_SWIZZLE_ALPHA)
+      if (swizzle[0] <= PIPE_SWIZZLE_W)
          swizzle[0] = tex_conv_table[view->format].swizzle[swizzle[0]];
-      if (swizzle[1] <= PIPE_SWIZZLE_ALPHA)
+      if (swizzle[1] <= PIPE_SWIZZLE_W)
          swizzle[1] = tex_conv_table[view->format].swizzle[swizzle[1]];
-      if (swizzle[2] <= PIPE_SWIZZLE_ALPHA)
+      if (swizzle[2] <= PIPE_SWIZZLE_W)
          swizzle[2] = tex_conv_table[view->format].swizzle[swizzle[2]];
-      if (swizzle[3] <= PIPE_SWIZZLE_ALPHA)
+      if (swizzle[3] <= PIPE_SWIZZLE_W)
          swizzle[3] = tex_conv_table[view->format].swizzle[swizzle[3]];
    }
 
    for (enum pipe_swizzle i = 0; i < 4; ++i)
       view->gl_swizzle[i] = to_gl_swizzle(swizzle[i]);
 
+   if (res->is_imported && vrend_format_is_bgra(view->texture->base.format)) {
+      /* Swap R/B channel for vulkan imported texture. */
+      GLenum tmp = view->gl_swizzle[0];
+      view->gl_swizzle[0] = view->gl_swizzle[2];
+      view->gl_swizzle[2] = tmp;
+
+      /* Don't decode vulkan imported texture. */
+      view->srgb_decode = GL_SKIP_DECODE_EXT;
+   }
+
    if (!has_bit(view->texture->storage_bits, VREND_STORAGE_GL_BUFFER)) {
       enum virgl_formats format;
       bool needs_view = false;
@@ -2727,20 +2847,26 @@ int vrend_create_sampler_view(struct vrend_context *ctx,
       else if (view->format != view->texture->base.format)
          needs_view = true;
 
-      unsigned base_layer = view->val0 & 0xffff;
-      int base_level = view->val1 & 0xff;
-
-      if (base_layer > 0 || base_level > 0)
+      if (view->u.tex.first_layer > 0 || view->u.tex.first_level > 0)
          needs_view = true;
 
       if (needs_view &&
           has_bit(view->texture->storage_bits, VREND_STORAGE_GL_IMMUTABLE) &&
           has_feature(feat_texture_view)) {
-        glGenTextures(1, &view->id);
         GLenum internalformat = tex_conv_table[format].internalformat;
-        unsigned max_layer = (view->val0 >> 16) & 0xffff;
-        int max_level = (view->val1 >> 8) & 0xff;
-        view->levels = (max_level - base_level) + 1;
+        view->levels = (view->u.tex.last_level - view->u.tex.first_level) + 1;
+
+        int num_layers = view->u.tex.last_layer - view->u.tex.first_layer + 1;
+
+        if (view->levels == 0 || num_layers <= 0) {
+            vrend_resource_reference(&view->texture, NULL);
+            FREE(view);
+            virgl_error("%s: Invalid number of layers (%d) or zero levels requested\n",
+                        __func__, num_layers);
+            return EINVAL;
+        }
+
+        glGenTextures(1, &view->gl_id);
 
         /* texture views for eglimage-backed bgr* resources are usually not
          * supported since they cause unintended red/blue channel-swapping.
@@ -2760,11 +2886,28 @@ int vrend_create_sampler_view(struct vrend_context *ctx,
               view->gl_swizzle[2] = temp;
         }
 
-        glTextureView(view->id, view->target, view->texture->id, internalformat,
-                      base_level, view->levels,
-                      base_layer, max_layer - base_layer + 1);
+        // Workaround for clients that attempt this non-spec-conformant (or at
+        // least non-spec-defined) behavior. Gallium seems to allow this for
+        // internally-allocated textures, but textures imported from dmabuf take
+        // a different path that imposes stricter enforcement of allowable
+        // internalformat for glTextureView().
+
+        GLuint real_internalformat =
+              vrend_resource_get_internal_format_override(view->texture);
+
+        if (util_format_has_alpha(view->format) &&
+            real_internalformat != GL_NONE) {
+           VREND_DEBUG(dbg_tex, ctx, "Fixing view with alpha into EGL-backed texture without alpha (format: %s, view: %s)\n",
+                       util_format_name(view->texture->base.format),
+                       util_format_name(view->format));
+           internalformat = real_internalformat;
+        }
+
+        glTextureView(view->gl_id, view->target, view->texture->gl_id, internalformat,
+                      view->u.tex.first_level, view->levels,
+                      view->u.tex.first_layer, num_layers);
 
-        glBindTexture(view->target, view->id);
+        glBindTexture(view->target, view->gl_id);
 
         if (util_format_is_depth_or_stencil(view->format)) {
            if (vrend_state.use_core_profile == false) {
@@ -2793,11 +2936,11 @@ int vrend_create_sampler_view(struct vrend_context *ctx,
                             view->srgb_decode);
         }
         glBindTexture(view->target, 0);
-      } else if (needs_view && view->val0 < ARRAY_SIZE(res->aux_plane_egl_image) &&
-            res->aux_plane_egl_image[view->val0]) {
-        void *image = res->aux_plane_egl_image[view->val0];
-        glGenTextures(1, &view->id);
-        glBindTexture(view->target, view->id);
+      } else if (needs_view && view->u.buf.first_element < ARRAY_SIZE(res->aux_plane_egl_image) &&
+            res->aux_plane_egl_image[view->u.buf.first_element]) {
+        void *image = res->aux_plane_egl_image[view->u.buf.first_element];
+        glGenTextures(1, &view->gl_id);
+        glBindTexture(view->target, view->gl_id);
         glEGLImageTargetTexture2DOES(view->target, (GLeglImageOES) image);
         glBindTexture(view->target, 0);
       }
@@ -2805,6 +2948,7 @@ int vrend_create_sampler_view(struct vrend_context *ctx,
 
    ret_handle = vrend_renderer_object_insert(ctx, view, handle, VIRGL_OBJECT_SAMPLER_VIEW);
    if (ret_handle == 0) {
+      vrend_resource_reference(&view->texture, NULL);
       FREE(view);
       return ENOMEM;
    }
@@ -2848,9 +2992,9 @@ static
 void debug_texture(ASSERTED const char *f, const struct vrend_resource *gt)
 {
    ASSERTED const struct pipe_resource *pr = &gt->base;
-#define PRINT_TARGET(X) case X: vrend_printf( #X); break
+#define PRINT_TARGET(X) case X: virgl_debug( #X); break
    VREND_DEBUG_EXT(dbg_tex, NULL,
-               vrend_printf("%s: ", f);
+               virgl_debug("%s: ", f);
                switch (tgsitargettogltarget(pr->target, pr->nr_samples)) {
                PRINT_TARGET(GL_TEXTURE_RECTANGLE_NV);
                PRINT_TARGET(GL_TEXTURE_1D);
@@ -2862,18 +3006,19 @@ void debug_texture(ASSERTED const char *f, const struct vrend_resource *gt)
                PRINT_TARGET(GL_TEXTURE_CUBE_MAP);
                PRINT_TARGET(GL_TEXTURE_CUBE_MAP_ARRAY);
                default:
-                  vrend_printf("UNKNOWN");
+                  virgl_debug("UNKNOWN");
                }
-               vrend_printf(" id:%d pipe_type:%d ms:%d format:%s size: %dx%dx%d mip:%d\n",
-                            gt->id, pr->target, pr->nr_samples, util_format_name(pr->format),
-                            pr->width0, pr->height0, pr->depth0, pr->last_level);
+               virgl_debug(" id:%d pipe_type:%d ms:%d format:%s size: %dx%dx%d mip:%d %s\n",
+                           gt->gl_id, pr->target, pr->nr_samples, util_format_name(pr->format),
+                           pr->width0, pr->height0, pr->depth0, pr->last_level,
+                           gt->gbm_bo ? "GBM" : "GL");
                );
 #undef PRINT_TARGET
 }
 
 void vrend_fb_bind_texture_id(struct vrend_resource *res,
-                              int id, int idx, uint32_t level,
-                              uint32_t layer, uint32_t samples)
+                              int id, GLuint idx, GLint level,
+                              GLint layer, uint32_t samples)
 {
    const struct util_format_description *desc = util_format_description(res->base.format);
    GLenum attachment = GL_COLOR_ATTACHMENT0 + idx;
@@ -2895,7 +3040,7 @@ void vrend_fb_bind_texture_id(struct vrend_resource *res,
    case GL_TEXTURE_2D_ARRAY:
    case GL_TEXTURE_2D_MULTISAMPLE_ARRAY:
    case GL_TEXTURE_CUBE_MAP_ARRAY:
-      if (layer == 0xffffffff)
+      if (layer < 0)
          glFramebufferTexture(GL_FRAMEBUFFER, attachment,
                               id, level);
       else
@@ -2903,7 +3048,7 @@ void vrend_fb_bind_texture_id(struct vrend_resource *res,
                                    id, level, layer);
       break;
    case GL_TEXTURE_3D:
-      if (layer == 0xffffffff)
+      if (layer < 0)
          glFramebufferTexture(GL_FRAMEBUFFER, attachment,
                               id, level);
       else if (vrend_state.use_gles)
@@ -2914,7 +3059,7 @@ void vrend_fb_bind_texture_id(struct vrend_resource *res,
                                 res->target, id, level, layer);
       break;
    case GL_TEXTURE_CUBE_MAP:
-      if (layer == 0xffffffff)
+      if (layer < 0)
          glFramebufferTexture(GL_FRAMEBUFFER, attachment,
                               id, level);
       else
@@ -2949,10 +3094,10 @@ void vrend_fb_bind_texture_id(struct vrend_resource *res,
 }
 
 void vrend_fb_bind_texture(struct vrend_resource *res,
-                           int idx,
-                           uint32_t level, uint32_t layer)
+                           GLuint idx,
+                           GLint level, GLint layer)
 {
-   vrend_fb_bind_texture_id(res, res->id, idx, level, layer, 0);
+   vrend_fb_bind_texture_id(res, res->gl_id, idx, level, layer, 0);
 }
 
 static void vrend_hw_set_zsurf_texture(struct vrend_context *ctx)
@@ -2963,19 +3108,16 @@ static void vrend_hw_set_zsurf_texture(struct vrend_context *ctx)
       glFramebufferTexture2D(GL_FRAMEBUFFER, GL_DEPTH_STENCIL_ATTACHMENT,
                              GL_TEXTURE_2D, 0, 0);
    } else {
-      uint32_t first_layer = surf->val1 & 0xffff;
-      uint32_t last_layer = (surf->val1 >> 16) & 0xffff;
-
       if (!surf->texture)
          return;
 
-      vrend_fb_bind_texture_id(surf->texture, surf->id, 0, surf->val0,
-                               first_layer != last_layer ? 0xffffffff : first_layer,
-                               surf->nr_samples);
+      vrend_fb_bind_texture_id(surf->texture, surf->gl_id, 0, surf->level,
+                               surf->first_layer != surf->last_layer ? -1 :
+                               (GLint)surf->first_layer, surf->nr_samples);
    }
 }
 
-static void vrend_hw_set_color_surface(struct vrend_sub_context *sub_ctx, int index)
+static void vrend_hw_set_color_surface(struct vrend_sub_context *sub_ctx, GLuint index)
 {
    struct vrend_surface *surf = sub_ctx->surf[index];
 
@@ -2985,11 +3127,11 @@ static void vrend_hw_set_color_surface(struct vrend_sub_context *sub_ctx, int in
       glFramebufferTexture2D(GL_FRAMEBUFFER, attachment,
                              GL_TEXTURE_2D, 0, 0);
    } else {
-      uint32_t first_layer = sub_ctx->surf[index]->val1 & 0xffff;
-      uint32_t last_layer = (sub_ctx->surf[index]->val1 >> 16) & 0xffff;
+      uint32_t first_layer = sub_ctx->surf[index]->first_layer;
+      uint32_t last_layer = sub_ctx->surf[index]->last_layer;
 
-      vrend_fb_bind_texture_id(surf->texture, surf->id, index, surf->val0,
-                               first_layer != last_layer ? 0xffffffff : first_layer,
+      vrend_fb_bind_texture_id(surf->texture, surf->gl_id, index, surf->level,
+                               first_layer != last_layer ? -1 : (GLint)first_layer,
                                surf->nr_samples);
    }
 }
@@ -3009,6 +3151,11 @@ static void vrend_hw_emit_framebuffer_state(struct vrend_sub_context *sub_ctx)
 
    if (sub_ctx->nr_cbufs == 0) {
       glReadBuffer(GL_NONE);
+      /* In core profile, must explicitly disable draw buffers when no color attachments */
+      if (vrend_state.use_core_profile) {
+         GLenum none_buf = GL_NONE;
+         glDrawBuffers(1, &none_buf);
+      }
       if (has_feature(feat_srgb_write_control)) {
          glDisable(GL_FRAMEBUFFER_SRGB_EXT);
          sub_ctx->framebuffer_srgb_enabled = false;
@@ -3016,8 +3163,7 @@ static void vrend_hw_emit_framebuffer_state(struct vrend_sub_context *sub_ctx)
    } else if (has_feature(feat_srgb_write_control)) {
       struct vrend_surface *surf = NULL;
       bool use_srgb = false;
-      int i;
-      for (i = 0; i < sub_ctx->nr_cbufs; i++) {
+      for (uint32_t i = 0; i < sub_ctx->nr_cbufs; i++) {
          if (sub_ctx->surf[i]) {
             surf = sub_ctx->surf[i];
             if (util_format_is_srgb(surf->format)) {
@@ -3036,7 +3182,7 @@ static void vrend_hw_emit_framebuffer_state(struct vrend_sub_context *sub_ctx)
 
    sub_ctx->swizzle_output_rgb_to_bgr = 0;
    sub_ctx->needs_manual_srgb_encode_bitmask = 0;
-   for (int i = 0; i < sub_ctx->nr_cbufs; i++) {
+   for (uint32_t i = 0; i < sub_ctx->nr_cbufs; i++) {
       struct vrend_surface *surf = sub_ctx->surf[i];
       if (!surf)
          continue;
@@ -3071,8 +3217,7 @@ void vrend_set_framebuffer_state(struct vrend_context *ctx,
                                  uint32_t zsurf_handle)
 {
    struct vrend_surface *surf, *zsurf;
-   int i;
-   int old_num;
+   uint32_t old_num;
    GLenum status;
    GLint new_height = -1;
    bool new_fbo_origin_upper_left = false;
@@ -3098,7 +3243,7 @@ void vrend_set_framebuffer_state(struct vrend_context *ctx,
    old_num = sub_ctx->nr_cbufs;
    sub_ctx->nr_cbufs = nr_cbufs;
 
-   for (i = 0; i < (int)nr_cbufs; i++) {
+   for (uint32_t i = 0; i < nr_cbufs; i++) {
       if (surf_handle[i] != 0) {
          surf = vrend_object_lookup(sub_ctx->object_hash, surf_handle[i], VIRGL_OBJECT_SURFACE);
          if (!surf) {
@@ -3115,7 +3260,7 @@ void vrend_set_framebuffer_state(struct vrend_context *ctx,
    }
 
    if (old_num > sub_ctx->nr_cbufs) {
-      for (i = sub_ctx->nr_cbufs; i < old_num; i++) {
+      for (uint32_t i = sub_ctx->nr_cbufs; i < old_num; i++) {
          vrend_surface_reference(&sub_ctx->surf[i], NULL);
          vrend_hw_set_color_surface(sub_ctx, i);
       }
@@ -3126,22 +3271,22 @@ void vrend_set_framebuffer_state(struct vrend_context *ctx,
       new_height = 0;
       new_fbo_origin_upper_left = false;
    } else if (sub_ctx->nr_cbufs == 0) {
-      new_height = u_minify(sub_ctx->zsurf->texture->base.height0, sub_ctx->zsurf->val0);
+      new_height = u_minify(sub_ctx->zsurf->texture->base.height0, sub_ctx->zsurf->level);
       new_fbo_origin_upper_left = sub_ctx->zsurf->texture->y_0_top ? true : false;
    }
    else {
       surf = NULL;
-      for (i = 0; i < sub_ctx->nr_cbufs; i++) {
+      for (uint32_t i = 0; i < sub_ctx->nr_cbufs; i++) {
          if (sub_ctx->surf[i]) {
             surf = sub_ctx->surf[i];
             break;
          }
       }
       if (surf == NULL) {
-         vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_SURFACE, i);
+         vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_SURFACE, sub_ctx->nr_cbufs);
          return;
       }
-      new_height = u_minify(surf->texture->base.height0, surf->val0);
+      new_height = u_minify(surf->texture->base.height0, surf->level);
       new_fbo_origin_upper_left = surf->texture->y_0_top ? true : false;
    }
 
@@ -3159,7 +3304,7 @@ void vrend_set_framebuffer_state(struct vrend_context *ctx,
    if (sub_ctx->nr_cbufs > 0 || sub_ctx->zsurf) {
       status = glCheckFramebufferStatus(GL_FRAMEBUFFER);
       if (status != GL_FRAMEBUFFER_COMPLETE)
-         vrend_printf("failed to complete framebuffer 0x%x %s\n", status, ctx->debug_name);
+         virgl_error("Failed to complete framebuffer 0x%x %s\n", status, ctx->debug_name);
    }
 
    sub_ctx->shader_dirty = true;
@@ -3199,13 +3344,7 @@ void vrend_set_viewport_states(struct vrend_context *ctx,
    GLsizei width, height;
    GLclampd near_val, far_val;
    bool viewport_is_negative = (state[0].scale[1] < 0) ? true : false;
-   uint i, idx;
-
-   if (num_viewports > PIPE_MAX_VIEWPORTS ||
-       start_slot > (PIPE_MAX_VIEWPORTS - num_viewports)) {
-      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_CMD_BUFFER, num_viewports);
-      return;
-   }
+   unsigned i, idx;
 
    for (i = 0; i < num_viewports; i++) {
       GLfloat abs_s1 = fabsf(state[i].scale[1]);
@@ -3268,12 +3407,9 @@ int vrend_create_vertex_elements_state(struct vrend_context *ctx,
    struct vrend_vertex_element_array *v;
    const struct util_format_description *desc;
    GLenum type;
-   uint i;
+   unsigned i;
    uint32_t ret_handle;
 
-   if (num_elements > PIPE_MAX_ATTRIBS)
-      return EINVAL;
-
    v = CALLOC_STRUCT(vrend_vertex_element_array);
    if (!v)
       return ENOMEM;
@@ -3282,12 +3418,12 @@ int vrend_create_vertex_elements_state(struct vrend_context *ctx,
    for (i = 0; i < num_elements; i++) {
       memcpy(&v->elements[i].base, &elements[i], sizeof(struct pipe_vertex_element));
 
-      desc = util_format_description(elements[i].src_format);
-      if (!desc) {
+      if (unlikely(elements[i].src_format >= PIPE_FORMAT_COUNT)) {
          FREE(v);
          return EINVAL;
       }
 
+      desc = util_format_description(elements[i].src_format);
       type = GL_FALSE;
       switch (desc->channel[0].type) {
       case UTIL_FORMAT_TYPE_FLOAT:
@@ -3341,35 +3477,12 @@ int vrend_create_vertex_elements_state(struct vrend_context *ctx,
       v->elements[i].type = type;
       if (desc->channel[0].normalized)
          v->elements[i].norm = GL_TRUE;
-      if (elements[i].src_format == PIPE_FORMAT_R11G11B10_FLOAT)
-         v->elements[i].nr_chan = 3;
-      else
-         v->elements[i].nr_chan = desc->nr_channels;
+      v->elements[i].nr_chan = desc->nr_channels;
 
-      if (desc->nr_channels == 4 && desc->swizzle[0] == UTIL_FORMAT_SWIZZLE_Z)
+      if (desc->nr_channels == 4 && desc->swizzle[0] == PIPE_SWIZZLE_Z)
          v->zyxw_bitmask |= 1 << i;
    }
 
-   if (has_feature(feat_gles31_vertex_attrib_binding)) {
-      glGenVertexArrays(1, &v->id);
-      glBindVertexArray(v->id);
-      for (i = 0; i < num_elements; i++) {
-         struct vrend_vertex_element *ve = &v->elements[i];
-         GLint size = !vrend_state.use_gles && (v->zyxw_bitmask & (1 << i)) ? GL_BGRA : ve->nr_chan;
-
-         if (util_format_is_pure_integer(ve->base.src_format)) {
-            UPDATE_INT_SIGN_MASK(ve->base.src_format, i,
-                                 v->signed_int_bitmask,
-                                 v->unsigned_int_bitmask);
-            glVertexAttribIFormat(i, size, ve->type, ve->base.src_offset);
-         }
-         else
-            glVertexAttribFormat(i, size, ve->type, ve->norm, ve->base.src_offset);
-         glVertexAttribBinding(i, ve->base.vertex_buffer_index);
-         glVertexBindingDivisor(i, ve->base.instance_divisor);
-         glEnableVertexAttribArray(i);
-      }
-   }
    ret_handle = vrend_renderer_object_insert(ctx, v, handle,
                                              VIRGL_OBJECT_VERTEX_ELEMENTS);
    if (!ret_handle) {
@@ -3398,6 +3511,44 @@ void vrend_bind_vertex_elements_state(struct vrend_context *ctx,
    if (ctx->sub->ve != v)
       ctx->sub->vbo_dirty = true;
    ctx->sub->ve = v;
+
+   if (v->count > vrend_state.max_vertex_attributes) {
+      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_TOO_MANY_VERTEX_ATTRIBUTES, handle);
+      return;
+   }
+
+   if (has_feature(feat_gles31_vertex_attrib_binding)) {
+      if (v->id == 0) {
+      glGenVertexArrays(1, &v->id);
+      glBindVertexArray(v->id);
+      for (uint32_t i = 0; i < v->count; i++) {
+         struct vrend_vertex_element *ve = &v->elements[i];
+         GLint size = !vrend_state.use_gles && (v->zyxw_bitmask & (1 << i)) ? GL_BGRA : ve->nr_chan;
+
+         if (util_format_is_pure_integer(ve->base.src_format)) {
+            UPDATE_INT_SIGN_MASK(ve->base.src_format, i,
+                                 v->signed_int_bitmask,
+                                 v->unsigned_int_bitmask);
+            glVertexAttribIFormat(i, size, ve->type, ve->base.src_offset);
+         }
+         else
+            glVertexAttribFormat(i, size, ve->type, ve->norm, ve->base.src_offset);
+         glVertexAttribBinding(i, ve->base.vertex_buffer_index);
+         glVertexBindingDivisor(i, ve->base.instance_divisor);
+         glEnableVertexAttribArray(i);
+         }
+      }
+   } else {
+      for (uint32_t i = 0; i < v->count; i++) {
+         struct vrend_vertex_element *ve = &v->elements[i];
+
+         if (util_format_is_pure_integer(ve->base.src_format)) {
+            UPDATE_INT_SIGN_MASK(ve->base.src_format, i,
+                                 v->signed_int_bitmask,
+                                 v->unsigned_int_bitmask);
+         }
+      }
+   }
 }
 
 void vrend_set_constants(struct vrend_context *ctx,
@@ -3422,7 +3573,9 @@ void vrend_set_constants(struct vrend_context *ctx,
       consts->num_allocated_consts = num_constant;
    }
 
-   memcpy(consts->consts, data, num_constant * sizeof(unsigned int));
+   if (num_constant > 0)
+      memcpy(consts->consts, data, num_constant * sizeof(unsigned int));
+
    consts->num_consts = num_constant;
 }
 
@@ -3441,7 +3594,7 @@ void vrend_set_uniform_buffer(struct vrend_context *ctx,
    if (res_handle) {
       res = vrend_renderer_ctx_res_lookup(ctx, res_handle);
 
-      if (!res) {
+      if (!res || !res->gl_id) {
          vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, res_handle);
          return;
       }
@@ -3469,20 +3622,17 @@ void vrend_set_index_buffer(struct vrend_context *ctx,
    ctx->sub->ib.index_size = index_size;
    ctx->sub->ib.offset = offset;
    if (res_handle) {
-      if (ctx->sub->index_buffer_res_id != res_handle) {
-         res = vrend_renderer_ctx_res_lookup(ctx, res_handle);
-         if (!res) {
+      res = vrend_renderer_ctx_res_lookup(ctx, res_handle);
+      if ((struct vrend_resource *)ctx->sub->ib.buffer != res) {
+         if (!res || !res->gl_id) {
             vrend_resource_reference((struct vrend_resource **)&ctx->sub->ib.buffer, NULL);
-            ctx->sub->index_buffer_res_id = 0;
             vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, res_handle);
             return;
          }
          vrend_resource_reference((struct vrend_resource **)&ctx->sub->ib.buffer, res);
-         ctx->sub->index_buffer_res_id = res_handle;
       }
    } else {
       vrend_resource_reference((struct vrend_resource **)&ctx->sub->ib.buffer, NULL);
-      ctx->sub->index_buffer_res_id = 0;
    }
 }
 
@@ -3508,7 +3658,7 @@ void vrend_set_single_vbo(struct vrend_context *ctx,
       vbo->res_id = 0;
    } else if (vbo->res_id != res_handle) {
       res = vrend_renderer_ctx_res_lookup(ctx, res_handle);
-      if (!res) {
+      if (!res || !res->gl_id) {
          vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, res_handle);
          vbo->res_id = 0;
          return;
@@ -3582,7 +3732,7 @@ static GLenum vrend_get_arb_format(enum virgl_formats format)
    case VIRGL_FORMAT_I32_SINT: return GL_R32I;
    case VIRGL_FORMAT_I32_UINT: return GL_R32UI;
    default:
-      vrend_printf("Texture format %s unsupported for texture buffers\n", util_format_name(format));
+      virgl_warn("Texture format %s unsupported for texture buffers\n", util_format_name(format));
       return GL_R8;
    }
 }
@@ -3592,30 +3742,32 @@ void vrend_set_single_sampler_view(struct vrend_context *ctx,
                                    uint32_t index,
                                    uint32_t handle)
 {
+   struct vrend_shader_view *shader_view = &ctx->sub->views[shader_type];
    struct vrend_sampler_view *view = NULL;
    struct vrend_texture *tex;
 
    if (handle) {
       view = vrend_object_lookup(ctx->sub->object_hash, handle, VIRGL_OBJECT_SAMPLER_VIEW);
       if (!view) {
-         ctx->sub->views[shader_type].views[index] = NULL;
+         vrend_sampler_view_reference(&shader_view->views[index], NULL);
          vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_HANDLE, handle);
          return;
       }
-      if (ctx->sub->views[shader_type].views[index] == view) {
+      if (shader_view->views[index] == view) {
          return;
       }
       /* we should have a reference to this texture taken at create time */
       tex = (struct vrend_texture *)view->texture;
       if (!tex) {
+         virgl_warn("sampler view %u missing texture reference\n", handle);
          return;
       }
 
-      ctx->sub->sampler_views_dirty[shader_type] |= 1u << index;
+      shader_view->dirty_mask |= (1u << index);
 
       if (!has_bit(view->texture->storage_bits, VREND_STORAGE_GL_BUFFER)) {
-         if (view->texture->id == view->id) {
-            glBindTexture(view->target, view->id);
+         if (view->texture->gl_id == view->gl_id) {
+            glBindTexture(view->target, view->gl_id);
 
             if (util_format_is_depth_or_stencil(view->format)) {
                if (vrend_state.use_core_profile == false) {
@@ -3632,17 +3784,15 @@ void vrend_set_single_sampler_view(struct vrend_context *ctx,
                }
             }
 
-            GLuint base_level = view->val1 & 0xff;
-            GLuint max_level = (view->val1 >> 8) & 0xff;
-            view->levels = max_level - base_level + 1;
+            view->levels = view->u.tex.last_level - view->u.tex.first_level + 1;
 
-            if (tex->cur_base != base_level) {
-               glTexParameteri(view->texture->target, GL_TEXTURE_BASE_LEVEL, base_level);
-               tex->cur_base = base_level;
+            if (tex->cur_base != view->u.tex.first_level) {
+               glTexParameteri(view->texture->target, GL_TEXTURE_BASE_LEVEL, view->u.tex.first_level);
+               tex->cur_base = view->u.tex.first_level;
             }
-            if (tex->cur_max != max_level) {
-               glTexParameteri(view->texture->target, GL_TEXTURE_MAX_LEVEL, max_level);
-               tex->cur_max = max_level;
+            if (tex->cur_max != view->u.tex.last_level) {
+               glTexParameteri(view->texture->target, GL_TEXTURE_MAX_LEVEL, view->u.tex.last_level);
+               tex->cur_max = view->u.tex.last_level;
             }
             if (memcmp(tex->cur_swizzle, view->gl_swizzle, 4 * sizeof(GLint))) {
                if (vrend_state.use_gles) {
@@ -3657,9 +3807,7 @@ void vrend_set_single_sampler_view(struct vrend_context *ctx,
             }
 
             if (tex->cur_srgb_decode != view->srgb_decode && util_format_is_srgb(tex->base.base.format)) {
-               if (has_feature(feat_samplers))
-                  ctx->sub->sampler_views_dirty[shader_type] |= (1u << index);
-               else if (has_feature(feat_texture_srgb_decode)) {
+               if (!has_feature(feat_samplers) && has_feature(feat_texture_srgb_decode)) {
                   glTexParameteri(view->texture->target, GL_TEXTURE_SRGB_DECODE_EXT,
                                   view->srgb_decode);
                   tex->cur_srgb_decode = view->srgb_decode;
@@ -3682,21 +3830,21 @@ void vrend_set_single_sampler_view(struct vrend_context *ctx,
          }
 
          if (has_feature(feat_texture_buffer_range)) {
-            unsigned offset = view->val0;
-            unsigned size = view->val1 - view->val0 + 1;
+            unsigned offset = view->u.buf.first_element;
+            unsigned size = view->u.buf.last_element - view->u.buf.first_element + 1;
             int blsize = util_format_get_blocksize(view->format);
 
             if (offset + size > vrend_state.max_texture_buffer_size)
                size = vrend_state.max_texture_buffer_size - offset;
             offset *= blsize;
             size *= blsize;
-            glTexBufferRange(GL_TEXTURE_BUFFER, internalformat, view->texture->id, offset, size);
+            glTexBufferRange(GL_TEXTURE_BUFFER, internalformat, view->texture->gl_id, offset, size);
          } else
-            glTexBuffer(GL_TEXTURE_BUFFER, internalformat, view->texture->id);
+            glTexBuffer(GL_TEXTURE_BUFFER, internalformat, view->texture->gl_id);
       }
    }
 
-   vrend_sampler_view_reference(&ctx->sub->views[shader_type].views[index], view);
+   vrend_sampler_view_reference(&shader_view->views[index], view);
 }
 
 void vrend_set_num_sampler_views(struct vrend_context *ctx,
@@ -3707,17 +3855,18 @@ void vrend_set_num_sampler_views(struct vrend_context *ctx,
    int last_slot = start_slot + num_sampler_views;
    int i;
 
-   for (i = last_slot; i < ctx->sub->views[shader_type].num_views; i++)
-      vrend_sampler_view_reference(&ctx->sub->views[shader_type].views[i], NULL);
+   struct vrend_shader_view *shader_view = &ctx->sub->views[shader_type];
+   for (i = last_slot; i < shader_view->max_num_views; i++)
+      vrend_sampler_view_reference(&shader_view->views[i], NULL);
 
-   ctx->sub->views[shader_type].num_views = last_slot;
+   shader_view->max_num_views = last_slot;
 }
 
-void vrend_set_single_image_view(struct vrend_context *ctx,
+int vrend_set_single_image_view(struct vrend_context *ctx,
                                  uint32_t shader_type,
                                  uint32_t index,
                                  uint32_t format, uint32_t access,
-                                 uint32_t layer_offset, uint32_t level_size,
+                                 uint32_t layers_or_offset, uint32_t level_or_size,
                                  uint32_t handle)
 {
    struct vrend_image_view *iview = &ctx->sub->image_views[shader_type][index];
@@ -3725,30 +3874,46 @@ void vrend_set_single_image_view(struct vrend_context *ctx,
 
    if (handle) {
       if (!has_feature(feat_images))
-         return;
+         return EINVAL;
 
       if (unlikely(format >= ARRAY_SIZE(tex_conv_table))) {
          vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_FORMAT, format);
-         return;
+         return EINVAL;
       }
 
       res = vrend_renderer_ctx_res_lookup(ctx, handle);
-      if (!res) {
+      if (!res || !res->gl_id) {
          vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, handle);
-         return;
+         return EINVAL;
+      }
+
+      if (has_bit(res->storage_bits, VREND_STORAGE_GL_TEXTURE)) {
+         uint16_t first_layer = layers_or_offset & 0xffff;
+         uint16_t last_layer = (layers_or_offset >> 16) & 0xffff;
+         if (last_layer - first_layer + 1 == 0)
+            return EINVAL;
+
+         iview->u.tex.last_layer = last_layer;
+         iview->u.tex.first_layer = first_layer;
+         iview->u.tex.level = level_or_size;
+      } else {
+         iview->u.buf.offset = layers_or_offset;
+         iview->u.buf.size = level_or_size;
       }
+
+
       vrend_resource_reference(&iview->texture, res);
       iview->vformat = format;
       iview->format = tex_conv_table[format].internalformat;
       iview->access = access;
-      iview->u.buf.offset = layer_offset;
-      iview->u.buf.size = level_size;
       ctx->sub->images_used_mask[shader_type] |= (1u << index);
    } else {
       vrend_resource_reference(&iview->texture, NULL);
       iview->format = 0;
       ctx->sub->images_used_mask[shader_type] &= ~(1u << index);
    }
+
+   return 0;
 }
 
 void vrend_set_single_ssbo(struct vrend_context *ctx,
@@ -3765,11 +3930,16 @@ void vrend_set_single_ssbo(struct vrend_context *ctx,
 
    if (handle) {
       res = vrend_renderer_ctx_res_lookup(ctx, handle);
-      if (!res) {
+      if (!res || !res->gl_id) {
          vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, handle);
          return;
       }
 
+      if (offset > res->base.width0 || length > res->base.width0 - offset) {
+         vrend_report_context_error(ctx, VIRGL_ERROR_CTX_SSBO_BINDING_RANGE, handle);
+         return;
+      }
+
       vrend_resource_reference(&ssbo->res, res);
       ssbo->buffer_offset = offset;
       ssbo->buffer_size = length;
@@ -3795,7 +3965,7 @@ void vrend_set_single_abo(struct vrend_context *ctx,
 
    if (handle) {
       res = vrend_renderer_ctx_res_lookup(ctx, handle);
-      if (!res) {
+      if (!res || !res->gl_id) {
          vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, handle);
          return;
       }
@@ -4046,12 +4216,12 @@ static inline void vrend_sync_shader_io(struct vrend_sub_context *sub_ctx,
 static bool vrend_get_swizzle(struct vrend_sampler_view *view,
                               enum pipe_swizzle swizzle[4])
 {
-   static const enum pipe_swizzle OOOR[] = {PIPE_SWIZZLE_ZERO, PIPE_SWIZZLE_ZERO, PIPE_SWIZZLE_ZERO, PIPE_SWIZZLE_RED};
-   static const enum pipe_swizzle RRR1[] = {PIPE_SWIZZLE_RED, PIPE_SWIZZLE_RED, PIPE_SWIZZLE_RED, PIPE_SWIZZLE_ONE};
-   static const enum pipe_swizzle RRRG[] = {PIPE_SWIZZLE_RED, PIPE_SWIZZLE_RED, PIPE_SWIZZLE_RED, PIPE_SWIZZLE_GREEN};
-   static const enum pipe_swizzle RRRR[] = {PIPE_SWIZZLE_RED, PIPE_SWIZZLE_RED, PIPE_SWIZZLE_RED, PIPE_SWIZZLE_RED};
-   static const enum pipe_swizzle RG01[] = {PIPE_SWIZZLE_RED, PIPE_SWIZZLE_GREEN, PIPE_SWIZZLE_ZERO, PIPE_SWIZZLE_ONE};
-   static const enum pipe_swizzle R001[] = {PIPE_SWIZZLE_RED, PIPE_SWIZZLE_ZERO, PIPE_SWIZZLE_ZERO, PIPE_SWIZZLE_ONE};
+   static const enum pipe_swizzle OOOR[] = {PIPE_SWIZZLE_0, PIPE_SWIZZLE_0, PIPE_SWIZZLE_0, PIPE_SWIZZLE_X};
+   static const enum pipe_swizzle RRR1[] = {PIPE_SWIZZLE_X, PIPE_SWIZZLE_X, PIPE_SWIZZLE_X, PIPE_SWIZZLE_1};
+   static const enum pipe_swizzle RRRG[] = {PIPE_SWIZZLE_X, PIPE_SWIZZLE_X, PIPE_SWIZZLE_X, PIPE_SWIZZLE_Y};
+   static const enum pipe_swizzle RRRR[] = {PIPE_SWIZZLE_X, PIPE_SWIZZLE_X, PIPE_SWIZZLE_X, PIPE_SWIZZLE_X};
+   static const enum pipe_swizzle RG01[] = {PIPE_SWIZZLE_X, PIPE_SWIZZLE_Y, PIPE_SWIZZLE_0, PIPE_SWIZZLE_1};
+   static const enum pipe_swizzle R001[] = {PIPE_SWIZZLE_X, PIPE_SWIZZLE_0, PIPE_SWIZZLE_0, PIPE_SWIZZLE_1};
 
    if (tex_conv_table[view->format].flags & VIRGL_TEXTURE_NEED_SWIZZLE) {
       swizzle[0] = tex_conv_table[view->format].swizzle[0];
@@ -4152,6 +4322,11 @@ static bool vrend_get_swizzle(struct vrend_sampler_view *view,
    return false;
 }
 
+static inline bool
+vrend_shader_use_core(struct vrend_context *ctx)
+{
+   return ctx->shader_cfg.glsl_version >= 140;
+}
 
 static inline void vrend_fill_shader_key(struct vrend_sub_context *sub_ctx,
                                          struct vrend_shader_selector *sel,
@@ -4160,7 +4335,6 @@ static inline void vrend_fill_shader_key(struct vrend_sub_context *sub_ctx,
    enum pipe_shader_type type = sel->type;
 
    if (vrend_state.use_core_profile) {
-      int i;
       bool add_alpha_test = true;
 
       /* Only use integer info when drawing to avoid stale info.
@@ -4172,7 +4346,7 @@ static inline void vrend_fill_shader_key(struct vrend_sub_context *sub_ctx,
          key->vs.attrib_unsigned_int_bitmask = sub_ctx->ve->unsigned_int_bitmask;
       }
       if (type == PIPE_SHADER_FRAGMENT) {
-         for (i = 0; i < sub_ctx->nr_cbufs; i++) {
+         for (uint32_t i = 0; i < sub_ctx->nr_cbufs; i++) {
             if (!sub_ctx->surf[i])
                continue;
             if (vrend_format_is_emulated_alpha(sub_ctx->surf[i]->format))
@@ -4193,10 +4367,11 @@ static inline void vrend_fill_shader_key(struct vrend_sub_context *sub_ctx,
             key->alpha_test = sub_ctx->dsa_state.alpha.func;
          }
       }
+   }
 
+   if (vrend_shader_use_core(sub_ctx->parent)) {
       key->pstipple_enabled = sub_ctx->rs_state.poly_stipple_enable;
       key->color_two_side = sub_ctx->rs_state.light_twoside;
-
       key->flatshade = sub_ctx->rs_state.flatshade ? true : false;
    }
 
@@ -4214,7 +4389,7 @@ static inline void vrend_fill_shader_key(struct vrend_sub_context *sub_ctx,
    if (type == PIPE_SHADER_GEOMETRY)
       key->gs.emit_clip_distance = sub_ctx->rs_state.clip_plane_enable != 0;
 
-   for (int i = 0; i < sub_ctx->views[type].num_views; i++) {
+   for (int i = 0; i < sub_ctx->views[type].max_num_views; i++) {
       struct vrend_sampler_view *view = sub_ctx->views[type].views[i];
       if (!view)
          continue;
@@ -4240,13 +4415,11 @@ static int vrend_shader_create(struct vrend_context *ctx,
                                struct vrend_shader *shader,
                                struct vrend_shader_key *key)
 {
-   static uint32_t uid;
-
-   shader->uid = ++uid;
-
    if (shader->sel->tokens) {
 
-      VREND_DEBUG(dbg_shader_tgsi, ctx, "shader\n%s\n", shader->sel->tmp_buf);
+      VREND_DEBUG(dbg_shader_tgsi, ctx, "TGSI received:");
+      VREND_DEBUG_EXT(dbg_shader_tgsi, ctx, vrend_dump_tgsi(shader->sel->tokens, 0));
+      VREND_DEBUG(dbg_shader_tgsi, ctx, "\n");
 
       bool ret = vrend_convert_shader(ctx, &ctx->shader_cfg, shader->sel->tokens,
                                       shader->sel->req_local_mem, key, &shader->sel->sinfo,
@@ -4344,33 +4517,88 @@ static int vrend_finish_shader(struct vrend_context *ctx,
    return vrend_shader_select(ctx->sub, sel, NULL) ? EINVAL : 0;
 }
 
-int vrend_create_shader(struct vrend_context *ctx,
-                        uint32_t handle,
-                        const struct pipe_stream_output_info *so_info,
-                        uint32_t req_local_mem,
-                        const char *shd_text, uint32_t offlen, uint32_t num_tokens,
-                        enum pipe_shader_type type, uint32_t pkt_length)
+static int vrend_shader_assign_tgsi(struct vrend_context *ctx,
+                                    struct vrend_shader_selector *sel,
+                                    const char *shader_buf,
+                                    uint32_t current_length,
+                                    uint32_t num_tokens)
 {
-   struct vrend_shader_selector *sel = NULL;
-   int ret_handle;
-   bool finished = false;
-   int ret;
+   struct tgsi_token *tokens;
 
-   if (type > PIPE_SHADER_COMPUTE)
+   /* check for null termination */
+   if (current_length < 4 || !memchr(shader_buf + current_length - 4, '\0', 4))
       return EINVAL;
 
-   if (type == PIPE_SHADER_GEOMETRY &&
-       !has_feature(feat_geometry_shader))
+   tokens = calloc(num_tokens + 10, sizeof(struct tgsi_token));
+   if (!tokens)
+      return ENOMEM;
+
+   if (!tgsi_text_translate(shader_buf, tokens, num_tokens + 10)) {
+      free(tokens);
       return EINVAL;
+   }
+
+   if (vrend_finish_shader(ctx, sel, tokens)) {
+      free(tokens);
+      return EINVAL;
+   }
+
+   free(tokens);
+   return 0;
+}
+
+static int vrend_shader_store_long_shader(uint32_t handle,
+                                          struct vrend_shader_selector *sel,
+                                          uint32_t pkt_length_bytes,
+                                          uint32_t expected_token_count,
+                                          const char *shd_text,
+                                          struct vrend_long_shader_buffer **lsb)
+{
+   /* We only got a partial shader, start a long shader transfer */
+   struct vrend_long_shader_buffer *lsbuf = CALLOC_STRUCT(vrend_long_shader_buffer);
+   if (!lsbuf)
+      return ENOMEM;
+
+   lsbuf->handle = handle;
+   vrend_shader_state_reference(&lsbuf->sel, sel);
+   lsbuf->current_length = pkt_length_bytes;
+   lsbuf->total_length = expected_token_count * 4;
+   lsbuf->tmp_buf = malloc(lsbuf->total_length);
+   if (!lsbuf->tmp_buf) {
+      vrend_destroy_long_shader_buffer(lsbuf);
+      return ENOMEM;
+   }
+
+   memcpy(lsbuf->tmp_buf, shd_text, pkt_length_bytes);
+   *lsb = lsbuf;
+   return 0;
+}
+
+int vrend_create_shader(struct vrend_context *ctx,
+                        uint32_t handle,
+                        const struct pipe_stream_output_info *so_info,
+                        uint32_t req_local_mem,
+                        const char *shd_text, uint32_t offlen, uint32_t num_tokens,
+                        enum pipe_shader_type type, uint32_t pkt_length)
+{
+   if (type == PIPE_SHADER_GEOMETRY &&
+       !has_feature(feat_geometry_shader)) {
+      virgl_error("Geometry shader not supported\n");
+      return EINVAL;
+   }
 
    if ((type == PIPE_SHADER_TESS_CTRL ||
         type == PIPE_SHADER_TESS_EVAL) &&
-       !has_feature(feat_tessellation))
+       !has_feature(feat_tessellation)) {
+      virgl_error("Tesselation shaders not supported\n");
        return EINVAL;
+   }
 
    if (type == PIPE_SHADER_COMPUTE &&
-       !has_feature(feat_compute_shader))
+       !has_feature(feat_compute_shader)) {
+      virgl_error("Compute shaders not supported\n");
       return EINVAL;
+   }
 
    /* offlen & VIRGL_OBJ_SHADER_OFFSET_CONT declares whether we have a new shader or
     * a shader continuation
@@ -4383,128 +4611,110 @@ int vrend_create_shader(struct vrend_context *ctx,
 
    /* if we have an in progress one - don't allow a new shader
       of that type or a different handle. */
-   if (sub_ctx->long_shader_in_progress_handle[type]) {
-      if (new_shader == true)
+   if (sub_ctx->long_shader_in_progress[type]) {
+      if (new_shader == true) {
+         virgl_error("Expected long shader continuation, got new shader\n");
          return EINVAL;
-      if (handle != sub_ctx->long_shader_in_progress_handle[type])
+      }
+      if (handle != sub_ctx->long_shader_in_progress[type]->handle) {
+         virgl_error("Long shader continuation handle invalid\n");
          return EINVAL;
+      }
+   }
+
+   /* Ensure that we won't hit an overflow */
+   if (pkt_length >= (UINT32_MAX >> 2)) {
+      virgl_error("Packed length overflow\n");
+      return EINVAL;
    }
 
    const uint32_t pkt_length_bytes = pkt_length * 4;
 
    if (new_shader) {
       const uint32_t expected_token_count = (offlen + 3) / 4;  /* round up count */
-      if (expected_token_count < pkt_length)
+      if (expected_token_count < pkt_length) {
+         virgl_error("Invalid expected token count\n");
         return EINVAL;
+      }
 
+      struct vrend_shader_selector *sel;
       sel = vrend_create_shader_state(so_info, req_local_mem, type);
-      if (sel == NULL)
+      if (sel == NULL) {
+         virgl_error("Unable to allocate shader state\n");
          return ENOMEM;
+      }
 
-      sel->buf_len = expected_token_count * 4;
-      sel->tmp_buf = malloc(sel->buf_len);
-      if (!sel->tmp_buf) {
-         ret = ENOMEM;
-         goto error;
+      int ret_handle = vrend_renderer_object_insert(ctx, sel, handle, VIRGL_OBJECT_SHADER);
+      if (ret_handle == 0) {
+         vrend_destroy_shader_selector(sel);
+         return ENOMEM;
       }
 
-      memcpy(sel->tmp_buf, shd_text, pkt_length_bytes);
       if (expected_token_count > pkt_length) {
-         sel->buf_offset = pkt_length_bytes;
-         sub_ctx->long_shader_in_progress_handle[type] = handle;
-      } else
-         finished = true;
+         /* We only got a partial shader, start a long shader transfer */
+         int ret = vrend_shader_store_long_shader(handle, sel,
+                                                  pkt_length_bytes, expected_token_count,
+                                                  shd_text,
+                                                  &sub_ctx->long_shader_in_progress[type]);
+         if (ret != 0) {
+            vrend_renderer_object_destroy(ctx, handle);
+            virgl_error("Error storing long shader\n");
+            return ret;
+         }
+      } else {
+         int ret = vrend_shader_assign_tgsi(ctx, sel,
+                                            shd_text, pkt_length_bytes,
+                                            num_tokens);
+         if (ret != 0) {
+            vrend_renderer_object_destroy(ctx, handle);
+            virgl_error("Error assigning TGSI\n");
+            return ret;
+         }
+      }
    } else {
-      sel = vrend_object_lookup(sub_ctx->object_hash, handle, VIRGL_OBJECT_SHADER);
-      if (!sel) {
-         vrend_printf( "got continuation without original shader %d\n", handle);
-         ret = EINVAL;
-         goto error;
+      struct vrend_long_shader_buffer *lsbuf = sub_ctx->long_shader_in_progress[type];
+      if (!lsbuf) {
+         virgl_error("Got continuation without original long shader %u\n", handle);
+         vrend_renderer_object_destroy(ctx, handle);
+         return EINVAL;
       }
 
       offlen &= ~VIRGL_OBJ_SHADER_OFFSET_CONT;
-      if (offlen != sel->buf_offset) {
-         vrend_printf( "Got mismatched shader continuation %d vs %d\n",
-                 offlen, sel->buf_offset);
-         ret = EINVAL;
-         goto error;
-      }
-
-      /*make sure no overflow */
-      if (pkt_length_bytes < pkt_length ||
-          pkt_length_bytes + sel->buf_offset < pkt_length_bytes ||
-          pkt_length_bytes + sel->buf_offset < sel->buf_offset) {
-            ret = EINVAL;
-            goto error;
-          }
-
-      if ((pkt_length_bytes + sel->buf_offset) > sel->buf_len) {
-         vrend_printf("Got too large shader continuation %d vs %d\n",
-                      pkt_length_bytes + sel->buf_offset, sel->buf_len);
-         ret = EINVAL;
-         goto error;
-      }
-
-      memcpy(sel->tmp_buf + sel->buf_offset, shd_text, pkt_length_bytes);
-
-      sel->buf_offset += pkt_length_bytes;
-      if (sel->buf_offset >= sel->buf_len) {
-         finished = true;
-         shd_text = sel->tmp_buf;
-      }
-   }
-
-   if (finished) {
-      struct tgsi_token *tokens;
-
-      /* check for null termination */
-      uint32_t last_chunk_offset = sel->buf_offset ? sel->buf_offset : pkt_length_bytes;
-      if (last_chunk_offset < 4 || !memchr(shd_text + last_chunk_offset - 4, '\0', 4)) {
-         ret = EINVAL;
-         goto error;
-      }
-
-      tokens = calloc(num_tokens + 10, sizeof(struct tgsi_token));
-      if (!tokens) {
-         ret = ENOMEM;
-         goto error;
-      }
-
-      if (!tgsi_text_translate((const char *)shd_text, tokens, num_tokens + 10)) {
-         free(tokens);
-         ret = EINVAL;
-         goto error;
+      if (offlen != lsbuf->current_length) {
+         virgl_error("Got mismatched shader continuation %u vs %u\n",
+                 offlen, lsbuf->current_length);
+         sub_ctx->long_shader_in_progress[type] = NULL;
+         vrend_destroy_long_shader_buffer(lsbuf);
+         vrend_renderer_object_destroy(ctx, handle);
+         return EINVAL;
       }
 
-      if (vrend_finish_shader(ctx, sel, tokens)) {
-         free(tokens);
-         ret = EINVAL;
-         goto error;
-      } else if (!VREND_DEBUG_ENABLED) {
-         free(sel->tmp_buf);
-         sel->tmp_buf = NULL;
+      if (lsbuf->total_length - lsbuf->current_length < pkt_length_bytes) {
+         virgl_error("Got too large shader continuation %u vs %u\n",
+                      pkt_length_bytes + lsbuf->current_length, lsbuf->total_length);
+         sub_ctx->long_shader_in_progress[type] = NULL;
+         vrend_destroy_long_shader_buffer(lsbuf);
+         vrend_renderer_object_destroy(ctx, handle);
+         return EINVAL;
       }
-      free(tokens);
-      sub_ctx->long_shader_in_progress_handle[type] = 0;
-   }
 
-   if (new_shader) {
-      ret_handle = vrend_renderer_object_insert(ctx, sel, handle, VIRGL_OBJECT_SHADER);
-      if (ret_handle == 0) {
-         ret = ENOMEM;
-         goto error;
+      memcpy(lsbuf->tmp_buf + lsbuf->current_length, shd_text, pkt_length_bytes);
+      lsbuf->current_length += pkt_length_bytes;
+      if (lsbuf->current_length == lsbuf->total_length) {
+         int ret = vrend_shader_assign_tgsi(ctx, lsbuf->sel,
+                                            lsbuf->tmp_buf, lsbuf->current_length,
+                                            num_tokens);
+         sub_ctx->long_shader_in_progress[type] = NULL;
+         vrend_destroy_long_shader_buffer(lsbuf);
+         if (ret != 0) {
+            virgl_error("Error assigning TGSI\n");
+            vrend_renderer_object_destroy(ctx, handle);
+            return ret;
+         }
       }
    }
 
    return 0;
-
-error:
-   if (new_shader)
-      vrend_destroy_shader_selector(sel);
-   else
-      vrend_renderer_object_destroy(ctx, handle);
-
-   return ret;
 }
 
 void vrend_bind_shader(struct vrend_context *ctx,
@@ -4552,56 +4762,26 @@ vrend_color_encode_as_srgb(float color) {
       : 1.055f * powf(color, (1.f / 2.4f)) - 0.055f;
 }
 
-void vrend_clear(struct vrend_context *ctx,
-                 unsigned buffers,
-                 const union pipe_color_union *color,
-                 double depth, unsigned stencil)
-{
-   GLbitfield bits = 0;
-   struct vrend_sub_context *sub_ctx = ctx->sub;
-
-   if (ctx->in_error)
-      return;
-
-   if (ctx->ctx_switch_pending)
-      vrend_finish_context_switch(ctx);
-
-   vrend_update_frontface_state(sub_ctx);
-   if (sub_ctx->stencil_state_dirty)
-      vrend_update_stencil_state(sub_ctx);
-   if (sub_ctx->scissor_state_dirty)
-      vrend_update_scissor_state(sub_ctx);
-   if (sub_ctx->viewport_state_dirty)
-      vrend_update_viewport_state(sub_ctx);
-
-   vrend_use_program(ctx->sub, NULL);
-
-   glDisable(GL_SCISSOR_TEST);
-
-   float colorf[4];
-   memcpy(colorf, color->f, sizeof(colorf));
-
-   {
-      struct vrend_surface *surf = sub_ctx->surf[0];
-      if (sub_ctx->nr_cbufs && surf &&
-          util_format_is_srgb(surf->format) &&
-          !vrend_resource_supports_view(surf->texture, surf->format)) {
-         VREND_DEBUG(dbg_tex, ctx,
-                     "manually converting glClearColor from linear->srgb colorspace for EGL-backed framebuffer color attachment"
-                     " (surface format is %s; resource format is %s)\n",
-                     util_format_name(surf->format),
-                     util_format_name(surf->texture->base.format));
-         for (int i = 0; i < 3; ++i) // i < 3: don't convert alpha channel
-            colorf[i] = vrend_color_encode_as_srgb(colorf[i]);
-      }
+static void vrend_clear_prepare(struct vrend_sub_context *sub_ctx,
+                                struct vrend_surface *surf, unsigned buffers,
+                                float *colorf, double depth, unsigned stencil) {
+   if (surf && util_format_is_srgb(surf->format) &&
+       !vrend_resource_supports_view(surf->texture, surf->format)) {
+      VREND_DEBUG(dbg_tex, sub_ctx->parent,
+                  "manually converting glClearColor from linear->srgb colorspace for EGL-backed framebuffer color attachment"
+                  " (surface format is %s; resource format is %s)\n",
+                  util_format_name(surf->format),
+                  util_format_name(surf->texture->base.format));
+      for (int i = 0; i < 3; ++i) // i < 3: don't convert alpha channel
+         colorf[i] = vrend_color_encode_as_srgb(colorf[i]);
    }
 
    if (buffers & PIPE_CLEAR_COLOR) {
-      if (sub_ctx->nr_cbufs && sub_ctx->surf[0] && vrend_format_is_emulated_alpha(sub_ctx->surf[0]->format)) {
+      if (surf && vrend_format_is_emulated_alpha(surf->format)) {
          glClearColor(colorf[3], 0.0, 0.0, 0.0);
-      } else if (sub_ctx->nr_cbufs && sub_ctx->surf[0] &&
-                 vrend_resource_needs_redblue_swizzle(sub_ctx->surf[0]->texture, sub_ctx->surf[0]->format)) {
-         VREND_DEBUG(dbg_bgra, ctx, "swizzling glClearColor() since rendering surface is an externally-stored BGR* resource\n");
+      } else if (surf && 
+                 vrend_resource_needs_redblue_swizzle(surf->texture, surf->format)) {
+         VREND_DEBUG(dbg_bgra, sub_ctx->parent, "swizzling glClearColor() since rendering surface is an externally-stored BGR* resource\n");
          glClearColor(colorf[2], colorf[1], colorf[0], colorf[3]);
       } else {
          glClearColor(colorf[0], colorf[1], colorf[2], colorf[3]);
@@ -4625,7 +4805,7 @@ void vrend_clear(struct vrend_context *ctx,
       if (vrend_state.use_gles) {
          if (0.0f < depth && depth > 1.0f) {
             // Only warn, it is clamped by the function.
-            report_gles_warn(ctx, GLES_WARN_DEPTH_CLEAR);
+            report_gles_warn(sub_ctx->parent, GLES_WARN_DEPTH_CLEAR);
          }
          glClearDepthf(depth);
       } else {
@@ -4639,41 +4819,11 @@ void vrend_clear(struct vrend_context *ctx,
    }
 
    if (sub_ctx->hw_rs_state.rasterizer_discard)
-       glDisable(GL_RASTERIZER_DISCARD);
-
-   if (buffers & PIPE_CLEAR_COLOR) {
-      uint32_t mask = 0;
-      int i;
-      for (i = 0; i < sub_ctx->nr_cbufs; i++) {
-         if (sub_ctx->surf[i])
-            mask |= (1 << i);
-      }
-      if (mask != (buffers >> 2)) {
-         mask = buffers >> 2;
-         while (mask) {
-            i = u_bit_scan(&mask);
-            if (i < PIPE_MAX_COLOR_BUFS && sub_ctx->surf[i] && util_format_is_pure_uint(sub_ctx->surf[i] && sub_ctx->surf[i]->format))
-               glClearBufferuiv(GL_COLOR,
-                                i, (GLuint *)colorf);
-            else if (i < PIPE_MAX_COLOR_BUFS && sub_ctx->surf[i] && util_format_is_pure_sint(sub_ctx->surf[i] && sub_ctx->surf[i]->format))
-               glClearBufferiv(GL_COLOR,
-                                i, (GLint *)colorf);
-            else
-               glClearBufferfv(GL_COLOR,
-                                i, (GLfloat *)colorf);
-         }
-      }
-      else
-         bits |= GL_COLOR_BUFFER_BIT;
-   }
-   if (buffers & PIPE_CLEAR_DEPTH)
-      bits |= GL_DEPTH_BUFFER_BIT;
-   if (buffers & PIPE_CLEAR_STENCIL)
-      bits |= GL_STENCIL_BUFFER_BIT;
-
-   if (bits)
-      glClear(bits);
+      glDisable(GL_RASTERIZER_DISCARD);
+}
 
+static void vrend_clear_finish(struct vrend_sub_context *sub_ctx,
+                               unsigned buffers) {
    /* Is it really necessary to restore the old states? The only reason we
     * get here is because the guest cleared all those states but gallium
     * didn't forward them before calling the clear command
@@ -4711,30 +4861,94 @@ void vrend_clear(struct vrend_context *ctx,
                      sub_ctx->hw_blend_state.rt[0].colormask & PIPE_MASK_A ? GL_TRUE : GL_FALSE);
       }
    }
+
+   /* Restore previous scissor state */
    if (sub_ctx->hw_rs_state.scissor)
       glEnable(GL_SCISSOR_TEST);
    else
       glDisable(GL_SCISSOR_TEST);
 }
 
+void vrend_clear(struct vrend_context *ctx, unsigned buffers,
+                 const union pipe_color_union *color, double depth,
+                 unsigned stencil) {
+   GLbitfield bits = 0;
+   struct vrend_sub_context *sub_ctx = ctx->sub;
+
+   if (ctx->in_error)
+      return;
+
+   if (ctx->ctx_switch_pending)
+      vrend_finish_context_switch(ctx);
+
+   vrend_update_frontface_state(sub_ctx);
+   if (sub_ctx->stencil_state_dirty)
+      vrend_update_stencil_state(sub_ctx);
+   if (sub_ctx->scissor_state_dirty)
+      vrend_update_scissor_state(sub_ctx);
+   if (sub_ctx->viewport_state_dirty)
+      vrend_update_viewport_state(sub_ctx);
+
+   vrend_use_program(NULL);
+
+   glDisable(GL_SCISSOR_TEST);
+
+   float colorf[4];
+   memcpy(colorf, color->f, sizeof(colorf));
+
+   vrend_clear_prepare(sub_ctx, sub_ctx->nr_cbufs ? sub_ctx->surf[0] : NULL,
+                       buffers, colorf, depth, stencil);
+
+   if (buffers & PIPE_CLEAR_COLOR) {
+      uint32_t mask = 0;
+      for (uint32_t i = 0; i < sub_ctx->nr_cbufs; i++) {
+         if (sub_ctx->surf[i])
+            mask |= (1 << i);
+      }
+      if (mask != (buffers >> 2)) {
+         mask = buffers >> 2;
+         while (mask) {
+            int i = u_bit_scan(&mask);
+            if (i < PIPE_MAX_COLOR_BUFS && sub_ctx->surf[i] &&
+                util_format_is_pure_uint(sub_ctx->surf[i] &&
+                                         sub_ctx->surf[i]->format))
+                glClearBufferuiv(GL_COLOR, i, (GLuint *)colorf);
+            else if (i < PIPE_MAX_COLOR_BUFS && sub_ctx->surf[i] &&
+                     util_format_is_pure_sint(sub_ctx->surf[i] &&
+                                              sub_ctx->surf[i]->format))
+                glClearBufferiv(GL_COLOR, i, (GLint *)colorf);
+            else
+                glClearBufferfv(GL_COLOR, i, (GLfloat *)colorf);
+         }
+      } else
+         bits |= GL_COLOR_BUFFER_BIT;
+   }
+   if (buffers & PIPE_CLEAR_DEPTH)
+      bits |= GL_DEPTH_BUFFER_BIT;
+   if (buffers & PIPE_CLEAR_STENCIL)
+      bits |= GL_STENCIL_BUFFER_BIT;
+
+   if (bits)
+      glClear(bits);
+
+   vrend_clear_finish(sub_ctx, buffers);
+}
+
 int vrend_clear_texture(struct vrend_context* ctx,
-                         uint32_t handle, uint32_t level,
+                         struct vrend_resource *res, uint32_t level,
                          const struct pipe_box *box,
                          const void * data)
 {
    GLenum format, type;
-   struct vrend_resource *res;
-
-   res = vrend_renderer_ctx_res_lookup(ctx, handle);
-   if (!res) {
-      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, handle);
-      return EINVAL;
-   }
 
    enum virgl_formats fmt = res->base.format;
    format = tex_conv_table[fmt].glformat;
    type = tex_conv_table[fmt].gltype;
 
+   if (!has_feature(feat_clear_texture)) {
+      return EINVAL;
+   }
+
    /* 32-bit BGRA resources are always reordered to RGBA ordering before
     * submission to the host driver. Reorder red/blue color bytes in
     * the clear color to match. */
@@ -4747,12 +4961,12 @@ int vrend_clear_texture(struct vrend_context* ctx,
    }
 
    if (vrend_state.use_gles) {
-      glClearTexSubImageEXT(res->id, level,
+      glClearTexSubImageEXT(res->gl_id, level,
                             box->x, box->y, box->z,
                             box->width, box->height, box->depth,
                             format, type, data);
    } else {
-      glClearTexSubImage(res->id, level,
+      glClearTexSubImage(res->gl_id, level,
                          box->x, box->y, box->z,
                          box->width, box->height, box->depth,
                          format, type, data);
@@ -4760,6 +4974,75 @@ int vrend_clear_texture(struct vrend_context* ctx,
    return 0;
 }
 
+void vrend_clear_surface(struct vrend_context *ctx, uint32_t surf_handle,
+                         unsigned buffers, const union pipe_color_union *color,
+                         unsigned dstx, unsigned dsty, unsigned width,
+                         unsigned height, bool render_condition_enabled) {
+   struct vrend_surface *surf;
+   GLbitfield bits = 0;
+   struct vrend_sub_context *sub_ctx = ctx->sub;
+
+   surf = vrend_object_lookup(sub_ctx->object_hash, surf_handle,
+                              VIRGL_OBJECT_SURFACE);
+   if (!surf) {
+      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_SURFACE,
+                                 surf_handle);
+      return;
+   }
+
+   if (!vrend_format_can_render(surf->format) &&
+       !vrend_format_is_ds(surf->format)) {
+      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_FORMAT,
+                                 surf->format);
+      return;
+   }
+
+   if (render_condition_enabled == false)
+      vrend_pause_render_condition(ctx, true);
+
+   glScissor(dstx, dsty, width, height);
+   glEnable(GL_SCISSOR_TEST);
+   ctx->sub->scissor_state_dirty = (1 << 0);
+
+   // Do clear on blit framebuffer to avoid messing with main fb
+   glBindFramebuffer(GL_FRAMEBUFFER, ctx->sub->blit_fb_ids[0]);
+   vrend_fb_bind_texture_id(
+       surf->texture, surf->gl_id, 0, surf->level,
+       surf->first_layer != surf->last_layer ? -1 : (GLint)surf->first_layer,
+       surf->nr_samples);
+
+   // When doing clear_render_target color->f contains clear color
+   float colorf[4];
+   memcpy(colorf, color->f, sizeof(colorf));
+
+   // When doing clear_depth_stencil color encodes depth and stencil 
+   double depth;
+   memcpy(&depth, color->ui, sizeof(double));
+   unsigned int stencil = color->ui[3];
+
+   vrend_clear_prepare(sub_ctx, surf, buffers, colorf, depth, stencil);
+
+   if (buffers & PIPE_CLEAR_COLOR0)
+      bits |= GL_COLOR_BUFFER_BIT;
+   if (buffers & PIPE_CLEAR_DEPTH)
+      bits |= GL_DEPTH_BUFFER_BIT;
+   if (buffers & PIPE_CLEAR_STENCIL)
+      bits |= GL_STENCIL_BUFFER_BIT;
+
+   glClear(bits);
+
+   vrend_clear_finish(sub_ctx, buffers);
+
+   glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D,
+                          0, 0);
+   glFramebufferTexture2D(GL_FRAMEBUFFER, GL_DEPTH_STENCIL_ATTACHMENT,
+                          GL_TEXTURE_2D, 0, 0);
+   glBindFramebuffer(GL_FRAMEBUFFER, ctx->sub->fb_id);
+
+   if (render_condition_enabled == false)
+      vrend_pause_render_condition(ctx, false);
+}
+
 static void vrend_update_scissor_state(struct vrend_sub_context *sub_ctx)
 {
    struct pipe_scissor_state *ss;
@@ -4826,7 +5109,7 @@ static GLenum get_gs_xfb_mode(GLenum mode)
    case GL_TRIANGLE_STRIP:
       return GL_TRIANGLES;
    default:
-      vrend_printf( "illegal gs transform feedback mode %d\n", mode);
+      virgl_warn("Illegal gs transform feedback mode %d\n", mode);
       return GL_POINTS;
    }
 }
@@ -4842,7 +5125,7 @@ static GLenum get_tess_xfb_mode(int mode, bool is_point_mode)
    case GL_LINES:
       return GL_LINES;
    default:
-      vrend_printf( "illegal gs transform feedback mode %d\n", mode);
+      virgl_warn("Illegal gs transform feedback mode %d\n", mode);
       return GL_POINTS;
    }
 }
@@ -4864,7 +5147,7 @@ static GLenum get_xfb_mode(GLenum mode)
    case GL_LINE_STRIP:
       return GL_LINES;
    default:
-      vrend_printf( "failed to translate TFB %d\n", mode);
+      virgl_warn("Failed to translate TFB %d\n", mode);
       return GL_POINTS;
    }
 }
@@ -4891,7 +5174,7 @@ static void vrend_draw_bind_vertex_legacy(struct vrend_context *ctx,
       res = (struct vrend_resource *)ctx->sub->vbo[vbo_index].base.buffer;
 
       if (!res) {
-         vrend_printf("cannot find vbo buf %d %d %d\n", i, va->count, ctx->sub->prog->ss[PIPE_SHADER_VERTEX]->sel->sinfo.num_inputs);
+         virgl_warn("Cannot find vbo buf %d %d %d\n", i, va->count, ctx->sub->prog->ss[PIPE_SHADER_VERTEX]->sel->sinfo.num_inputs);
          continue;
       }
 
@@ -4903,9 +5186,9 @@ static void vrend_draw_bind_vertex_legacy(struct vrend_context *ctx,
          } else loc = -1;
 
          if (loc == -1) {
-            vrend_printf("%s: cannot find loc %d %d %d\n", ctx->debug_name, i, va->count, ctx->sub->prog->ss[PIPE_SHADER_VERTEX]->sel->sinfo.num_inputs);
+            virgl_warn("%s: Cannot find loc %d %d %d\n", ctx->debug_name, i, va->count, ctx->sub->prog->ss[PIPE_SHADER_VERTEX]->sel->sinfo.num_inputs);
             if (i == 0) {
-               vrend_printf("%s: shader probably didn't compile - skipping rendering\n", ctx->debug_name);
+               virgl_warn("%s: Shader probably didn't compile - skipping rendering\n", ctx->debug_name);
                return;
             }
             continue;
@@ -4913,11 +5196,11 @@ static void vrend_draw_bind_vertex_legacy(struct vrend_context *ctx,
       }
 
       if (ve->type == GL_FALSE) {
-         vrend_printf("failed to translate vertex type - skipping render\n");
+         virgl_warn("Failed to translate vertex type - skipping render\n");
          return;
       }
 
-      glBindBuffer(GL_ARRAY_BUFFER, res->id);
+      glBindBuffer(GL_ARRAY_BUFFER, res->gl_id);
 
       struct vrend_vertex_buffer *vbo = &ctx->sub->vbo[vbo_index];
 
@@ -4993,7 +5276,7 @@ static void vrend_draw_bind_vertex_binding(struct vrend_context *ctx,
          for (i = 0; i < ctx->sub->num_vbos; i++) {
             struct vrend_resource *res = (struct vrend_resource *)vbo[i].base.buffer;
             if (res) {
-               buffers[i] = res->id;
+               buffers[i] = res->gl_id;
                offsets[i] = vbo[i].base.buffer_offset;
                strides[i] = vbo[i].base.stride;
             } else {
@@ -5014,7 +5297,7 @@ static void vrend_draw_bind_vertex_binding(struct vrend_context *ctx,
          for (i = 0; i < ctx->sub->num_vbos; i++) {
             struct vrend_resource *res = (struct vrend_resource *)vbo[i].base.buffer;
             if (res)
-               glBindVertexBuffer(i, res->id, vbo[i].base.buffer_offset, vbo[i].base.stride);
+               glBindVertexBuffer(i, res->gl_id, vbo[i].base.buffer_offset, vbo[i].base.stride);
             else
                glBindVertexBuffer(i, 0, 0, 0);
          }
@@ -5026,22 +5309,26 @@ static void vrend_draw_bind_vertex_binding(struct vrend_context *ctx,
    }
 }
 
-static int vrend_draw_bind_samplers_shader(struct vrend_sub_context *sub_ctx,
-                                           int shader_type,
-                                           int next_sampler_id)
+static GLuint vrend_draw_bind_samplers_shader(struct vrend_sub_context *sub_ctx,
+                                              int shader_type,
+                                              GLuint next_sampler_id)
 {
+   struct vrend_linked_shader_program *sprog = sub_ctx->prog;
+   struct vrend_shader_view *shader_view = &sub_ctx->views[shader_type];
+
    int sampler_index = 0;
-   int n_samplers = 0;
-   uint32_t dirty = sub_ctx->sampler_views_dirty[shader_type];
-   uint32_t mask = sub_ctx->prog->samplers_used_mask[shader_type];
-   struct vrend_shader_view *sviews = &sub_ctx->views[shader_type];
+   uint32_t dirty = shader_view->dirty_mask;
+   uint32_t mask = sprog->samplers_used_mask[shader_type];
 
    while (mask) {
       int i = u_bit_scan(&mask);
+      struct vrend_sampler_view *tview = shader_view->views[i];
 
-      struct vrend_sampler_view *tview = sviews->views[i];
       if ((dirty & (1 << i)) && tview) {
-         if (sub_ctx->prog->shadow_samp_mask[shader_type] & (1 << i)) {
+         glActiveTexture(GL_TEXTURE0 + next_sampler_id);
+         glUniform1i(sprog->sampler_locs[shader_type][sampler_index], next_sampler_id);
+
+         if (sprog->shadow_samp_mask[shader_type] & (1 << i)) {
             struct vrend_texture *tex = (struct vrend_texture *)tview->texture;
 
             /* The modes LUMINANCE, INTENSITY, and ALPHA only apply when a depth texture
@@ -5060,12 +5347,12 @@ static int vrend_draw_bind_samplers_shader(struct vrend_sub_context *sub_ctx,
                memcpy(tex->cur_swizzle, swizzle, 4 * sizeof(GLint));
             }
 
-            glUniform4f(sub_ctx->prog->shadow_samp_mask_locs[shader_type][sampler_index],
+            glUniform4f(sprog->shadow_samp_mask_locs[shader_type][sampler_index],
                         (tview->gl_swizzle[0] == GL_ZERO || tview->gl_swizzle[0] == GL_ONE) ? 0.0 : 1.0,
                         (tview->gl_swizzle[1] == GL_ZERO || tview->gl_swizzle[1] == GL_ONE) ? 0.0 : 1.0,
                         (tview->gl_swizzle[2] == GL_ZERO || tview->gl_swizzle[2] == GL_ONE) ? 0.0 : 1.0,
                         (tview->gl_swizzle[3] == GL_ZERO || tview->gl_swizzle[3] == GL_ONE) ? 0.0 : 1.0);
-            glUniform4f(sub_ctx->prog->shadow_samp_add_locs[shader_type][sampler_index],
+            glUniform4f(sprog->shadow_samp_add_locs[shader_type][sampler_index],
                         tview->gl_swizzle[0] == GL_ONE ? 1.0 : 0.0,
                         tview->gl_swizzle[1] == GL_ONE ? 1.0 : 0.0,
                         tview->gl_swizzle[2] == GL_ONE ? 1.0 : 0.0,
@@ -5073,46 +5360,44 @@ static int vrend_draw_bind_samplers_shader(struct vrend_sub_context *sub_ctx,
          }
 
          if (tview->texture) {
-            GLuint id = tview->id;
-            struct vrend_resource *texture = tview->texture;
+            GLuint id = tview->gl_id;
             GLenum target = tview->target;
 
             debug_texture(__func__, tview->texture);
 
             if (has_bit(tview->texture->storage_bits, VREND_STORAGE_GL_BUFFER)) {
-               id = texture->tbo_tex_id;
+               id = tview->texture->tbo_tex_id;
                target = GL_TEXTURE_BUFFER;
             }
 
-            glActiveTexture(GL_TEXTURE0 + next_sampler_id);
             glBindTexture(target, id);
+            vrend_apply_sampler_state(sub_ctx, tview->texture,
+                                      shader_view->samplers[i],
+                                      next_sampler_id, tview);
 
             if (vrend_state.use_gles) {
                const unsigned levels = tview->levels ? tview->levels : tview->texture->base.last_level + 1u;
-               sub_ctx->texture_levels[shader_type][n_samplers++] = levels;
+               shader_view->texture_levels[sampler_index] = levels;
             }
-
-            if (sub_ctx->views[shader_type].old_ids[i] != id ||
-                sub_ctx->sampler_views_dirty[shader_type] & (1 << i)) {
-               vrend_apply_sampler_state(sub_ctx, texture, shader_type, i,
-                                         next_sampler_id, tview);
-               sviews->old_ids[i] = id;
-            }
-            dirty &= ~(1 << i);
          }
       }
       sampler_index++;
       next_sampler_id++;
    }
 
-   sub_ctx->n_samplers[shader_type] = n_samplers;
-   sub_ctx->sampler_views_dirty[shader_type] = dirty;
+   shader_view->num_used_views = sampler_index;
+   shader_view->dirty_mask = 0;
+
+   // Since we use a dirty mask to elide some unnecessary state update API
+   // calls, we must ensure that a later glBindTexture() used for another reason
+   // (such as texture allocation) doesn't affect our fragile sampler bindings.
+   glActiveTexture(GL_TEXTURE0 + vrend_state.max_texture_units - 1);
 
    return next_sampler_id;
 }
 
-static int vrend_draw_bind_ubo_shader(struct vrend_sub_context *sub_ctx,
-                                      int shader_type, int next_ubo_id)
+static GLuint vrend_draw_bind_ubo_shader(struct vrend_sub_context *sub_ctx,
+                                         int shader_type, GLuint next_ubo_id)
 {
    uint32_t mask, dirty, update;
    struct pipe_constant_buffer *cb;
@@ -5134,7 +5419,7 @@ static int vrend_draw_bind_ubo_shader(struct vrend_sub_context *sub_ctx,
          cb = &sub_ctx->cbs[shader_type][i];
          res = (struct vrend_resource *)cb->buffer;
 
-         glBindBufferRange(GL_UNIFORM_BUFFER, next_ubo_id, res->id,
+         glBindBufferRange(GL_UNIFORM_BUFFER, next_ubo_id, res->gl_id,
                            cb->buffer_offset, cb->buffer_size);
          dirty &= ~(1 << i);
       }
@@ -5184,7 +5469,7 @@ static void vrend_draw_bind_ssbo_shader(struct vrend_sub_context *sub_ctx,
 
       ssbo = &sub_ctx->ssbo[shader_type][i];
       res = (struct vrend_resource *)ssbo->res;
-      glBindBufferRange(GL_SHADER_STORAGE_BUFFER, i + offset, res->id,
+      glBindBufferRange(GL_SHADER_STORAGE_BUFFER, i + offset, res->gl_id,
                         ssbo->buffer_offset, ssbo->buffer_size);
    }
 }
@@ -5205,7 +5490,7 @@ static void vrend_draw_bind_abo_shader(struct vrend_sub_context *sub_ctx)
 
       abo = &sub_ctx->abo[i];
       res = (struct vrend_resource *)abo->res;
-      glBindBufferRange(GL_ATOMIC_COUNTER_BUFFER, i, res->id,
+      glBindBufferRange(GL_ATOMIC_COUNTER_BUFFER, i, res->gl_id,
                         abo->buffer_offset, abo->buffer_size);
    }
 }
@@ -5227,16 +5512,17 @@ static void vrend_draw_bind_images_shader(struct vrend_sub_context *sub_ctx, int
    if (!has_feature(feat_images))
       return;
 
-   mask = sub_ctx->images_used_mask[shader_type];
+   mask = sub_ctx->images_used_mask[shader_type] & sub_ctx->prog->images_used_mask[shader_type];
+
    while (mask) {
       unsigned i = u_bit_scan(&mask);
       int image_unit = i + sub_ctx->prog->ss[shader_type]->sel->sinfo.image_binding_offset;
       int binding = sub_ctx->prog->img_locs[shader_type][i];
 
-      if (binding == -1 || !(sub_ctx->prog->images_used_mask[shader_type] & (1 << i)))
+      if (binding == -1)
           continue;
       iview = &sub_ctx->image_views[shader_type][i];
-      tex_id = iview->texture->id;
+      tex_id = iview->texture->gl_id;
       if (has_bit(iview->texture->storage_bits, VREND_STORAGE_GL_BUFFER)) {
          if (!iview->texture->tbo_tex_id)
             glGenTextures(1, &iview->texture->tbo_tex_id);
@@ -5265,12 +5551,12 @@ static void vrend_draw_bind_images_shader(struct vrend_sub_context *sub_ctx, int
             break;
          default:
             /* This should not be possible, warn and set a default format. */
-            vrend_printf("%s: Unsupported format block bit size %d\n", __func__,
-                         util_format_get_blocksizebits(iview->vformat));
+            virgl_warn("%s: Unsupported format block bit size %d\n", __func__,
+                       util_format_get_blocksizebits(iview->vformat));
             format = GL_R8UI;
          }
 
-         glBindBufferARB(GL_TEXTURE_BUFFER, iview->texture->id);
+         glBindBufferARB(GL_TEXTURE_BUFFER, iview->texture->gl_id);
          glBindTexture(GL_TEXTURE_BUFFER, iview->texture->tbo_tex_id);
 
          if (has_feature(feat_arb_or_gles_ext_texture_buffer)) {
@@ -5284,10 +5570,10 @@ static void vrend_draw_bind_images_shader(struct vrend_sub_context *sub_ctx, int
                unsigned size = iview->u.buf.size / blsize;
                if (offset + size > vrend_state.max_texture_buffer_size)
                   size = vrend_state.max_texture_buffer_size - offset;
-               glTexBufferRange(GL_TEXTURE_BUFFER, format, iview->texture->id, iview->u.buf.offset,
+               glTexBufferRange(GL_TEXTURE_BUFFER, format, iview->texture->gl_id, iview->u.buf.offset,
                                 size * blsize);
             } else {
-               glTexBuffer(GL_TEXTURE_BUFFER, format, iview->texture->id);
+               glTexBuffer(GL_TEXTURE_BUFFER, format, iview->texture->gl_id);
             }
          }
 
@@ -5312,7 +5598,7 @@ static void vrend_draw_bind_images_shader(struct vrend_sub_context *sub_ctx, int
                glDeleteTextures(1, &iview->view_id);
 
             glGenTextures(1, &iview->view_id);
-            glTextureView(iview->view_id, iview->texture->target, iview->texture->id,
+            glTextureView(iview->view_id, iview->texture->target, iview->texture->gl_id,
                           tex_conv_table[iview->texture->base.format].internalformat, level, 1,
                           first_layer, num_layers);
             tex_id = iview->view_id;
@@ -5333,7 +5619,7 @@ static void vrend_draw_bind_images_shader(struct vrend_sub_context *sub_ctx, int
          access = GL_READ_WRITE;
          break;
       default:
-         vrend_printf( "Invalid access specified\n");
+         virgl_warn("Invalid access specified\n");
          return;
       }
 
@@ -5345,7 +5631,7 @@ static void vrend_draw_bind_images_shader(struct vrend_sub_context *sub_ctx, int
 static void
 vrend_fill_sysval_uniform_block (struct vrend_sub_context *sub_ctx)
 {
-   if (sub_ctx->prog->virgl_block_bind == -1)
+   if (sub_ctx->prog->virgl_block_bind == GL_INVALID_INDEX)
       return;
 
    if (sub_ctx->sysvalue_data_cookie != sub_ctx->prog->sysvalue_data_cookie) {
@@ -5359,7 +5645,7 @@ vrend_fill_sysval_uniform_block (struct vrend_sub_context *sub_ctx)
 
 static void vrend_draw_bind_objects(struct vrend_sub_context *sub_ctx, bool new_program)
 {
-   int next_ubo_id = 0, next_sampler_id = 0;
+   GLuint next_ubo_id = 0, next_sampler_id = 0;
    for (int shader_type = PIPE_SHADER_VERTEX; shader_type <= sub_ctx->last_shader_idx; shader_type++) {
       vrend_set_active_pipeline_stage(sub_ctx->prog, shader_type);
 
@@ -5374,13 +5660,13 @@ static void vrend_draw_bind_objects(struct vrend_sub_context *sub_ctx, bool new_
          if (sub_ctx->prog->tex_levels_uniform_id[shader_type] != -1) {
             vrend_set_active_pipeline_stage(sub_ctx->prog, shader_type);
             glUniform1iv(sub_ctx->prog->tex_levels_uniform_id[shader_type],
-                         sub_ctx->n_samplers[shader_type],
-                         sub_ctx->texture_levels[shader_type]);
+                         sub_ctx->views[shader_type].num_used_views,
+                         sub_ctx->views[shader_type].texture_levels);
          }
       }
    }
 
-   if (sub_ctx->prog->virgl_block_bind != -1)
+   if (sub_ctx->prog->virgl_block_bind != GL_INVALID_INDEX)
       glBindBufferRange(GL_UNIFORM_BUFFER, sub_ctx->prog->virgl_block_bind,
                         sub_ctx->prog->ubo_sysval_buffer_id,
                         0, sizeof(struct sysval_uniform_block));
@@ -5390,16 +5676,23 @@ static void vrend_draw_bind_objects(struct vrend_sub_context *sub_ctx, bool new_
    vrend_set_active_pipeline_stage(sub_ctx->prog, PIPE_SHADER_FRAGMENT);
 }
 
-static
-void vrend_inject_tcs(struct vrend_sub_context *sub_ctx, int vertices_per_patch)
+static bool
+vrend_inject_tcs(struct vrend_sub_context *sub_ctx, uint8_t vertices_per_patch)
 {
    struct pipe_stream_output_info so_info;
 
    memset(&so_info, 0, sizeof(so_info));
    struct vrend_shader_selector *sel = vrend_create_shader_state(&so_info,
                                                                  false, PIPE_SHADER_TESS_CTRL);
-   struct vrend_shader *shader;
-   shader = CALLOC_STRUCT(vrend_shader);
+   if (!sel)
+      return false;
+
+   struct vrend_shader *shader = CALLOC_STRUCT(vrend_shader);
+   if (!shader) {
+      vrend_destroy_shader_selector(sel);
+      return false;
+   }
+
    vrend_fill_shader_key(sub_ctx, sel, &shader->key);
 
    shader->sel = sel;
@@ -5414,7 +5707,7 @@ void vrend_inject_tcs(struct vrend_sub_context *sub_ctx, int vertices_per_patch)
       FREE(shader);
       vrend_report_context_error(sub_ctx->parent, VIRGL_ERROR_CTX_ILLEGAL_SHADER, sel->type);
       vrend_destroy_shader_selector(sel);
-      return;
+      return false;
    }
    // Need to add inject the selected shader to the shader selector and then the code below
    // can continue
@@ -5423,23 +5716,27 @@ void vrend_inject_tcs(struct vrend_sub_context *sub_ctx, int vertices_per_patch)
    sub_ctx->shaders[PIPE_SHADER_TESS_CTRL] = sel;
 
    vrend_compile_shader(sub_ctx, shader);
+   return true;
 }
 
+enum select_program_result {
+    PROGRAMM_ERROR,
+    PROGRAMM_NO_CHANGE,
+    PROGRAMM_NEW
+};
 
-static bool
-vrend_select_program(struct vrend_sub_context *sub_ctx, ubyte vertices_per_patch)
+static enum select_program_result
+vrend_select_program(struct vrend_sub_context *sub_ctx, uint8_t vertices_per_patch)
 {
    struct vrend_linked_shader_program *prog;
    bool fs_dirty, vs_dirty, gs_dirty, tcs_dirty, tes_dirty;
    bool dual_src = util_blend_state_is_dual(&sub_ctx->blend_state, 0);
-   bool new_program = false;
 
-   struct vrend_shader_selector **shaders = sub_ctx->shaders;
 
-   sub_ctx->shader_dirty = false;
+   struct vrend_shader_selector **shaders = sub_ctx->shaders;
 
    if (!shaders[PIPE_SHADER_VERTEX] || !shaders[PIPE_SHADER_FRAGMENT]) {
-      return false;
+      return PROGRAMM_ERROR;
    }
 
    // For some GPU, we'd like to use integer variable in generated GLSL if
@@ -5456,7 +5753,8 @@ vrend_select_program(struct vrend_sub_context *sub_ctx, ubyte vertices_per_patch
       vrend_shader_select(sub_ctx, shaders[PIPE_SHADER_TESS_CTRL], &tcs_dirty);
    else if (vrend_state.use_gles && shaders[PIPE_SHADER_TESS_EVAL]) {
       VREND_DEBUG(dbg_shader, sub_ctx->parent, "Need to inject a TCS\n");
-      vrend_inject_tcs(sub_ctx, vertices_per_patch);
+      if (!vrend_inject_tcs(sub_ctx, vertices_per_patch))
+         goto fail;
 
       vrend_shader_select(sub_ctx, shaders[PIPE_SHADER_VERTEX], &vs_dirty);
    }
@@ -5478,7 +5776,8 @@ vrend_select_program(struct vrend_sub_context *sub_ctx, ubyte vertices_per_patch
       vrend_shader_select(sub_ctx, shaders[PIPE_SHADER_TESS_CTRL], &tcs_dirty);
    else if (vrend_state.use_gles && shaders[PIPE_SHADER_TESS_EVAL]) {
       VREND_DEBUG(dbg_shader, sub_ctx->parent, "Need to inject a TCS\n");
-      vrend_inject_tcs(sub_ctx, vertices_per_patch);
+      if (!vrend_inject_tcs(sub_ctx, vertices_per_patch))
+         goto fail;
    }
    sub_ctx->drawing = true;
    vrend_shader_select(sub_ctx, shaders[PIPE_SHADER_VERTEX], &vs_dirty);
@@ -5494,7 +5793,7 @@ vrend_select_program(struct vrend_sub_context *sub_ctx, ubyte vertices_per_patch
       struct vrend_shader *shader = sel->current;
       if (shader && !shader->is_compiled) {
          if (!vrend_compile_shader(sub_ctx, shader))
-            return false;
+            return PROGRAMM_ERROR;
       }
       if (vrend_state.use_gles && sel->sinfo.gles_use_tex_query_level)
          gles_emulate_query_texture_levels_mask |= 1 << i;
@@ -5547,7 +5846,7 @@ vrend_select_program(struct vrend_sub_context *sub_ctx, ubyte vertices_per_patch
                                    tes_id ? sub_ctx->shaders[PIPE_SHADER_TESS_EVAL]->current : NULL,
                                    separable);
          if (!prog)
-            return false;
+            return PROGRAMM_ERROR;
          prog->gles_use_query_texturelevel_mask = gles_emulate_query_texture_levels_mask;
       } else if (separable) {
           /* UBO block bindings are reset to zero if the programs are
@@ -5569,7 +5868,7 @@ vrend_select_program(struct vrend_sub_context *sub_ctx, ubyte vertices_per_patch
           }
 
           if (need_rebind) {
-             vrend_use_program(sub_ctx, prog);
+             vrend_use_program(prog);
              rebind_ubo_and_sampler_locs(prog, last_shader);
           }
       }
@@ -5577,8 +5876,10 @@ vrend_select_program(struct vrend_sub_context *sub_ctx, ubyte vertices_per_patch
       sub_ctx->last_shader_idx = sub_ctx->shaders[PIPE_SHADER_TESS_EVAL] ? PIPE_SHADER_TESS_EVAL : (sub_ctx->shaders[PIPE_SHADER_GEOMETRY] ? PIPE_SHADER_GEOMETRY : PIPE_SHADER_FRAGMENT);
    } else
       prog = sub_ctx->prog;
+
+   enum select_program_result new_program = PROGRAMM_NO_CHANGE;
    if (sub_ctx->prog != prog) {
-      new_program = true;
+      new_program = PROGRAMM_NEW;
       sub_ctx->prog_ids[PIPE_SHADER_VERTEX] = vs_id;
       sub_ctx->prog_ids[PIPE_SHADER_FRAGMENT] = fs_id;
       sub_ctx->prog_ids[PIPE_SHADER_GEOMETRY] = gs_id;
@@ -5590,17 +5891,19 @@ vrend_select_program(struct vrend_sub_context *sub_ctx, ubyte vertices_per_patch
       /* mark all constbufs and sampler views as dirty */
       for (int stage = PIPE_SHADER_VERTEX; stage <= PIPE_SHADER_FRAGMENT; stage++) {
          sub_ctx->const_bufs_dirty[stage] = ~0;
-         sub_ctx->sampler_views_dirty[stage] = ~0;
+         sub_ctx->views[stage].dirty_mask = ~0;
       }
 
       prog->ref_context = sub_ctx;
    }
    sub_ctx->cs_shader_dirty = true;
+   sub_ctx->shader_dirty = false;
+
    return new_program;
 
 fail:
-   vrend_printf( "failure to compile shader variants: %s\n", sub_ctx->parent->debug_name);
-   return false;
+   virgl_error("Failure to compile shader variants: %s\n", sub_ctx->parent->debug_name);
+   return PROGRAMM_ERROR;
 }
 
 void vrend_link_program_hook(struct vrend_context *ctx, uint32_t *handles)
@@ -5666,13 +5969,13 @@ int vrend_draw_vbo(struct vrend_context *ctx,
                    uint32_t cso, uint32_t indirect_handle,
                    uint32_t indirect_draw_count_handle)
 {
-   bool new_program = false;
+   enum select_program_result program_select_result = PROGRAMM_NO_CHANGE;
    struct vrend_resource *indirect_res = NULL;
    struct vrend_resource *indirect_params_res = NULL;
    struct vrend_sub_context *sub_ctx = ctx->sub;
 
    if (ctx->in_error)
-      return 0;
+      return ENOTRECOVERABLE;
 
    if (info->instance_count && !has_feature(feat_draw_instance))
       return EINVAL;
@@ -5687,9 +5990,9 @@ int vrend_draw_vbo(struct vrend_context *ctx,
       if (!has_feature(feat_indirect_draw))
          return EINVAL;
       indirect_res = vrend_renderer_ctx_res_lookup(ctx, indirect_handle);
-      if (!indirect_res) {
+      if (!indirect_res || !indirect_res->gl_id) {
          vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, indirect_handle);
-         return 0;
+         return EINVAL;
       }
    }
 
@@ -5699,9 +6002,9 @@ int vrend_draw_vbo(struct vrend_context *ctx,
          return EINVAL;
 
       indirect_params_res = vrend_renderer_ctx_res_lookup(ctx, indirect_draw_count_handle);
-      if (!indirect_params_res){
+      if (!indirect_params_res || !indirect_params_res->gl_id){
          vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, indirect_draw_count_handle);
-         return 0;
+         return EINVAL;
       }
    }
 
@@ -5730,21 +6033,16 @@ int vrend_draw_vbo(struct vrend_context *ctx,
       sub_ctx->prim_mode = (int)info->mode;
    }
 
-   if (!sub_ctx->ve) {
-      vrend_printf("illegal VE setup - skipping renderering\n");
-      return 0;
-   }
-
    if (sub_ctx->shader_dirty || sub_ctx->swizzle_output_rgb_to_bgr ||
        sub_ctx->needs_manual_srgb_encode_bitmask || sub_ctx->vbo_dirty)
-      new_program = vrend_select_program(sub_ctx, info->vertices_per_patch);
+      program_select_result = vrend_select_program(sub_ctx, info->vertices_per_patch);
 
-   if (!sub_ctx->prog) {
-      vrend_printf("dropping rendering due to missing shaders: %s\n", ctx->debug_name);
+   if (!sub_ctx->prog || program_select_result == PROGRAMM_ERROR) {
+      virgl_error("Dropping rendering due to missing shaders: %s\n", ctx->debug_name);
       return 0;
    }
 
-   vrend_use_program(sub_ctx, sub_ctx->prog);
+   vrend_use_program(sub_ctx->prog);
 
    if (has_feature(feat_draw_parameters) &&
        sub_ctx->prog->reads_drawid &&
@@ -5769,21 +6067,42 @@ int vrend_draw_vbo(struct vrend_context *ctx,
       }
    }
 
-   vrend_draw_bind_objects(sub_ctx, new_program);
+   vrend_draw_bind_objects(sub_ctx, program_select_result == PROGRAMM_NEW);
    vrend_fill_sysval_uniform_block(sub_ctx);
 
-   if (has_feature(feat_gles31_vertex_attrib_binding))
-      vrend_draw_bind_vertex_binding(ctx, sub_ctx->ve);
-   else
-      vrend_draw_bind_vertex_legacy(ctx, sub_ctx->ve);
+   if (has_feature(feat_gles31_vertex_attrib_binding)) {
+      if (sub_ctx->ve) {
+         vrend_draw_bind_vertex_binding(ctx, sub_ctx->ve);
+      } else {
+         glBindVertexArray(sub_ctx->vaoid);
+      }
+   } else {
+      if (sub_ctx->ve) {
+         vrend_draw_bind_vertex_legacy(ctx, sub_ctx->ve);
+      } else {
+         struct vrend_vertex_element_array va;
+         va.count = 0;
+         vrend_draw_bind_vertex_legacy(ctx, &va);
+      }
+   }
 
    if (info->indexed) {
       struct vrend_resource *res = (struct vrend_resource *)sub_ctx->ib.buffer;
       if (!res) {
-         vrend_printf( "VBO missing indexed array buffer\n");
+         virgl_error("VBO missing indexed array buffer\n");
          return 0;
       }
-      glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, res->id);
+
+      if (!indirect_handle) {
+         uint32_t expected_size = sub_ctx->ib.index_size * info->count + sub_ctx->ib.offset;
+         if (expected_size > res->base.width0) {
+            virgl_error("Indexed array buffer (%u) not large enough for draw operation "
+                        "(req. %u\n", res->base.width0, expected_size);
+            return 0;
+         }
+      }
+
+      glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, res->gl_id);
    } else
       glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, 0);
 
@@ -5806,28 +6125,22 @@ int vrend_draw_vbo(struct vrend_context *ctx,
    if (info->primitive_restart) {
       if (vrend_state.use_gles) {
          glEnable(GL_PRIMITIVE_RESTART_FIXED_INDEX);
-      } else if (has_feature(feat_nv_prim_restart)) {
-         glEnableClientState(GL_PRIMITIVE_RESTART_NV);
-         glPrimitiveRestartIndexNV(info->restart_index);
       } else if (has_feature(feat_gl_prim_restart)) {
          glEnable(GL_PRIMITIVE_RESTART);
          glPrimitiveRestartIndex(info->restart_index);
+      } else if (has_feature(feat_nv_prim_restart)) {
+         glEnableClientState(GL_PRIMITIVE_RESTART_NV);
+         glPrimitiveRestartIndexNV(info->restart_index);
       }
    }
 
    if (has_feature(feat_indirect_draw)) {
-      GLint buf = indirect_res ? indirect_res->id : 0;
-      if (sub_ctx->draw_indirect_buffer != buf) {
-         glBindBuffer(GL_DRAW_INDIRECT_BUFFER, buf);
-         sub_ctx->draw_indirect_buffer = buf;
-      }
+      GLint buf = indirect_res ? indirect_res->gl_id : 0;
+      glBindBuffer(GL_DRAW_INDIRECT_BUFFER, buf);
 
       if (has_feature(feat_indirect_params)) {
-         GLint buf = indirect_params_res ? indirect_params_res->id : 0;
-         if (sub_ctx->draw_indirect_params_buffer != buf) {
-            glBindBuffer(GL_PARAMETER_BUFFER_ARB, buf);
-            sub_ctx->draw_indirect_params_buffer = buf;
-         }
+         GLint buf = indirect_params_res ? indirect_params_res->gl_id : 0;
+         glBindBuffer(GL_PARAMETER_BUFFER_ARB, buf);
       }
    }
 
@@ -5847,7 +6160,7 @@ int vrend_draw_vbo(struct vrend_context *ctx,
                                  blend_mask_shader != 0 &&
                                  blend_mode != 0 &&
                                  alpha_dst_factor == 0;
-   if(use_advanced_blending) {
+   if (use_advanced_blending) {
       GLenum blend = translate_blend_func_advanced(blend_mode);
       glBlendEquation(blend);
       glEnable(GL_BLEND);
@@ -5925,10 +6238,10 @@ int vrend_draw_vbo(struct vrend_context *ctx,
    if (info->primitive_restart) {
       if (vrend_state.use_gles) {
          glDisable(GL_PRIMITIVE_RESTART_FIXED_INDEX);
-      } else if (has_feature(feat_nv_prim_restart)) {
-         glDisableClientState(GL_PRIMITIVE_RESTART_NV);
       } else if (has_feature(feat_gl_prim_restart)) {
          glDisable(GL_PRIMITIVE_RESTART);
+      } else if (has_feature(feat_nv_prim_restart)) {
+         glDisableClientState(GL_PRIMITIVE_RESTART_NV);
       }
    }
 
@@ -5962,21 +6275,19 @@ void vrend_launch_grid(struct vrend_context *ctx,
       struct vrend_linked_shader_program *prog;
       bool cs_dirty;
 
-      sub_ctx->cs_shader_dirty = false;
-
       if (!sub_ctx->shaders[PIPE_SHADER_COMPUTE]) {
-         vrend_printf("dropping rendering due to missing shaders: %s\n", ctx->debug_name);
+         virgl_error("Dropping rendering due to missing shaders: %s\n", ctx->debug_name);
          return;
       }
 
       vrend_shader_select(sub_ctx, sub_ctx->shaders[PIPE_SHADER_COMPUTE], &cs_dirty);
       if (!sub_ctx->shaders[PIPE_SHADER_COMPUTE]->current) {
-         vrend_printf( "failure to select compute shader variant: %s\n", ctx->debug_name);
+         virgl_error("Failure to select compute shader variant: %s\n", ctx->debug_name);
          return;
       }
       if (!sub_ctx->shaders[PIPE_SHADER_COMPUTE]->current->is_compiled) {
-         if(!vrend_compile_shader(sub_ctx, sub_ctx->shaders[PIPE_SHADER_COMPUTE]->current)) {
-            vrend_printf( "failure to compile compute shader variant: %s\n", ctx->debug_name);
+         if (!vrend_compile_shader(sub_ctx, sub_ctx->shaders[PIPE_SHADER_COMPUTE]->current)) {
+            virgl_error("Failure to compile compute shader variant: %s\n", ctx->debug_name);
             return;
          }
       }
@@ -5998,15 +6309,16 @@ void vrend_launch_grid(struct vrend_context *ctx,
          prog->ref_context = sub_ctx;
       }
       sub_ctx->shader_dirty = true;
+      sub_ctx->cs_shader_dirty = false;
    }
 
    if (!sub_ctx->prog) {
-      vrend_printf("%s: Skipping compute shader execution due to missing shaders: %s\n",
+      virgl_error("%s: Skipping compute shader execution due to missing shaders: %s\n",
                    __func__, ctx->debug_name);
       return;
    }
 
-   vrend_use_program(sub_ctx, sub_ctx->prog);
+   vrend_use_program(sub_ctx->prog);
 
    vrend_set_active_pipeline_stage(sub_ctx->prog, PIPE_SHADER_COMPUTE);
    vrend_draw_bind_ubo_shader(sub_ctx, PIPE_SHADER_COMPUTE, 0);
@@ -6018,14 +6330,14 @@ void vrend_launch_grid(struct vrend_context *ctx,
 
    if (indirect_handle) {
       indirect_res = vrend_renderer_ctx_res_lookup(ctx, indirect_handle);
-      if (!indirect_res) {
+      if (!indirect_res || !indirect_res->gl_id) {
          vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, indirect_handle);
          return;
       }
    }
 
    if (indirect_res)
-      glBindBuffer(GL_DISPATCH_INDIRECT_BUFFER, indirect_res->id);
+      glBindBuffer(GL_DISPATCH_INDIRECT_BUFFER, indirect_res->gl_id);
    else
       glBindBuffer(GL_DISPATCH_INDIRECT_BUFFER, 0);
 
@@ -6169,8 +6481,19 @@ static inline bool is_const_blend(int blend_factor)
 
 static void vrend_hw_emit_blend(struct vrend_sub_context *sub_ctx, struct pipe_blend_state *state)
 {
+   bool logicop_changed = false;
    if (state->logicop_enable != sub_ctx->hw_blend_state.logicop_enable) {
       sub_ctx->hw_blend_state.logicop_enable = state->logicop_enable;
+      logicop_changed = true;
+   }
+
+   if (state->logicop_enable &&
+       state->logicop_func != sub_ctx->hw_blend_state.logicop_func) {
+      sub_ctx->hw_blend_state.logicop_func = state->logicop_func;
+      logicop_changed = true;
+   }
+
+   if (logicop_changed) {
       if (vrend_state.use_gles) {
          if (can_emulate_logicop(state->logicop_func))
             sub_ctx->shader_dirty = true;
@@ -6195,7 +6518,7 @@ static void vrend_hw_emit_blend(struct vrend_sub_context *sub_ctx, struct pipe_b
          if (state->rt[i].blend_enable) {
             bool dual_src = util_blend_state_is_dual(&sub_ctx->blend_state, i);
             if (dual_src && !has_feature(feat_dual_src_blend)) {
-               vrend_printf( "dual src blend requested but not supported for rt %d\n", i);
+               virgl_error("Dual src blend requested but not supported for rt %d\n", i);
                continue;
             }
 
@@ -6221,7 +6544,7 @@ static void vrend_hw_emit_blend(struct vrend_sub_context *sub_ctx, struct pipe_b
       if (state->rt[0].blend_enable) {
          bool dual_src = util_blend_state_is_dual(&sub_ctx->blend_state, 0);
          if (dual_src && !has_feature(feat_dual_src_blend)) {
-            vrend_printf( "dual src blend requested but not supported for rt 0\n");
+            virgl_error("Dual src blend requested but not supported for rt 0\n");
          }
          glBlendFuncSeparate(translate_blend_factor(state->rt[0].rgb_src_factor),
                              translate_blend_factor(state->rt[0].rgb_dst_factor),
@@ -6278,14 +6601,13 @@ static void vrend_patch_blend_state(struct vrend_sub_context *sub_ctx)
    struct pipe_blend_state *state = &sub_ctx->blend_state;
    bool swizzle_blend_color = false;
    struct pipe_blend_color blend_color = sub_ctx->blend_color;
-   int i;
 
    if (sub_ctx->nr_cbufs == 0) {
       sub_ctx->blend_state_dirty = false;
       return;
    }
 
-   for (i = 0; i < (state->independent_blend_enable ? PIPE_MAX_COLOR_BUFS : 1); i++) {
+   for (uint32_t i = 0; i < (state->independent_blend_enable ? PIPE_MAX_COLOR_BUFS : 1); i++) {
       if (i < sub_ctx->nr_cbufs && sub_ctx->surf[i]) {
          if (vrend_format_is_emulated_alpha(sub_ctx->surf[i]->format)) {
             if (state->rt[i].blend_enable) {
@@ -6354,69 +6676,76 @@ void vrend_object_bind_blend(struct vrend_context *ctx,
    ctx->sub->blend_state_dirty = true;
 }
 
-static void vrend_hw_emit_dsa(struct vrend_context *ctx)
+static void vrend_hw_emit_dsa(struct vrend_sub_context *sub_ctx)
 {
-   struct pipe_depth_stencil_alpha_state *state = &ctx->sub->dsa_state;
+   struct pipe_depth_stencil_alpha_state *state = &sub_ctx->dsa_state;
 
    if (state->depth.enabled) {
-      vrend_depth_test_enable(ctx, true);
+      vrend_depth_test_enable(sub_ctx, true);
       glDepthFunc(GL_NEVER + state->depth.func);
       if (state->depth.writemask)
          glDepthMask(GL_TRUE);
       else
          glDepthMask(GL_FALSE);
    } else
-      vrend_depth_test_enable(ctx, false);
+      vrend_depth_test_enable(sub_ctx, false);
 
    if (state->alpha.enabled) {
-      vrend_alpha_test_enable(ctx, true);
+      vrend_alpha_test_enable(sub_ctx, true);
       if (!vrend_state.use_core_profile)
          glAlphaFunc(GL_NEVER + state->alpha.func, state->alpha.ref_value);
    } else
-      vrend_alpha_test_enable(ctx, false);
+      vrend_alpha_test_enable(sub_ctx, false);
 
 
 }
-void vrend_object_bind_dsa(struct vrend_context *ctx,
-                           uint32_t handle)
+
+static void vrend_object_bind_dsa_to_sub_context(struct vrend_sub_context *sub_ctx,
+                                                 uint32_t handle)
 {
    struct vrend_depth_stencil_alpha_state *state;
 
    if (handle == 0) {
-      if (ctx->sub->dsa) {
+      if (sub_ctx->dsa) {
          // unbind and set default state
-         memset(&ctx->sub->dsa_state, 0, sizeof(ctx->sub->dsa_state));
-         ctx->sub->dsa->owning_sub = NULL;
-         ctx->sub->dsa = NULL;
-         ctx->sub->stencil_state_dirty = true;
-         ctx->sub->shader_dirty = true;
-         vrend_hw_emit_dsa(ctx);
+         memset(&sub_ctx->dsa_state, 0, sizeof(sub_ctx->dsa_state));
+         sub_ctx->dsa->owning_sub = NULL;
+         sub_ctx->dsa = NULL;
+         sub_ctx->stencil_state_dirty = true;
+         sub_ctx->shader_dirty = true;
+         vrend_hw_emit_dsa(sub_ctx);
       }
 
       return;
    }
 
-   state = vrend_object_lookup(ctx->sub->object_hash, handle, VIRGL_OBJECT_DSA);
+   state = vrend_object_lookup(sub_ctx->object_hash, handle, VIRGL_OBJECT_DSA);
    if (!state) {
-      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_HANDLE, handle);
+      vrend_report_context_error(sub_ctx->parent, VIRGL_ERROR_CTX_ILLEGAL_HANDLE, handle);
       return;
    }
 
-   if (ctx->sub->dsa != state) {
-      ctx->sub->stencil_state_dirty = true;
-      ctx->sub->shader_dirty = true;
+   if (sub_ctx->dsa != state) {
+      sub_ctx->stencil_state_dirty = true;
+      sub_ctx->shader_dirty = true;
    }
 
-   ctx->sub->dsa_state = state->base;
-   ctx->sub->dsa = state;
-   state->owning_sub = ctx->sub;
+   sub_ctx->dsa_state = state->base;
+   sub_ctx->dsa = state;
+   state->owning_sub = sub_ctx;
 
-   if (ctx->sub->sysvalue_data.alpha_ref_val != state->base.alpha.ref_value) {
-      ctx->sub->sysvalue_data.alpha_ref_val = state->base.alpha.ref_value;
-      ctx->sub->sysvalue_data_cookie++;
+   if (sub_ctx->sysvalue_data.alpha_ref_val != state->base.alpha.ref_value) {
+      sub_ctx->sysvalue_data.alpha_ref_val = state->base.alpha.ref_value;
+      sub_ctx->sysvalue_data_cookie++;
    }
 
-   vrend_hw_emit_dsa(ctx);
+   vrend_hw_emit_dsa(sub_ctx);
+}
+
+void vrend_object_bind_dsa(struct vrend_context *ctx,
+                           uint32_t handle)
+{
+   vrend_object_bind_dsa_to_sub_context (ctx->sub, handle);
 }
 
 static void vrend_update_frontface_state(struct vrend_sub_context *sub_ctx)
@@ -6481,8 +6810,7 @@ static inline GLenum translate_fill(uint32_t mode)
    case PIPE_POLYGON_MODE_FILL:
       return GL_FILL;
    default:
-      assert(0);
-      return 0;
+      return GL_NONE;
    }
 }
 
@@ -6525,11 +6853,12 @@ static void vrend_hw_emit_rs(struct vrend_context *ctx)
          glDisable(GL_RASTERIZER_DISCARD);
    }
 
+
    if (vrend_state.use_gles == true) {
-      if (translate_fill(state->fill_front) != GL_FILL) {
+      if (state->fill_front != PIPE_POLYGON_MODE_FILL) {
          report_gles_warn(ctx, GLES_WARN_POLYGON_MODE);
       }
-      if (translate_fill(state->fill_back) != GL_FILL) {
+      if (state->fill_back != PIPE_POLYGON_MODE_FILL) {
          report_gles_warn(ctx, GLES_WARN_POLYGON_MODE);
       }
    } else if (vrend_state.use_core_profile == false) {
@@ -6586,7 +6915,7 @@ static void vrend_hw_emit_rs(struct vrend_context *ctx)
           glClipControl(GL_LOWER_LEFT, depthrule);
           ctx->sub->hw_rs_state.clip_halfz = state->clip_halfz;
        } else {
-          vrend_printf("No clip control supported\n");
+          virgl_warn("No clip control supported\n");
        }
    }
    if (state->flatshade_first != ctx->sub->hw_rs_state.flatshade_first) {
@@ -6607,7 +6936,7 @@ static void vrend_hw_emit_rs(struct vrend_context *ctx)
    else
        glPolygonOffset(state->offset_scale, state->offset_units);
 
-   if (vrend_state.use_core_profile == false) {
+   if (!vrend_shader_use_core(ctx)) {
       if (state->poly_stipple_enable)
          glEnable(GL_POLYGON_STIPPLE);
       else
@@ -6642,7 +6971,7 @@ static void vrend_hw_emit_rs(struct vrend_context *ctx)
          glCullFace(GL_FRONT_AND_BACK);
          break;
       default:
-         vrend_printf( "unhandled cull-face: %x\n", state->cull_face);
+         virgl_warn("Unhandled cull-face: %x\n", state->cull_face);
       }
       glEnable(GL_CULL_FACE);
    } else
@@ -6723,10 +7052,10 @@ static void vrend_hw_emit_rs(struct vrend_context *ctx)
 
    if (has_feature(feat_multisample)) {
       if (has_feature(feat_sample_mask)) {
-	 if (state->multisample)
-	    glEnable(GL_SAMPLE_MASK);
-	 else
-	    glDisable(GL_SAMPLE_MASK);
+         if (state->multisample)
+            glEnable(GL_SAMPLE_MASK);
+         else
+            glDisable(GL_SAMPLE_MASK);
       }
 
       /* GLES doesn't have GL_MULTISAMPLE */
@@ -6795,8 +7124,6 @@ void vrend_bind_sampler_states(struct vrend_context *ctx,
       return;
    }
 
-   ctx->sub->num_sampler_states[shader_type] = num_states;
-
    for (i = 0; i < num_states; i++) {
       if (handles[i] == 0)
          state = NULL;
@@ -6804,25 +7131,23 @@ void vrend_bind_sampler_states(struct vrend_context *ctx,
          state = vrend_object_lookup(ctx->sub->object_hash, handles[i], VIRGL_OBJECT_SAMPLER_STATE);
 
       if (!state && handles[i])
-         vrend_printf("Failed to bind sampler state (handle=%d)\n", handles[i]);
+         virgl_warn("Failed to bind sampler state (handle=%d)\n", handles[i]);
 
       if (state)
          state->sub_ctx = ctx->sub;
-      ctx->sub->sampler_state[shader_type][start_slot + i] = state;
-      ctx->sub->sampler_views_dirty[shader_type] |= (1 << (start_slot + i));
+      ctx->sub->views[shader_type].samplers[start_slot + i] = state;
+      ctx->sub->views[shader_type].dirty_mask |= (1u << (start_slot + i));
    }
 }
 
 static void vrend_apply_sampler_state(struct vrend_sub_context *sub_ctx,
                                       struct vrend_resource *res,
-                                      uint32_t shader_type,
-                                      int id,
-                                      int sampler_id,
+                                      struct vrend_sampler_state *sampler_state,
+                                      GLuint sampler_id,
                                       struct vrend_sampler_view *tview)
 {
    struct vrend_texture *tex = (struct vrend_texture *)res;
-   struct vrend_sampler_state *vstate = sub_ctx->sampler_state[shader_type][id];
-   struct pipe_sampler_state *state = &vstate->base;
+   struct pipe_sampler_state *state = &sampler_state->base;
    bool set_all = false;
    GLenum target = tex->base.target;
 
@@ -6847,7 +7172,7 @@ static void vrend_apply_sampler_state(struct vrend_sub_context *sub_ctx,
     */
    bool is_emulated_alpha = vrend_format_is_emulated_alpha(tview->format);
    if (has_feature(feat_samplers)) {
-      int sampler = vstate->ids[tview->srgb_decode == GL_SKIP_DECODE_EXT ? 0 : 1];
+      int sampler = sampler_state->ids[tview->srgb_decode == GL_SKIP_DECODE_EXT ? 0 : 1];
       if (is_emulated_alpha) {
          union pipe_color_union border_color;
          border_color = state->border_color;
@@ -6864,11 +7189,11 @@ static void vrend_apply_sampler_state(struct vrend_sub_context *sub_ctx,
       set_all = true;
 
    if (tex->state.wrap_s != state->wrap_s || set_all)
-      glTexParameteri(target, GL_TEXTURE_WRAP_S, convert_wrap(state->wrap_s));
+      glTexParameteri(target, GL_TEXTURE_WRAP_S, convert_wrap(sub_ctx->parent, state->wrap_s));
    if (tex->state.wrap_t != state->wrap_t || set_all)
-      glTexParameteri(target, GL_TEXTURE_WRAP_T, convert_wrap(state->wrap_t));
+      glTexParameteri(target, GL_TEXTURE_WRAP_T, convert_wrap(sub_ctx->parent, state->wrap_t));
    if (tex->state.wrap_r != state->wrap_r || set_all)
-      glTexParameteri(target, GL_TEXTURE_WRAP_R, convert_wrap(state->wrap_r));
+      glTexParameteri(target, GL_TEXTURE_WRAP_R, convert_wrap(sub_ctx->parent, state->wrap_r));
    if (tex->state.min_img_filter != state->min_img_filter ||
        tex->state.min_mip_filter != state->min_mip_filter || set_all)
       glTexParameterf(target, GL_TEXTURE_MIN_FILTER, convert_min_filter(state->min_img_filter, state->min_mip_filter));
@@ -6988,28 +7313,24 @@ static void free_fence_locked(struct vrend_fence *fence)
 
 static void vrend_free_fences(void)
 {
-   struct vrend_fence *fence, *stor;
-
    /* this is called after vrend_free_sync_thread */
    assert(!vrend_state.sync_thread);
 
-   LIST_FOR_EACH_ENTRY_SAFE(fence, stor, &vrend_state.fence_list, fences)
+   list_for_each_entry_safe(struct vrend_fence, fence, &vrend_state.fence_list, fences)
       free_fence_locked(fence);
-   LIST_FOR_EACH_ENTRY_SAFE(fence, stor, &vrend_state.fence_wait_list, fences)
+   list_for_each_entry_safe(struct vrend_fence, fence, &vrend_state.fence_wait_list, fences)
       free_fence_locked(fence);
 }
 
 static void vrend_free_fences_for_context(struct vrend_context *ctx)
 {
-   struct vrend_fence *fence, *stor;
-
    if (vrend_state.sync_thread) {
       mtx_lock(&vrend_state.fence_mutex);
-      LIST_FOR_EACH_ENTRY_SAFE(fence, stor, &vrend_state.fence_list, fences) {
+      list_for_each_entry_safe(struct vrend_fence, fence, &vrend_state.fence_list, fences) {
          if (fence->ctx == ctx)
             free_fence_locked(fence);
       }
-      LIST_FOR_EACH_ENTRY_SAFE(fence, stor, &vrend_state.fence_wait_list, fences) {
+      list_for_each_entry_safe(struct vrend_fence, fence, &vrend_state.fence_wait_list, fences) {
          if (fence->ctx == ctx)
             free_fence_locked(fence);
       }
@@ -7019,7 +7340,7 @@ static void vrend_free_fences_for_context(struct vrend_context *ctx)
       }
       mtx_unlock(&vrend_state.fence_mutex);
    } else {
-      LIST_FOR_EACH_ENTRY_SAFE(fence, stor, &vrend_state.fence_list, fences) {
+      list_for_each_entry_safe(struct vrend_fence, fence, &vrend_state.fence_list, fences) {
          if (fence->ctx == ctx)
             free_fence_locked(fence);
       }
@@ -7038,7 +7359,7 @@ static bool do_wait(struct vrend_fence *fence, bool can_block)
    do {
       GLenum glret = glClientWaitSync(fence->glsyncobj, 0, timeout);
       if (glret == GL_WAIT_FAILED) {
-         vrend_printf( "wait sync failed: illegal fence object %p\n", fence->glsyncobj);
+         virgl_warn("Wait sync failed: illegal fence object %p\n", (void*) fence->glsyncobj);
       }
       done = glret != GL_TIMEOUT_EXPIRED;
    } while (!done && can_block);
@@ -7108,7 +7429,7 @@ static void wait_sync(struct vrend_fence *fence)
          ts.tv_sec += 5;
          ret = cnd_timedwait(&vrend_state.poll_cond, &vrend_state.poll_mutex, &ts);
          if (ret)
-            vrend_printf("timeout (5s) waiting for renderer poll() to finish.");
+            virgl_warn("timeout (5s) waiting for renderer poll() to finish.");
       } while (vrend_state.polling && ret);
    }
 
@@ -7128,7 +7449,6 @@ static void wait_sync(struct vrend_fence *fence)
 static int thread_sync(UNUSED void *arg)
 {
    virgl_gl_context gl_context = vrend_state.sync_context;
-   struct vrend_fence *fence, *stor;
 
    u_thread_setname("vrend-sync");
 
@@ -7136,13 +7456,13 @@ static int thread_sync(UNUSED void *arg)
    vrend_clicbs->make_current_surfaceless(gl_context);
 
    while (!vrend_state.stop_sync_thread) {
-      if (LIST_IS_EMPTY(&vrend_state.fence_wait_list) &&
+      if (list_is_empty(&vrend_state.fence_wait_list) &&
           cnd_wait(&vrend_state.fence_cond, &vrend_state.fence_mutex) != 0) {
-         vrend_printf( "error while waiting on condition\n");
+         virgl_warn("Error while waiting on condition\n");
          break;
       }
 
-      LIST_FOR_EACH_ENTRY_SAFE(fence, stor, &vrend_state.fence_wait_list, fences) {
+      list_for_each_entry_safe(struct vrend_fence, fence, &vrend_state.fence_wait_list, fences) {
          if (vrend_state.stop_sync_thread)
             break;
          list_del(&fence->fences);
@@ -7161,7 +7481,7 @@ static int thread_sync(UNUSED void *arg)
 
 static void vrend_renderer_use_threaded_sync(void)
 {
-   struct virgl_gl_ctx_param ctx_params;
+   struct virgl_gl_ctx_param ctx_params = {0};
 
    ctx_params.shared = true;
    ctx_params.major_ver = vrend_state.gl_major_ver;
@@ -7171,15 +7491,17 @@ static void vrend_renderer_use_threaded_sync(void)
 
    vrend_state.sync_context = vrend_clicbs->create_gl_context_surfaceless(0, &ctx_params);
    if (vrend_state.sync_context == NULL) {
-      vrend_printf( "failed to create sync opengl context\n");
+      virgl_error("Failed to create sync opengl context\n");
       return;
    }
 
-   vrend_state.eventfd = create_eventfd(0);
    if (vrend_state.eventfd == -1) {
-      vrend_printf( "Failed to create eventfd\n");
-      vrend_clicbs->destroy_gl_context_surfaceless(vrend_state.sync_context);
-      return;
+      vrend_state.eventfd = create_eventfd(0);
+      if (vrend_state.eventfd == -1) {
+         virgl_error("Failed to create eventfd\n");
+         vrend_clicbs->destroy_gl_context_surfaceless(vrend_state.sync_context);
+         return;
+      }
    }
 
    cnd_init(&vrend_state.fence_cond);
@@ -7188,6 +7510,8 @@ static void vrend_renderer_use_threaded_sync(void)
    mtx_init(&vrend_state.poll_mutex, mtx_plain);
    vrend_state.polling = false;
 
+   assert(!vrend_state.sync_thread);
+
    vrend_state.sync_thread = u_thread_create(thread_sync, NULL);
    if (!vrend_state.sync_thread) {
       close(vrend_state.eventfd);
@@ -7208,7 +7532,7 @@ static void vrend_debug_cb(UNUSED GLenum source, GLenum type, UNUSED GLuint id,
       return;
    }
 
-   vrend_printf( "ERROR: %s\n", message);
+   virgl_error("ERROR: %s\n", message);
 }
 
 static void vrend_pipe_resource_unref(struct pipe_resource *pres,
@@ -7254,7 +7578,7 @@ static enum virgl_resource_fd_type vrend_pipe_resource_export_fd(UNUSED struct p
                                                                  UNUSED int *fd,
                                                                  UNUSED void *data)
 {
-#ifdef ENABLE_MINIGBM_ALLOCATION
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
    struct vrend_resource *res = (struct vrend_resource *)pres;
 
    if (res->storage_bits & VREND_STORAGE_GBM_BUFFER) {
@@ -7280,7 +7604,7 @@ bool vrend_check_no_error(struct vrend_context *ctx)
 #ifdef CHECK_GL_ERRORS
       vrend_report_context_error(ctx, VIRGL_ERROR_CTX_UNKNOWN, err);
 #else
-      vrend_printf("GL error reported (%d) for context %d\n", err, ctx->ctx_id);
+      virgl_warn("GL error reported (%d) for context %d\n", err, ctx->ctx_id);
 #endif
       err = glGetError();
    }
@@ -7310,11 +7634,7 @@ static bool use_integer(void) {
       return true;
 
    const char * a = (const char *) glGetString(GL_VENDOR);
-   if (!a)
-       return false;
-   if (strcmp(a, "ARM") == 0)
-      return true;
-   return false;
+   return a && !(strcmp(a, "ARM") && strcmp(a, "Google Inc. (Apple)"));
 }
 
 int vrend_renderer_init(const struct vrend_if_cbs *cbs, uint32_t flags)
@@ -7322,7 +7642,7 @@ int vrend_renderer_init(const struct vrend_if_cbs *cbs, uint32_t flags)
    bool gles;
    int gl_ver;
    virgl_gl_context gl_context;
-   struct virgl_gl_ctx_param ctx_params;
+   struct virgl_gl_ctx_param ctx_params = {0};
 
    vrend_clicbs = cbs;
 
@@ -7336,6 +7656,10 @@ int vrend_renderer_init(const struct vrend_if_cbs *cbs, uint32_t flags)
    }
 
    ctx_params.shared = false;
+   if (flags & VREND_USE_COMPAT_CONTEXT) {
+      ctx_params.compat_ctx = true;
+   }
+
    for (uint32_t i = 0; i < ARRAY_SIZE(gl_versions); i++) {
       ctx_params.major_ver = gl_versions[i].major;
       ctx_params.minor_ver = gl_versions[i].minor;
@@ -7345,9 +7669,44 @@ int vrend_renderer_init(const struct vrend_if_cbs *cbs, uint32_t flags)
          break;
    }
 
+   if (!gl_context) {
+      virgl_error("Unable to create %s context >= 3.0\n",
+                  flags & VREND_USE_GLES ? "GLES" : "OpenGL");
+      goto fail;
+   }
+
    vrend_clicbs->make_current(gl_context);
    gl_ver = epoxy_gl_version();
 
+   /* Surface the full GL strings early for debugging/profile confirmation. */
+   const GLubyte *gl_ver_str = glGetString(GL_VERSION);
+   const GLubyte *gl_renderer_str = glGetString(GL_RENDERER);
+   const GLubyte *glsl_ver_str = glGetString(GL_SHADING_LANGUAGE_VERSION);
+
+   /* On macOS+Metal the GL_VERSION string can start with just the numeric
+    * version and "Metal"; reshape it to a clearer OpenGL 4.x label for logs.
+    */
+   char gl_ver_buf[128];
+   const char *gl_ver_display = gl_ver_str ? (const char *)gl_ver_str : "(null)";
+#ifdef __APPLE__
+   int gl_major_num = gl_ver / 10;
+   int gl_minor_num = gl_ver % 10;
+   if (gl_ver_str && strstr((const char *)gl_ver_str, "Metal")) {
+      snprintf(gl_ver_buf, sizeof(gl_ver_buf), "OpenGL %d.%d (Metal)", gl_major_num, gl_minor_num);
+      gl_ver_display = gl_ver_buf;
+   }
+#endif
+
+      virgl_info("GL strings: version='%s' renderer='%s' glsl='%s'\n",
+            gl_ver_display,
+            gl_renderer_str ? (const char *)gl_renderer_str : "(null)",
+            glsl_ver_str ? (const char *)glsl_ver_str : "(null)");
+      /* Duplicate to stderr so it survives restrictive log levels or early aborts. */
+      fprintf(stderr, "GL strings: version='%s' renderer='%s' glsl='%s'\n",
+         gl_ver_display,
+         gl_renderer_str ? (const char *)gl_renderer_str : "(null)",
+         glsl_ver_str ? (const char *)glsl_ver_str : "(null)");
+
    /* enable error output as early as possible */
    if (vrend_debug(NULL, dbg_khr) && epoxy_has_gl_extension("GL_KHR_debug")) {
       glDebugMessageCallback(vrend_debug_cb, NULL);
@@ -7363,15 +7722,15 @@ int vrend_renderer_init(const struct vrend_if_cbs *cbs, uint32_t flags)
    vrend_state.gl_minor_ver = gl_ver % 10;
 
    if (gles) {
-      vrend_printf( "gl_version %d - es profile enabled\n", gl_ver);
+      virgl_info("gl_version %d - es profile enabled\n", gl_ver);
       vrend_state.use_gles = true;
       /* for now, makes the rest of the code use the most GLES 3.x like path */
       vrend_state.use_core_profile = true;
    } else if (gl_ver > 30 && !epoxy_has_gl_extension("GL_ARB_compatibility")) {
-      vrend_printf( "gl_version %d - core profile enabled\n", gl_ver);
+      virgl_info("gl_version %d - core profile enabled\n", gl_ver);
       vrend_state.use_core_profile = true;
    } else {
-      vrend_printf( "gl_version %d - compat profile\n", gl_ver);
+      virgl_info("gl_version %d - compat profile\n", gl_ver);
    }
 
    vrend_state.use_integer = use_integer();
@@ -7379,11 +7738,29 @@ int vrend_renderer_init(const struct vrend_if_cbs *cbs, uint32_t flags)
    init_features(gles ? 0 : gl_ver,
                  gles ? gl_ver : 0);
 
+   /* Disable host video decode/encode paths entirely in this build to avoid
+    * guest CREATE_VIDEO_BUFFER commands on configurations that cannot service
+    * them (e.g., macOS ANGLE/Metal). This also keeps caps consistent with
+    * the advertised zero video caps below.
+    */
+   vrend_state.video_available = false;
+
+#ifdef __APPLE__
+   /* macOS core GL 4.1 lacks GL_ARB_copy_image; force fallback paths. */
+   clear_feature(feat_copy_image);
+#endif
+
    if (!vrend_winsys_has_gl_colorspace())
       clear_feature(feat_srgb_write_control) ;
 
    glGetIntegerv(GL_MAX_DRAW_BUFFERS, (GLint *) &vrend_state.max_draw_buffers);
 
+   /* For testing we need to know maximum */
+   glGetIntegerv(GL_MAX_VERTEX_ATTRIBS, (GLint*)&vrend_state.max_vertex_attributes);
+
+   glGetIntegerv(GL_MAX_COMBINED_TEXTURE_IMAGE_UNITS,
+                 (GLint*)&vrend_state.max_texture_units);
+
    /* Mesa clamps this value to 8 anyway, so just make sure that this side
     * doesn't exceed the number to be on the save side when using 8-bit masks
     * for the color buffers */
@@ -7392,7 +7769,7 @@ int vrend_renderer_init(const struct vrend_if_cbs *cbs, uint32_t flags)
 
    if (!has_feature(feat_arb_robustness) &&
        !has_feature(feat_gles_khr_robustness)) {
-      vrend_printf("WARNING: running without ARB/KHR robustness in place may crash\n");
+      virgl_warn("Running without ARB/KHR robustness in place may crash\n");
    }
 
    /* callbacks for when we are cleaning up the object table */
@@ -7438,6 +7815,11 @@ int vrend_renderer_init(const struct vrend_if_cbs *cbs, uint32_t flags)
 
    /* create 0 context */
    vrend_state.ctx0 = vrend_create_context(0, strlen("HOST"), "HOST");
+   if (!vrend_state.ctx0) {
+      virgl_error("Unable to create %s vrend context\n",
+                  flags & VREND_USE_GLES ? "GLES" : "OpenGL");
+      goto fail;
+   }
 
    vrend_state.eventfd = -1;
    if (flags & VREND_USE_THREAD_SYNC) {
@@ -7452,9 +7834,14 @@ int vrend_renderer_init(const struct vrend_if_cbs *cbs, uint32_t flags)
    vrend_state.use_egl_fence = virgl_egl_supports_fences(egl);
 #endif
 
-   if (!vrend_check_no_error(vrend_state.ctx0) || !has_feature(feat_ubo)) {
-      vrend_renderer_fini();
-      return EINVAL;
+   if (!vrend_check_no_error(vrend_state.ctx0)) {
+      virgl_error("vrend context creation resulted in errors\n");
+      goto cleanup_and_fail;
+   }
+
+   if (!has_feature(feat_ubo)) {
+      virgl_error("Host context missing support for required extension ARB_uniform_buffer_object\n");
+      goto cleanup_and_fail;
    }
 
 #ifdef ENABLE_VIDEO
@@ -7462,13 +7849,17 @@ int vrend_renderer_init(const struct vrend_if_cbs *cbs, uint32_t flags)
         if (vrend_clicbs->get_drm_fd)
             vrend_video_init(vrend_clicbs->get_drm_fd());
         else
-            vrend_printf("video disabled due to missing get_drm_fd\n");
+            virgl_warn("Video disabled due to missing get_drm_fd\n");
    }
 #endif
 
    vrend_state.d3d_share_texture = flags & VREND_D3D11_SHARE_TEXTURE;
 
    return 0;
+cleanup_and_fail:
+   vrend_renderer_fini();
+fail:
+   return EINVAL;
 }
 
 void
@@ -7498,8 +7889,6 @@ vrend_renderer_fini(void)
 
 static void vrend_destroy_sub_context(struct vrend_sub_context *sub)
 {
-   struct vrend_streamout_object *obj, *tmp;
-
    vrend_clicbs->make_current(sub->gl_context);
 
    if (has_feature(feat_images)) {
@@ -7555,17 +7944,15 @@ static void vrend_destroy_sub_context(struct vrend_sub_context *sub)
 
          glDisableVertexAttribArray(i);
       }
-      glDeleteVertexArrays(1, &sub->vaoid);
    }
-
+   glDeleteVertexArrays(1, &sub->vaoid);
    glBindVertexArray(0);
 
    if (sub->current_so)
       glBindTransformFeedback(GL_TRANSFORM_FEEDBACK, 0);
 
-   LIST_FOR_EACH_ENTRY_SAFE(obj, tmp, &sub->streamout_list, head) {
+   list_for_each_entry_safe(struct vrend_streamout_object, obj, &sub->streamout_list, head)
       vrend_destroy_streamout_object(obj);
-   }
 
    vrend_shader_state_reference(&sub->shaders[PIPE_SHADER_VERTEX], NULL);
    vrend_shader_state_reference(&sub->shaders[PIPE_SHADER_FRAGMENT], NULL);
@@ -7592,12 +7979,15 @@ static void vrend_destroy_sub_context(struct vrend_sub_context *sub)
       for (unsigned i = 0; i < PIPE_MAX_SHADER_IMAGES; i++) {
          glDeleteTextures(1, &sub->image_views[type][i].view_id);
       }
+
+      if (sub->long_shader_in_progress[type])
+         vrend_destroy_long_shader_buffer(sub->long_shader_in_progress[type]);
    }
 
    if (sub->zsurf)
       vrend_surface_reference(&sub->zsurf, NULL);
 
-   for (int i = 0; i < sub->nr_cbufs; i++) {
+   for (uint32_t i = 0; i < sub->nr_cbufs; i++) {
       if (!sub->surf[i])
          continue;
       vrend_surface_reference(&sub->surf[i], NULL);
@@ -7625,8 +8015,6 @@ void vrend_destroy_context(struct vrend_context *ctx)
 {
    bool switch_0 = (ctx == vrend_state.current_ctx);
    struct vrend_context *cur = vrend_state.current_ctx;
-   struct vrend_sub_context *sub, *tmp;
-   struct vrend_untyped_resource *untyped_res, *untyped_res_tmp;
    if (switch_0) {
       vrend_state.current_ctx = NULL;
       vrend_state.current_hw_ctx = NULL;
@@ -7647,9 +8035,14 @@ void vrend_destroy_context(struct vrend_context *ctx)
 
    vrend_set_index_buffer(ctx, 0, 0, 0);
 
-   LIST_FOR_EACH_ENTRY_SAFE(sub, tmp, &ctx->sub_ctxs, head)
+   list_for_each_entry_safe_rev(struct vrend_sub_context, sub, &ctx->sub_ctxs, head) {
+      ctx->sub = sub;
       vrend_destroy_sub_context(sub);
-   if(ctx->ctx_id)
+   }
+   ctx->sub = NULL;
+   ctx->sub0 = NULL;
+
+   if (ctx->ctx_id)
       vrend_renderer_force_ctx_0();
 
    vrend_free_fences_for_context(ctx);
@@ -7658,7 +8051,11 @@ void vrend_destroy_context(struct vrend_context *ctx)
    vrend_video_destroy_context(ctx->video);
 #endif
 
-   LIST_FOR_EACH_ENTRY_SAFE(untyped_res, untyped_res_tmp, &ctx->untyped_resources, head)
+   list_for_each_entry_safe(struct vrend_resource, res, &ctx->vrend_resources, head) {
+      free(res);
+   }
+
+   list_for_each_entry_safe(struct vrend_untyped_resource, untyped_res, &ctx->untyped_resources, head)
       free(untyped_res);
    vrend_ctx_resource_fini_table(ctx->res_hash);
 
@@ -7674,6 +8071,7 @@ void vrend_destroy_context(struct vrend_context *ctx)
 
 struct vrend_context *vrend_create_context(int id, uint32_t nlen, const char *debug_name)
 {
+
    struct vrend_context *grctx = CALLOC_STRUCT(vrend_context);
 
    if (!grctx)
@@ -7681,8 +8079,8 @@ struct vrend_context *vrend_create_context(int id, uint32_t nlen, const char *de
 
    if (nlen && debug_name) {
       strncpy(grctx->debug_name, debug_name,
-	      nlen < sizeof(grctx->debug_name) - 1 ?
-	      nlen : sizeof(grctx->debug_name) - 1);
+              nlen < sizeof(grctx->debug_name) - 1 ?
+              nlen : sizeof(grctx->debug_name) - 1);
       grctx->debug_name[sizeof(grctx->debug_name) - 1] = 0;
    }
 
@@ -7721,7 +8119,14 @@ struct vrend_context *vrend_create_context(int id, uint32_t nlen, const char *de
    vrend_renderer_create_sub_ctx(grctx, 0);
    vrend_renderer_set_sub_ctx(grctx, 0);
 
-   grctx->shader_cfg.glsl_version = vrender_get_glsl_version();
+   int glver = get_glsl_version();
+   if (glver < 0) {
+      virgl_error("Unable to query GL version\n");
+      vrend_destroy_context(grctx);
+      return NULL;
+   }
+
+   grctx->shader_cfg.glsl_version = glver;
 
    if (!grctx->ctx_id)
       grctx->fence_retire = vrend_clicbs->ctx0_fence_retire;
@@ -7877,7 +8282,7 @@ static int check_resource_valid(const struct vrend_renderer_resource_create_args
          return -1;
       }
 
-#ifdef ENABLE_MINIGBM_ALLOCATION
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
       if (!virgl_gbm_gpu_import_required(args->bind)) {
          return 0;
       }
@@ -7983,31 +8388,31 @@ static void vrend_create_buffer(struct vrend_resource *gr, uint32_t width, uint3
       buffer_storage_flags |= GL_MAP_COHERENT_BIT;
 
    gr->storage_bits |= VREND_STORAGE_GL_BUFFER;
-   glGenBuffersARB(1, &gr->id);
-   glBindBufferARB(gr->target, gr->id);
+   glGenBuffersARB(1, &gr->gl_id);
+   glBindBufferARB(gr->target, gr->gl_id);
 
    if (buffer_storage_flags) {
       if (has_feature(feat_arb_buffer_storage) && !vrend_state.use_external_blob) {
          glBufferStorage(gr->target, width, NULL, buffer_storage_flags);
          gr->map_info = vrend_state.inferred_gl_caching_type;
       }
-#ifdef ENABLE_MINIGBM_ALLOCATION
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
       else if (has_feature(feat_memory_object_fd) && has_feature(feat_memory_object)) {
          GLuint memobj = 0;
          int fd = -1;
-	 int ret;
+         int ret;
 
          /* Could use VK too. */
          struct gbm_bo *bo = gbm_bo_create(gbm->device, width, 1,
                                            GBM_FORMAT_R8, GBM_BO_USE_LINEAR);
          if (!bo) {
-            vrend_printf("Failed to allocate emulated GL buffer backing storage");
+            virgl_error("Failed to allocate emulated GL buffer backing storage");
             return;
          }
 
          ret = virgl_gbm_export_fd(gbm->device, gbm_bo_get_handle(bo).u32, &fd);
          if (ret || fd < 0) {
-            vrend_printf("Failed to get file descriptor\n");
+            virgl_error("Failed to get file descriptor\n");
             return;
          }
 
@@ -8017,15 +8422,11 @@ static void vrend_create_buffer(struct vrend_resource *gr, uint32_t width, uint3
          gr->gbm_bo = bo;
          gr->memobj = memobj;
          gr->storage_bits |= VREND_STORAGE_GBM_BUFFER | VREND_STORAGE_GL_MEMOBJ;
-
-         if (!strcmp(gbm_device_get_backend_name(gbm->device), "i915"))
-            gr->map_info = VIRGL_RENDERER_MAP_CACHE_CACHED;
-         else
-            gr->map_info = VIRGL_RENDERER_MAP_CACHE_WC;
+         gr->map_info = virgl_gbm_get_map_info(bo);
       }
 #endif
       else {
-         vrend_printf("Missing buffer storage and interop extensions\n");
+         virgl_error("Missing buffer storage and interop extensions\n");
          return;
       }
 
@@ -8090,7 +8491,7 @@ vrend_resource_alloc_buffer(struct vrend_resource *gr, uint32_t flags)
       }
       vrend_create_buffer(gr, size, flags);
    } else {
-      vrend_printf("%s: Illegal buffer binding flags 0x%x\n", __func__, bind);
+      virgl_error("%s: Illegal buffer binding flags 0x%x\n", __func__, bind);
       return -EINVAL;
    }
 
@@ -8153,9 +8554,9 @@ static bool virgl_format_to_dxgi_format(uint32_t format, DXGI_FORMAT *dxgi)
    return false;
 }
 
-static UINT virgl_bind_to_d3d_bind_flags(uint32_t flags)
+static unsigned virgl_bind_to_d3d_bind_flags(uint32_t flags)
 {
-   UINT ret = 0;
+   unsigned ret = 0;
 
    if (flags & PIPE_BIND_VERTEX_BUFFER)
       ret |= D3D11_BIND_VERTEX_BUFFER;
@@ -8175,7 +8576,7 @@ static UINT virgl_bind_to_d3d_bind_flags(uint32_t flags)
    return ret;
 }
 
-static UINT virgl_usage_to_d3d_usage(uint32_t usage)
+static unsigned virgl_usage_to_d3d_usage(uint32_t usage)
 {
    switch (usage) {
    case PIPE_USAGE_DEFAULT:
@@ -8273,7 +8674,7 @@ fail:
  */
 static void vrend_resource_gbm_init(struct vrend_resource *gr, uint32_t format)
 {
-#ifdef ENABLE_MINIGBM_ALLOCATION
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
    uint32_t gbm_flags = virgl_gbm_convert_flags(gr->base.bind);
    uint32_t gbm_format = 0;
    if (virgl_gbm_convert_format(&format, &gbm_format))
@@ -8301,11 +8702,7 @@ static void vrend_resource_gbm_init(struct vrend_resource *gr, uint32_t format)
    gr->gbm_bo = bo;
    gr->storage_bits |= VREND_STORAGE_GBM_BUFFER;
 
-   /* This is true so far, but maybe gbm_bo_get_caching_type is needed in the future. */
-   if (!strcmp(gbm_device_get_backend_name(gbm->device), "i915"))
-      gr->map_info = VIRGL_RENDERER_MAP_CACHE_CACHED;
-   else
-      gr->map_info = VIRGL_RENDERER_MAP_CACHE_WC;
+   gr->map_info = virgl_gbm_get_map_info(bo);
 
    if (!virgl_gbm_gpu_import_required(gr->base.bind))
       return;
@@ -8329,7 +8726,7 @@ static int vrend_resource_alloc_texture(struct vrend_resource *gr,
                                         enum virgl_formats format,
                                         void *image_oes)
 {
-   uint level;
+   unsigned level;
    GLenum internalformat, glformat, gltype;
    struct vrend_texture *gt = (struct vrend_texture *)gr;
    struct pipe_resource *pr = &gr->base;
@@ -8357,7 +8754,7 @@ static int vrend_resource_alloc_texture(struct vrend_resource *gr,
        !(tex_conv_table[format].flags & VIRGL_TEXTURE_CAN_TARGET_RECTANGLE)) {
       /* for some guests this is the only usage of rect */
       if (pr->width0 != 1 || pr->height0 != 1) {
-         vrend_printf("Warning: specifying format incompatible with GL_TEXTURE_RECTANGLE_NV\n");
+         virgl_warn("Specifying format incompatible with GL_TEXTURE_RECTANGLE_NV\n");
       }
       gr->target = GL_TEXTURE_2D;
    }
@@ -8372,8 +8769,8 @@ static int vrend_resource_alloc_texture(struct vrend_resource *gr,
       gr->target = GL_TEXTURE_2D_ARRAY;
    }
 
-   glGenTextures(1, &gr->id);
-   glBindTexture(gr->target, gr->id);
+   glGenTextures(1, &gr->gl_id);
+   glBindTexture(gr->target, gr->gl_id);
 
    debug_texture(__func__, gr);
 
@@ -8389,10 +8786,10 @@ static int vrend_resource_alloc_texture(struct vrend_resource *gr,
               format == VIRGL_FORMAT_NV21 ||
               format == VIRGL_FORMAT_YV12 ||
               format == VIRGL_FORMAT_P010) && glGetError() != GL_NO_ERROR) {
-            vrend_printf("glEGLImageTargetTexture2DOES maybe fail\n");
+            virgl_warn("glEGLImageTargetTexture2DOES maybe fail\n");
          }
       } else {
-         vrend_printf( "missing GL_OES_EGL_image extensions\n");
+         virgl_error("Missing GL_OES_EGL_image extensions\n");
          glBindTexture(gr->target, 0);
          return EINVAL;
       }
@@ -8403,13 +8800,25 @@ static int vrend_resource_alloc_texture(struct vrend_resource *gr,
       gltype = tex_conv_table[format].gltype;
 
       if (internalformat == 0) {
-         vrend_printf("unknown format is %d\n", pr->format);
+         virgl_error("Unknown format is %d\n", pr->format);
          glBindTexture(gr->target, 0);
          return EINVAL;
       }
 
       if (pr->nr_samples > 1) {
-         if (format_can_texture_storage) {
+         /* Metal backend (macOS): Silently downgrade MSAA to non-MSAA when not supported.
+          * gl=es mode (ANGLE) handles MSAA correctly, so only apply this workaround
+          * for desktop GL where Metal backend doesn't support multisampled textures. */
+         const char *renderer_str = (const char *)glGetString(GL_RENDERER);
+         bool is_metal_backend = (renderer_str && strstr(renderer_str, "Metal"));
+         
+         if (is_metal_backend && !vrend_state.use_gles && !has_feature(feat_storage_multisample)) {
+            /* Metal backend: No MSAA support, downgrade to regular texture */
+            fprintf(stderr, "[VREND] Metal backend: MSAA texture requested (samples=%d, target=0x%x), downgrading to non-MSAA\n",
+                    pr->nr_samples, gr->target);
+            gr->target = (gr->target == GL_TEXTURE_2D_MULTISAMPLE) ? GL_TEXTURE_2D : GL_TEXTURE_2D_ARRAY;
+            pr->nr_samples = 0;
+         } else if (format_can_texture_storage) {
             if (gr->target == GL_TEXTURE_2D_MULTISAMPLE) {
                glTexStorage2DMultisample(gr->target, pr->nr_samples,
                                          internalformat, pr->width0, pr->height0,
@@ -8430,7 +8839,9 @@ static int vrend_resource_alloc_texture(struct vrend_resource *gr,
                                        GL_TRUE);
             }
          }
-      } else if (gr->target == GL_TEXTURE_CUBE_MAP) {
+      }
+      
+      if (pr->nr_samples <= 1 && gr->target == GL_TEXTURE_CUBE_MAP) {
             int i;
             if (format_can_texture_storage)
                glTexStorage2D(GL_TEXTURE_CUBE_MAP, pr->last_level + 1, internalformat, pr->width0, pr->height0);
@@ -8499,7 +8910,7 @@ static int vrend_resource_alloc_texture(struct vrend_resource *gr,
    glBindTexture(gr->target, 0);
 
    if (image_oes && gr->gbm_bo) {
-#ifdef ENABLE_MINIGBM_ALLOCATION
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
       if (!has_bit(gr->storage_bits, VREND_STORAGE_GL_BUFFER) &&
             !vrend_format_can_texture_view(gr->base.format)) {
          for (int i = 0; i < gbm_bo_get_plane_count(gr->gbm_bo); i++) {
@@ -8526,7 +8937,7 @@ vrend_resource_create(const struct vrend_renderer_resource_create_args *args)
 
    ret = check_resource_valid(args, error_string);
    if (ret) {
-      vrend_printf("%s, Illegal resource parameters, error: %s\n", __func__, error_string);
+      virgl_error("%s, Illegal resource parameters, error: %s\n", __func__, error_string);
       return NULL;
    }
 
@@ -8574,9 +8985,9 @@ vrend_renderer_resource_create(const struct vrend_renderer_resource_create_args
 void vrend_renderer_resource_destroy(struct vrend_resource *res)
 {
    if (has_bit(res->storage_bits, VREND_STORAGE_GL_TEXTURE)) {
-      glDeleteTextures(1, &res->id);
+      glDeleteTextures(1, &res->gl_id);
    } else if (has_bit(res->storage_bits, VREND_STORAGE_GL_BUFFER)) {
-      glDeleteBuffers(1, &res->id);
+      glDeleteBuffers(1, &res->gl_id);
       if (res->tbo_tex_id)
          glDeleteTextures(1, &res->tbo_tex_id);
    } else if (has_bit(res->storage_bits, VREND_STORAGE_HOST_SYSTEM_MEMORY)) {
@@ -8601,7 +9012,7 @@ void vrend_renderer_resource_destroy(struct vrend_resource *res)
       }
    }
 #endif
-#ifdef ENABLE_MINIGBM_ALLOCATION
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
    if (res->gbm_bo)
       gbm_bo_destroy(res->gbm_bo);
 #endif
@@ -8632,7 +9043,7 @@ static void vrend_scale_depth(void *ptr, int size, float scale_val)
       GLuint value = ival[i];
       GLfloat d = ((float)(value >> 8) * myscale) * scale_val;
       d = CLAMP(d, 0.0F, 1.0F);
-      ival[i] = (int)(d / myscale) << 8;
+      ival[i] = (GLuint)(d / myscale) << 8;
    }
 }
 
@@ -8727,50 +9138,50 @@ static bool check_transfer_iovec(struct vrend_resource *res,
    return (info->iovec && info->iovec_cnt) || res->iov;
 }
 
-static bool check_transfer_bounds(struct vrend_resource *res,
-                                  const struct vrend_transfer_info *info)
+static inline bool resource_contains_box(struct vrend_resource *res,
+                                         const struct pipe_box *box,
+                                         uint32_t level)
 {
-   int lwidth, lheight;
+   int64_t end_x, end_y, end_z;
+   int64_t width, height, depth;
 
    /* check mipmap level is in bounds */
-   if (info->level > res->base.last_level)
-      return false;
-   if (info->box->x < 0 || info->box->y < 0)
-      return false;
-   /* these will catch bad y/z/w/d with 1D textures etc */
-   lwidth = u_minify(res->base.width0, info->level);
-   if (info->box->width > lwidth || info->box->width < 0)
-      return false;
-   if (info->box->x > lwidth)
-      return false;
-   if (info->box->width + info->box->x > lwidth)
+   if (unlikely(level > res->base.last_level))
       return false;
 
-   lheight = u_minify(res->base.height0, info->level);
-   if (info->box->height > lheight || info->box->height < 0)
-      return false;
-   if (info->box->y > lheight)
-      return false;
-   if (info->box->height + info->box->y > lheight)
-      return false;
+   width = u_minify(res->base.width0, level);
+   height = u_minify(res->base.height0, level);
 
-   if (res->base.target == PIPE_TEXTURE_3D) {
-      int ldepth = u_minify(res->base.depth0, info->level);
-      if (info->box->depth > ldepth || info->box->depth < 0)
-         return false;
-      if (info->box->z > ldepth)
-         return false;
-      if (info->box->z + info->box->depth > ldepth)
-         return false;
-   } else {
-      if (info->box->depth > (int)res->base.array_size)
-         return false;
-      if (info->box->z > (int)res->base.array_size)
-         return false;
-      if (info->box->z + info->box->depth > (int)res->base.array_size)
-         return false;
+   /* The z value has two meanings depending of the texture type */
+   switch (res->base.target) {
+   case PIPE_TEXTURE_CUBE:
+   case PIPE_TEXTURE_1D_ARRAY:
+   case PIPE_TEXTURE_2D_ARRAY:
+   case PIPE_TEXTURE_CUBE_ARRAY:
+      depth = res->base.array_size;
+      break;
+   case PIPE_TEXTURE_3D:
+      depth = u_minify(res->base.depth0, level);
+      break;
+   default:
+      depth = 1;
+      break;
    }
 
+   /* check that the starting point is not outside of the range */
+   if (unlikely(box->x < 0 || box->y < 0 || box->z < 0 ||
+                box->x > width || box->y > height || box->z > depth))
+      return false;
+
+   end_x = (int64_t) box->x + (int64_t) box->width;
+   end_y = (int64_t) box->y + (int64_t) box->height;
+   end_z = (int64_t) box->z + (int64_t) box->depth;
+
+   /* check that the end point is not outside of the range */
+   if (unlikely(end_x < 0 || end_y < 0 || end_z < 0 ||
+                end_x > width || end_y > height  || end_z > depth ))
+      return false;
+
    return true;
 }
 
@@ -8876,6 +9287,74 @@ static void vrend_swizzle_data_bgra(uint64_t size, void *data) {
    }
 }
 
+static void vrend_swizzle_and_collapse_data_bgrx(uint64_t size, void *data) {
+   const size_t in_bpp = 4;
+   const size_t out_bpp = 3;
+
+   uint8_t *in_pixel, *out_pixel;
+   in_pixel = out_pixel = data;
+
+   // in-place modification, so output cursor must not lead
+   assert(in_bpp >= out_bpp);
+
+   uint8_t r, g, b;
+   const size_t num_pixels = size / in_bpp;
+   for (size_t i = 0; i < num_pixels; ++i) {
+      b = *(in_pixel + 0);
+      g = *(in_pixel + 1);
+      r = *(in_pixel + 2);
+
+      *(out_pixel + 0) = r;
+      *(out_pixel + 1) = g;
+      *(out_pixel + 2) = b;
+
+      in_pixel += in_bpp;
+      out_pixel += out_bpp;
+   }
+}
+
+static void vrend_collapse_data_r8g8b8x8(uint64_t size, void *data) {
+   const size_t in_bpp = 4;
+   const size_t out_bpp = 3;
+
+   uint8_t *in_pixel, *out_pixel;
+   in_pixel = out_pixel = data;
+
+   // in-place modification, so output cursor must not lead
+   assert(in_bpp >= out_bpp);
+
+   const size_t num_pixels = size / in_bpp;
+   for (size_t i = 0; i < num_pixels; ++i) {
+      *(out_pixel + 0) = *(in_pixel + 0);
+      *(out_pixel + 1) = *(in_pixel + 1);
+      *(out_pixel + 2) = *(in_pixel + 2);
+
+      in_pixel += in_bpp;
+      out_pixel += out_bpp;
+   }
+}
+
+static void vrend_collapse_data_r16g16b16x16(uint64_t size, void *data) {
+   const size_t in_channels = 4;
+   const size_t out_channels = 3;
+
+   uint16_t *in_pixel, *out_pixel;
+   in_pixel = out_pixel = data;
+
+   // in-place modification, so output cursor must not lead
+   assert(in_channels >= out_channels);
+
+   const size_t num_pixels = size / in_channels / 2;
+   for (size_t i = 0; i < num_pixels; ++i) {
+      *(out_pixel + 0) = *(in_pixel + 0);
+      *(out_pixel + 1) = *(in_pixel + 1);
+      *(out_pixel + 2) = *(in_pixel + 2);
+
+      in_pixel += in_channels;
+      out_pixel += out_channels;
+   }
+}
+
 static int vrend_renderer_transfer_write_iov(struct vrend_context *ctx,
                                              struct vrend_resource *res,
                                              const struct iovec *iov, int num_iovs,
@@ -8906,14 +9385,14 @@ static int vrend_renderer_transfer_write_iov(struct vrend_context *ctx,
       if (!info->synchronized)
          map_flags |= GL_MAP_UNSYNCHRONIZED_BIT;
 
-      glBindBufferARB(res->target, res->id);
+      glBindBufferARB(res->target, res->gl_id);
       data = glMapBufferRange(res->target, info->box->x, info->box->width, map_flags);
       if (data == NULL) {
-	 vrend_printf("map failed for element buffer\n");
-	 vrend_read_from_iovec_cb(iov, num_iovs, info->offset, info->box->width, &iov_buffer_upload, &d);
+         virgl_error("Map failed for element buffer\n");
+         vrend_read_from_iovec_cb(iov, num_iovs, info->offset, info->box->width, &iov_buffer_upload, &d);
       } else {
-	 vrend_read_from_iovec(iov, num_iovs, info->offset, data, info->box->width);
-	 glUnmapBuffer(res->target);
+         vrend_read_from_iovec(iov, num_iovs, info->offset, data, info->box->width);
+         glUnmapBuffer(res->target);
       }
       glBindBufferARB(res->target, 0);
    } else {
@@ -8925,11 +9404,11 @@ static int vrend_renderer_transfer_write_iov(struct vrend_context *ctx,
       bool compressed;
       bool invert = false;
       float depth_scale;
-      GLuint send_size = 0;
+      uint64_t send_size = 0;
       uint32_t stride = info->stride;
       uint32_t layer_stride = info->layer_stride;
 
-      vrend_use_program(ctx->sub, 0);
+      vrend_use_program(NULL);
 
       if (!stride)
          stride = util_format_get_nblocksx(res->base.format, u_minify(res->base.width0, info->level)) * elsize;
@@ -8954,7 +9433,9 @@ static int vrend_renderer_transfer_write_iov(struct vrend_context *ctx,
       }
 
       send_size = util_format_get_nblocks(res->base.format, info->box->width,
-                                          info->box->height) * elsize;
+                                          info->box->height);
+      send_size *= elsize;
+
       if (res->target == GL_TEXTURE_3D ||
           res->target == GL_TEXTURE_1D_ARRAY ||
           res->target == GL_TEXTURE_2D_ARRAY ||
@@ -8965,9 +9446,19 @@ static int vrend_renderer_transfer_write_iov(struct vrend_context *ctx,
          return EINVAL;
 
       if (need_temp) {
+         /* functions like glCompressedTexSubImage3D only support
+          * a buffer size of GLsizei = uint32_t, anything larger
+          * is bogous */
+         if (send_size > UINT_MAX) {
+            virgl_error("Used write size out of range %"PRIu64"\n", send_size);
+            return EINVAL;
+         }
+
          data = malloc(send_size);
-         if (!data)
+         if (!data) {
+            virgl_error("Memory allocation failed for %"PRIu64"\n", send_size);
             return ENOMEM;
+         }
          read_transfer_data(iov, num_iovs, data, res->base.format, info->offset,
                             stride, layer_stride, info->box, invert);
       } else {
@@ -9016,8 +9507,8 @@ static int vrend_renderer_transfer_write_iov(struct vrend_context *ctx,
          glDrawBuffers(1, &buffers);
          glDisable(GL_BLEND);
 
-         vrend_depth_test_enable(ctx, false);
-         vrend_alpha_test_enable(ctx, false);
+         vrend_depth_test_enable(ctx->sub, false);
+         vrend_alpha_test_enable(ctx->sub, false);
          vrend_stencil_test_enable(ctx->sub, false);
 
          glPixelZoom(1.0f, res->y_0_top ? -1.0f : 1.0f);
@@ -9027,7 +9518,7 @@ static int vrend_renderer_transfer_write_iov(struct vrend_context *ctx,
          glDeleteFramebuffers(1, &fb_id);
       } else {
          uint32_t comp_size;
-         glBindTexture(res->target, res->id);
+         glBindTexture(res->target, res->gl_id);
 
          if (compressed) {
             glformat = tex_conv_table[res->base.format].internalformat;
@@ -9045,9 +9536,41 @@ static int vrend_renderer_transfer_write_iov(struct vrend_context *ctx,
 
          /* GLES doesn't allow format conversions, which we need for BGRA resources with RGBA
           * internal format. So we fallback to performing a CPU swizzle before uploading. */
-         if (vrend_state.use_gles && vrend_format_is_bgra(res->base.format)) {
-            VREND_DEBUG(dbg_bgra, ctx, "manually swizzling bgra->rgba on upload since gles+bgra\n");
-            vrend_swizzle_data_bgra(send_size, data);
+         if (vrend_state.use_gles) {
+            if (vrend_resource_get_internal_format_override(res) != GL_NONE) {
+               glformat = GL_RGB;
+               glPixelStorei(GL_UNPACK_ROW_LENGTH, 0);
+               glPixelStorei(GL_UNPACK_ALIGNMENT, 1);
+
+               if (vrend_format_is_bgra(res->base.format)) {
+                  /* to make matters worse, if the resource is actually backed by a 24bpp memory
+                   * layout, but the sent data is 32bpp (with ignored alpha), then we must:
+                *   - swizzle r/b channels
+                *   - collapse to 24bpp
+                * To do so, we reconfigure the unpack params and perform in-place modification of
+                * the sent data (and perform the inverse on the readback path).
+                */
+                  VREND_DEBUG(dbg_bgra, ctx, "manually swizzling+collapsing bgrx(32bpp)->rgb(24bpp) on upload since gles+bgrx\n");
+                  vrend_swizzle_and_collapse_data_bgrx(send_size, data);
+               } else {
+                  const struct util_format_description *descr =
+                        util_format_description(res->base.format);
+                  switch (descr->channel[0].size) {
+                  case 8:
+                     vrend_collapse_data_r8g8b8x8(send_size, data);
+                     break;
+                  case 16:
+                     vrend_collapse_data_r16g16b16x16(send_size, data);
+                     break;
+                  default:
+                     assert(0 && "unsupported collaps four channel to three channel");
+                  }
+               }
+
+            } else if (vrend_format_is_bgra(res->base.format)) {
+               VREND_DEBUG(dbg_bgra, ctx, "manually swizzling bgra->rgba on upload since gles+bgra\n");
+               vrend_swizzle_data_bgra(send_size, data);
+            }
          }
 
          /* mipmaps are usually passed in one iov, and we need to keep the offset
@@ -9152,10 +9675,10 @@ static int vrend_transfer_send_getteximage(struct vrend_resource *res,
                                            const struct vrend_transfer_info *info)
 {
    GLenum format, type;
-   uint32_t tex_size;
    char *data;
    int elsize = util_format_get_blocksize(res->base.format);
    int compressed = util_format_is_compressed(res->base.format);
+   uint32_t iov_size = vrend_get_iovec_size(iov, num_iovs);
    GLenum target;
    uint32_t send_offset = 0;
    format = tex_conv_table[res->base.format].glformat;
@@ -9164,8 +9687,14 @@ static int vrend_transfer_send_getteximage(struct vrend_resource *res,
    if (compressed)
       format = tex_conv_table[res->base.format].internalformat;
 
-   tex_size = util_format_get_nblocks(res->base.format, u_minify(res->base.width0, info->level), u_minify(res->base.height0, info->level)) *
-              util_format_get_blocksize(res->base.format) * vrend_get_texture_depth(res, info->level);
+   uint64_t tex_size = util_format_get_nblocks(res->base.format,
+                                               u_minify(res->base.width0, info->level),
+                                               u_minify(res->base.height0, info->level));
+   tex_size *= util_format_get_blocksize(res->base.format);
+   tex_size *= vrend_get_texture_depth(res, info->level);
+
+   if (tex_size > iov_size)
+      return EINVAL;
 
    if (info->box->z && res->target != GL_TEXTURE_CUBE_MAP) {
       send_offset = util_format_get_nblocks(res->base.format, u_minify(res->base.width0, info->level), u_minify(res->base.height0, info->level)) * util_format_get_blocksize(res->base.format) * info->box->z;
@@ -9191,7 +9720,7 @@ static int vrend_transfer_send_getteximage(struct vrend_resource *res,
       break;
    }
 
-   glBindTexture(res->target, res->id);
+   glBindTexture(res->target, res->gl_id);
    if (res->target == GL_TEXTURE_CUBE_MAP) {
       target = GL_TEXTURE_CUBE_MAP_POSITIVE_X + info->box->z;
    } else
@@ -9231,6 +9760,24 @@ static void do_readpixels(struct vrend_resource *res,
                           GLenum format, GLenum type,
                           GLsizei bufSize, void *data)
 {
+   /* Instrumentation to observe scanout readback health. When
+    * VREND_DUMP_PRESENT is set, log a hash of the first readback. Also
+    * emit a one-time entry log to confirm the path is actually called.
+    */
+   static bool dump_present = false;
+   static bool dump_done = false;
+   static bool logged_entry = false;
+   if (!dump_done) {
+      const char *env = getenv("VREND_DUMP_PRESENT");
+      dump_present = env && *env && strcmp(env, "0") != 0;
+      dump_done = true;
+   }
+   if (!logged_entry) {
+      virgl_info("present-readback entry: w=%d h=%d fmt=0x%x type=0x%x bufSize=%d\n",
+                 width, height, format, type, bufSize);
+      logged_entry = true;
+   }
+
    GLuint fb_id;
 
    glGenFramebuffers(1, &fb_id);
@@ -9255,13 +9802,13 @@ static void do_readpixels(struct vrend_resource *res,
           type != GL_INT && type != GL_FLOAT) {
          glGetIntegerv(GL_IMPLEMENTATION_COLOR_READ_TYPE, &imp);
          if (imp != (GLint)type) {
-            vrend_printf( "GL_IMPLEMENTATION_COLOR_READ_TYPE is not expected native type 0x%x != imp 0x%x\n", type, imp);
+            virgl_warn("GL_IMPLEMENTATION_COLOR_READ_TYPE is not expected native type 0x%x != imp 0x%x\n", type, imp);
          }
       }
       if (format != GL_RGBA && format != GL_RGBA_INTEGER) {
          glGetIntegerv(GL_IMPLEMENTATION_COLOR_READ_FORMAT, &imp);
          if (imp != (GLint)format) {
-            vrend_printf( "GL_IMPLEMENTATION_COLOR_READ_FORMAT is not expected native format 0x%x != imp 0x%x\n", format, imp);
+            virgl_warn("GL_IMPLEMENTATION_COLOR_READ_FORMAT is not expected native format 0x%x != imp 0x%x\n", format, imp);
          }
       }
    }
@@ -9280,6 +9827,21 @@ static void do_readpixels(struct vrend_resource *res,
    else
       glReadPixels(x, y, width, height, format, type, data);
 
+   if (dump_present) {
+      /* Simple rolling hash (not cryptographic) to detect non-zero content. */
+      uint64_t hash = 1469598103934665603ull; /* FNV-1a offset */
+      size_t n = bufSize < 65536 ? bufSize : 65536; /* cap for speed */
+      const uint8_t *p = (const uint8_t *)data;
+      for (size_t i = 0; i < n; i++) {
+         hash ^= p[i];
+         hash *= 1099511628211ull; /* FNV-1a prime */
+      }
+      virgl_info("present-readback hash: w=%d h=%d fmt=0x%x type=0x%x hash=0x%016" PRIx64 "\n",
+                 width, height, format, type, hash);
+      /* Only log first hit to avoid spam. */
+      dump_present = false;
+   }
+
    glDeleteFramebuffers(1, &fb_id);
 }
 
@@ -9294,18 +9856,38 @@ static int vrend_transfer_send_readpixels(struct vrend_context *ctx,
    bool actually_invert, separate_invert = false;
    GLenum format, type;
    GLint y1;
-   uint32_t send_size = 0;
+   uint64_t send_size = 0;
    uint32_t h = u_minify(res->base.height0, info->level);
    int elsize = util_format_get_blocksize(res->base.format);
    float depth_scale;
    int row_stride = info->stride / elsize;
    GLint old_fbo;
 
-   vrend_use_program(ctx->sub, 0);
+   vrend_use_program(NULL);
 
    enum virgl_formats fmt = res->base.format;
 
    format = tex_conv_table[fmt].glformat;
+   if (vrend_state.use_gles &&
+       res->gbm_bo) {
+      switch (fmt) {
+      case VIRGL_FORMAT_B8G8R8X8_UNORM:
+         format = tex_conv_table[VIRGL_FORMAT_B8G8R8A8_UNORM].glformat;
+         break;
+      case VIRGL_FORMAT_B8G8R8X8_SRGB:
+         format = tex_conv_table[VIRGL_FORMAT_B8G8R8A8_SRGB].glformat;
+         break;
+      case VIRGL_FORMAT_R8G8B8X8_UNORM:
+         format = tex_conv_table[VIRGL_FORMAT_R8G8B8A8_UNORM].glformat;
+         break;
+      case VIRGL_FORMAT_R8G8B8X8_SRGB:
+         format = tex_conv_table[VIRGL_FORMAT_R8G8B8A8_SRGB].glformat;
+         break;
+      default:
+         break;
+      }
+   }
+
    type = tex_conv_table[fmt].gltype;
    /* if we are asked to invert and reading from a front then don't */
 
@@ -9325,10 +9907,20 @@ static int vrend_transfer_send_readpixels(struct vrend_context *ctx,
       need_temp = true;
 
    if (need_temp) {
-      send_size = util_format_get_nblocks(res->base.format, info->box->width, info->box->height) * info->box->depth * util_format_get_blocksize(res->base.format);
+      send_size = util_format_get_nblocks(res->base.format, info->box->width, info->box->height);
+      send_size *= info->box->depth;
+      send_size *= util_format_get_blocksize(res->base.format);
+
+      /* glReadnPixels only supports a buffer size of GLsizei = uint32_t, anything larger
+       * is bogous */
+      if (send_size > UINT_MAX) {
+         virgl_error("Readback size out of range %"PRIu64"\n", send_size);
+         return EINVAL;
+      }
+
       data = malloc(send_size);
       if (!data) {
-         vrend_printf("malloc failed %d\n", send_size);
+         virgl_error("Memory allocation failed for %"PRIu64"\n", send_size);
          return ENOMEM;
       }
    } else {
@@ -9423,7 +10015,7 @@ static int vrend_transfer_send_readonly(struct vrend_resource *res,
                                         UNUSED const struct vrend_transfer_info *info)
 {
    bool same_iov = true;
-   uint i;
+   unsigned i;
 
    if (res->num_iovs == (uint32_t)num_iovs) {
       for (i = 0; i < res->num_iovs; i++) {
@@ -9450,12 +10042,12 @@ static int vrend_transfer_send_readonly(struct vrend_resource *res,
 }
 
 static int vrend_renderer_transfer_send_iov(struct vrend_context *ctx,
-					    struct vrend_resource *res,
+                                            struct vrend_resource *res,
                                             const struct iovec *iov, int num_iovs,
                                             const struct vrend_transfer_info *info)
 {
-   if (is_only_bit(res->storage_bits, VREND_STORAGE_GUEST_MEMORY) ||
-       (has_bit(res->storage_bits, VREND_STORAGE_HOST_SYSTEM_MEMORY) && res->iov)) {
+   if ((is_only_bit(res->storage_bits, VREND_STORAGE_GUEST_MEMORY) ||
+       has_bit(res->storage_bits, VREND_STORAGE_HOST_SYSTEM_MEMORY)) && res->iov) {
       return vrend_copy_iovec(res->iov, res->num_iovs, info->box->x,
                               iov, num_iovs, info->offset,
                               info->box->width, res->ptr);
@@ -9469,15 +10061,12 @@ static int vrend_renderer_transfer_send_iov(struct vrend_context *ctx,
    }
 
    if (has_bit(res->storage_bits, VREND_STORAGE_GL_BUFFER)) {
-      uint32_t send_size = info->box->width * util_format_get_blocksize(res->base.format);
-      void *data;
-
-      glBindBufferARB(res->target, res->id);
-      data = glMapBufferRange(res->target, info->box->x, info->box->width, GL_MAP_READ_BIT);
+      glBindBufferARB(res->target, res->gl_id);
+      void *data = glMapBufferRange(res->target, info->box->x, info->box->width, GL_MAP_READ_BIT);
       if (!data)
-         vrend_printf("unable to open buffer for reading %d\n", res->target);
+         virgl_error("Unable to open buffer for reading %d\n", res->target);
       else
-         vrend_write_to_iovec(iov, num_iovs, info->offset, data, send_size);
+         vrend_write_to_iovec(iov, num_iovs, info->offset, data, info->box->width);
       glUnmapBuffer(res->target);
       glBindBufferARB(res->target, 0);
    } else {
@@ -9502,6 +10091,12 @@ static int vrend_renderer_transfer_send_iov(struct vrend_context *ctx,
    return 0;
 }
 
+/* Forward declaration for MSAA staging path below. */
+static void vrend_renderer_blit_int(struct vrend_context *ctx,
+                                    struct vrend_resource *src_res,
+                                    struct vrend_resource *dst_res,
+                                    const struct pipe_blit_info *info);
+
 static int vrend_renderer_transfer_internal(struct vrend_context *ctx,
                                             struct vrend_resource *res,
                                             const struct vrend_transfer_info *info,
@@ -9525,21 +10120,24 @@ static int vrend_renderer_transfer_internal(struct vrend_context *ctx,
       num_iovs = res->num_iovs;
    }
 
-#ifdef ENABLE_MINIGBM_ALLOCATION
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
    if (res->gbm_bo && (transfer_mode == VIRGL_TRANSFER_TO_HOST ||
                        !has_bit(res->storage_bits, VREND_STORAGE_EGL_IMAGE))) {
-      assert(!info->synchronized);
-      return virgl_gbm_transfer(res->gbm_bo, transfer_mode, iov, num_iovs, info);
+      const bool success = virgl_gbm_transfer(res->gbm_bo, transfer_mode, iov, num_iovs, info) == 0;
+      if (success)
+         return 0;
+      else
+         virgl_warn("GBM upload failed, try GL code path\n");
    }
 #endif
 
-   if (!check_transfer_bounds(res, info)) {
-      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_TRANSFER_IOV_BOUNDS, res->id);
+   if (!resource_contains_box(res, info->box, info->level)) {
+      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_TRANSFER_IOV_BOUNDS, res->gl_id);
       return EINVAL;
    }
 
    if (!check_iov_bounds(res, info, iov, num_iovs)) {
-      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_TRANSFER_IOV_BOUNDS, res->id);
+      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_TRANSFER_IOV_BOUNDS, res->gl_id);
       return EINVAL;
    }
 
@@ -9605,7 +10203,7 @@ int vrend_transfer_inline_write(struct vrend_context *ctx,
       return EINVAL;
    }
 
-   if (!check_transfer_bounds(res, info)) {
+   if (!resource_contains_box(res, info->box, info->level)) {
       vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_CMD_BUFFER, dst_handle);
       return EINVAL;
    }
@@ -9615,7 +10213,7 @@ int vrend_transfer_inline_write(struct vrend_context *ctx,
       return EINVAL;
    }
 
-#ifdef ENABLE_MINIGBM_ALLOCATION
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
    if (res->gbm_bo) {
       assert(!info->synchronized);
       return virgl_gbm_transfer(res->gbm_bo,
@@ -9629,33 +10227,141 @@ int vrend_transfer_inline_write(struct vrend_context *ctx,
    return vrend_renderer_transfer_write_iov(ctx, res, info->iovec, info->iovec_cnt, info);
 
 }
-
-int vrend_renderer_copy_transfer3d(struct vrend_context *ctx,
-                                   uint32_t dst_handle,
-                                   uint32_t src_handle,
-                                   const struct vrend_transfer_info *info)
+static int vrend_renderer_copy_transfer3d_msaa(struct vrend_context *ctx,
+                                               struct vrend_resource *dst_res,
+                                               struct vrend_resource *src_res,
+                                               const struct vrend_transfer_info *info)
 {
-   struct vrend_resource *src_res, *dst_res;
+   /* Multisample textures reject TexSubImage uploads; stage into single-sample
+    * texture and resolve via blit to avoid GL_INVALID_OPERATION on macOS core.
+    */
+   if (dst_res->target != GL_TEXTURE_2D_MULTISAMPLE &&
+       dst_res->target != GL_TEXTURE_2D_MULTISAMPLE_ARRAY)
+      return EINVAL;
 
-   src_res = vrend_renderer_ctx_res_lookup(ctx, src_handle);
-   dst_res = vrend_renderer_ctx_res_lookup(ctx, dst_handle);
+   const GLenum staging_target = (dst_res->target == GL_TEXTURE_2D_MULTISAMPLE_ARRAY) ?
+                                 GL_TEXTURE_2D_ARRAY : GL_TEXTURE_2D;
 
-   if (!src_res) {
-      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, src_handle);
-      return EINVAL;
+   virgl_warn("copy_transfer3d msaa staging: dst target=%u samples=%u level=%u box=[%d,%d,%d %dx%dx%d]\n",
+              dst_res->target, dst_res->base.nr_samples, info->level,
+              info->box->x, info->box->y, info->box->z,
+              info->box->width, info->box->height, info->box->depth);
+
+   struct vrend_resource staging = *dst_res;
+   staging.target = staging_target;
+   staging.base.nr_samples = 1;
+#ifdef PIPE_TEXTURE_2D_MULTISAMPLE_ARRAY
+   if (dst_res->base.target == PIPE_TEXTURE_2D_MULTISAMPLE_ARRAY)
+      staging.base.target = PIPE_TEXTURE_2D_ARRAY;
+   else
+#endif
+      staging.base.target = PIPE_TEXTURE_2D;
+   staging.storage_bits = VREND_STORAGE_GL_TEXTURE;
+   staging.gbm_bo = NULL;
+   staging.egl_image = 0;
+   staging.iov = NULL;
+   staging.num_iovs = 0;
+
+   glGenTextures(1, &staging.gl_id);
+   glBindTexture(staging_target, staging.gl_id);
+   glTexParameteri(staging_target, GL_TEXTURE_MIN_FILTER, GL_NEAREST);
+   glTexParameteri(staging_target, GL_TEXTURE_MAG_FILTER, GL_NEAREST);
+
+   const GLint internalformat = tex_conv_table[staging.base.format].internalformat;
+   const GLsizei level_w = u_minify(staging.base.width0, info->level);
+   const GLsizei level_h = u_minify(staging.base.height0, info->level);
+   const GLsizei level_d = (staging_target == GL_TEXTURE_2D_ARRAY) ? staging.base.array_size : 1;
+
+   if (util_format_is_compressed(staging.base.format)) {
+      const GLsizei comp_size = util_format_get_2d_size(staging.base.format,
+                                                        util_format_get_stride(staging.base.format, level_w),
+                                                        level_h);
+      if (staging_target == GL_TEXTURE_2D_ARRAY) {
+         glCompressedTexImage3D(staging_target, info->level, internalformat,
+                                level_w, level_h, level_d, 0,
+                                comp_size * level_d, NULL);
+      } else {
+         glCompressedTexImage2D(staging_target, info->level, internalformat,
+                                level_w, level_h, 0, comp_size, NULL);
+      }
+   } else {
+      if (staging_target == GL_TEXTURE_2D_ARRAY) {
+         glTexImage3D(staging_target, info->level, internalformat,
+                      level_w, level_h, level_d, 0,
+                      tex_conv_table[staging.base.format].glformat,
+                      tex_conv_table[staging.base.format].gltype,
+                      NULL);
+      } else {
+         glTexImage2D(staging_target, info->level, internalformat,
+                      level_w, level_h, 0,
+                      tex_conv_table[staging.base.format].glformat,
+                      tex_conv_table[staging.base.format].gltype,
+                      NULL);
+      }
    }
 
-   if (!dst_res) {
-      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, dst_handle);
-      return EINVAL;
+   int ret = vrend_renderer_transfer_write_iov(ctx, &staging, src_res->iov,
+                                               src_res->num_iovs, info);
+   if (ret) {
+      glDeleteTextures(1, &staging.gl_id);
+      return ret;
    }
 
-   if (!src_res->iov) {
-      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, dst_handle);
-      return EINVAL;
+   struct pipe_blit_info blit = { 0 };
+   blit.src.resource = &staging.base;
+   blit.src.level = info->level;
+   blit.src.box.x = info->box->x;
+   blit.src.box.y = info->box->y;
+   blit.src.box.z = info->box->z;
+   blit.src.box.width = info->box->width;
+   blit.src.box.height = info->box->height;
+   blit.src.box.depth = info->box->depth;
+   blit.src.format = staging.base.format;
+
+   blit.dst.resource = &dst_res->base;
+   blit.dst.level = info->level;
+   blit.dst.box = blit.src.box;
+   blit.dst.format = dst_res->base.format;
+
+   blit.mask = PIPE_MASK_RGBA;
+   if (vrend_format_is_ds(dst_res->base.format)) {
+      blit.mask = PIPE_MASK_Z;
+      if (util_format_has_stencil(util_format_description(dst_res->base.format)))
+         blit.mask |= PIPE_MASK_S;
    }
+   blit.filter = PIPE_TEX_FILTER_NEAREST;
+   blit.scissor_enable = false;
+   blit.render_condition_enable = false;
+   blit.alpha_blend = false;
 
-   if (!check_transfer_bounds(dst_res, info)) {
+   /* Force shader-based blit to MSAA target; GL forbids single->multi FBO blits. */
+   vrend_renderer_blit_int(ctx, &staging, dst_res, &blit);
+
+   virgl_warn("copy_transfer3d msaa staging: shader blit completed\n");
+
+   glDeleteTextures(1, &staging.gl_id);
+   return 0;
+}
+
+
+int vrend_renderer_copy_transfer3d(struct vrend_context *ctx,
+                                   uint32_t dst_handle,
+
+                                   struct vrend_resource *dst_res,
+                                   struct vrend_resource *src_res,
+                                   const struct vrend_transfer_info *info)
+{
+   static int copy_log_budget = 8;
+   if (copy_log_budget > 0) {
+      virgl_warn("copy_transfer3d: dst target=%u base.target=%u samples=%u format=%s level=%u box=[%d,%d,%d %dx%dx%d]\n",
+                 dst_res->target, dst_res->base.target, dst_res->base.nr_samples,
+                 util_format_name(dst_res->base.format), info->level,
+                 info->box->x, info->box->y, info->box->z,
+                 info->box->width, info->box->height, info->box->depth);
+      copy_log_budget--;
+   }
+
+   if (!resource_contains_box(dst_res, info->box, info->level)) {
       vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_CMD_BUFFER, dst_handle);
       return EINVAL;
    }
@@ -9665,8 +10371,8 @@ int vrend_renderer_copy_transfer3d(struct vrend_context *ctx,
       return EINVAL;
    }
 
-#ifdef ENABLE_MINIGBM_ALLOCATION
-   if (dst_res->gbm_bo) {
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
+   if (dst_res->gbm_bo && !TRANSFER_NO_GBM_MAPPING(info)) {
       bool use_gbm = true;
 
       /* The guest uses copy transfers against busy resources to avoid
@@ -9689,46 +10395,39 @@ int vrend_renderer_copy_transfer3d(struct vrend_context *ctx,
       }
 
       if (use_gbm) {
-         return virgl_gbm_transfer(dst_res->gbm_bo,
-                                   VIRGL_TRANSFER_TO_HOST,
-                                   src_res->iov,
-                                   src_res->num_iovs,
-                                   info);
+         bool success = virgl_gbm_transfer(dst_res->gbm_bo,
+                                           VIRGL_TRANSFER_TO_HOST,
+                                           src_res->iov,
+                                           src_res->num_iovs,
+                                           info) == 0;
+         if (success)
+            return 0;
+         virgl_warn("GBM copy transfer failed, try GL\n");
       }
    }
 #endif
 
+  if (dst_res->target == GL_TEXTURE_2D_MULTISAMPLE ||
+      dst_res->target == GL_TEXTURE_2D_MULTISAMPLE_ARRAY) {
+     int ret = vrend_renderer_copy_transfer3d_msaa(ctx, dst_res, src_res, info);
+     if (!ret)
+        return 0;
+     virgl_warn("copy_transfer3d MSAA staging fallback failed (%d), trying direct upload\n", ret);
+  }
+
   return vrend_renderer_transfer_write_iov(ctx, dst_res, src_res->iov,
                                            src_res->num_iovs, info);
 }
 
 int vrend_renderer_copy_transfer3d_from_host(struct vrend_context *ctx,
-                                   uint32_t dst_handle,
-                                   uint32_t src_handle,
-                                   const struct vrend_transfer_info *info)
+                                             uint32_t dst_handle,
+                                             uint32_t src_handle,
+                                             struct vrend_resource *dst_res,
+                                             struct vrend_resource *src_res,
+                                             const struct vrend_transfer_info *info)
 {
-   struct vrend_resource *src_res, *dst_res;
-
-   src_res = vrend_renderer_ctx_res_lookup(ctx, src_handle);
-   dst_res = vrend_renderer_ctx_res_lookup(ctx, dst_handle);
-
-   if (!src_res) {
-      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, src_handle);
-      return EINVAL;
-   }
-
-   if (!dst_res) {
-      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, dst_handle);
-      return EINVAL;
-   }
-
-   if (!dst_res->iov) {
-      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, dst_handle);
-      return EINVAL;
-   }
-
-   if (!check_transfer_bounds(src_res, info)) {
-      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_CMD_BUFFER, dst_handle);
+   if (!resource_contains_box(src_res, info->box, info->level)) {
+      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_RESOURCE_OUT_OF_RANGE, src_handle);
       return EINVAL;
    }
 
@@ -9737,8 +10436,8 @@ int vrend_renderer_copy_transfer3d_from_host(struct vrend_context *ctx,
       return EINVAL;
    }
 
-#ifdef ENABLE_MINIGBM_ALLOCATION
-   if (src_res->gbm_bo) {
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
+   if (src_res->gbm_bo && !TRANSFER_NO_GBM_MAPPING(info)) {
       bool use_gbm = true;
 
       /* The guest uses copy transfers against busy resources to avoid
@@ -9766,11 +10465,14 @@ int vrend_renderer_copy_transfer3d_from_host(struct vrend_context *ctx,
       }
 
       if (use_gbm) {
-         return virgl_gbm_transfer(src_res->gbm_bo,
-                                   VIRGL_TRANSFER_FROM_HOST,
-                                   dst_res->iov,
-                                   dst_res->num_iovs,
-                                   info);
+         bool success = virgl_gbm_transfer(src_res->gbm_bo,
+                                           VIRGL_TRANSFER_FROM_HOST,
+                                           dst_res->iov,
+                                           dst_res->num_iovs,
+                                           info) == 0;
+         if (success)
+            return 0;
+         virgl_warn("GBM read failed, try GL\n");
       }
    }
 #endif
@@ -9803,21 +10505,17 @@ void vrend_set_scissor_state(struct vrend_context *ctx,
                              uint32_t num_scissor,
                              struct pipe_scissor_state *ss)
 {
-   if (start_slot < PIPE_MAX_VIEWPORTS &&
-       start_slot + num_scissor <= PIPE_MAX_VIEWPORTS) {
-      for (uint i = 0; i < num_scissor; i++) {
-         uint idx = start_slot + i;
-         ctx->sub->ss[idx] = ss[i];
-         ctx->sub->scissor_state_dirty |= (1 << idx);
-      }
-   } else
-      vrend_report_buffer_error(ctx, 0);
+    for (unsigned i = 0; i < num_scissor; i++) {
+      unsigned idx = start_slot + i;
+      ctx->sub->ss[idx] = ss[i];
+      ctx->sub->scissor_state_dirty |= (1 << idx);
+    }
 }
 
 void vrend_set_polygon_stipple(struct vrend_context *ctx,
                                struct pipe_poly_stipple *ps)
 {
-   if (vrend_state.use_core_profile) {
+   if (vrend_shader_use_core(ctx)) {
 
       /* std140 aligns array elements at 16 byte */
       for (int i = 0; i < VREND_POLYGON_STIPPLE_SIZE ; ++i)
@@ -9830,7 +10528,7 @@ void vrend_set_polygon_stipple(struct vrend_context *ctx,
 
 void vrend_set_clip_state(struct vrend_context *ctx, struct pipe_clip_state *ucp)
 {
-   if (vrend_state.use_core_profile) {
+   if (vrend_shader_use_core(ctx)) {
       ctx->sub->ucp_state = *ucp;
 
       ctx->sub->sysvalue_data_cookie++;
@@ -9882,15 +10580,15 @@ void vrend_set_tess_state(UNUSED struct vrend_context *ctx, const float tess_fac
 
 static void vrend_hw_emit_streamout_targets(UNUSED struct vrend_context *ctx, struct vrend_streamout_object *so_obj)
 {
-   uint i;
+   unsigned i;
 
    for (i = 0; i < so_obj->num_targets; i++) {
       if (!so_obj->so_targets[i])
          glBindBufferBase(GL_TRANSFORM_FEEDBACK_BUFFER, i, 0);
       else if (so_obj->so_targets[i]->buffer_offset || so_obj->so_targets[i]->buffer_size < so_obj->so_targets[i]->buffer->base.width0)
-         glBindBufferRange(GL_TRANSFORM_FEEDBACK_BUFFER, i, so_obj->so_targets[i]->buffer->id, so_obj->so_targets[i]->buffer_offset, so_obj->so_targets[i]->buffer_size);
+         glBindBufferRange(GL_TRANSFORM_FEEDBACK_BUFFER, i, so_obj->so_targets[i]->buffer->gl_id, so_obj->so_targets[i]->buffer_offset, so_obj->so_targets[i]->buffer_size);
       else
-         glBindBufferBase(GL_TRANSFORM_FEEDBACK_BUFFER, i, so_obj->so_targets[i]->buffer->id);
+         glBindBufferBase(GL_TRANSFORM_FEEDBACK_BUFFER, i, so_obj->so_targets[i]->buffer->gl_id);
    }
 }
 
@@ -9900,29 +10598,23 @@ void vrend_set_streamout_targets(struct vrend_context *ctx,
                                  uint32_t *handles)
 {
    struct vrend_so_target *target;
-   uint i;
+   unsigned i;
 
    if (!has_feature(feat_transform_feedback))
       return;
 
    if (num_targets) {
-      bool found = false;
-      struct vrend_streamout_object *obj;
-      LIST_FOR_EACH_ENTRY(obj, &ctx->sub->streamout_list, head) {
+      list_for_each_entry(struct vrend_streamout_object, obj, &ctx->sub->streamout_list, head) {
          if (obj->num_targets == num_targets) {
             if (!memcmp(handles, obj->handles, num_targets * 4)) {
-               found = true;
-               break;
+               ctx->sub->current_so = obj;
+               glBindTransformFeedback(GL_TRANSFORM_FEEDBACK, obj->id);
+               return;
             }
          }
       }
-      if (found) {
-         ctx->sub->current_so = obj;
-         glBindTransformFeedback(GL_TRANSFORM_FEEDBACK, obj->id);
-         return;
-      }
 
-      obj = CALLOC_STRUCT(vrend_streamout_object);
+      struct vrend_streamout_object *obj = CALLOC_STRUCT(vrend_streamout_object);
       if (has_feature(feat_transform_feedback2)) {
          glGenTransformFeedbacks(1, &obj->id);
          glBindTransformFeedback(GL_TRANSFORM_FEEDBACK, obj->id);
@@ -9934,6 +10626,9 @@ void vrend_set_streamout_targets(struct vrend_context *ctx,
             continue;
          target = vrend_object_lookup(ctx->sub->object_hash, handles[i], VIRGL_OBJECT_STREAMOUT_TARGET);
          if (!target) {
+            /* Remove the reference to the already bound targets because we will destroy the obj */
+            for (unsigned j = 0; j < i; ++j)
+               vrend_so_target_reference(&obj->so_targets[j], NULL);
             vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_HANDLE, handles[i]);
             free(obj);
             return;
@@ -9957,8 +10652,8 @@ static void vrend_resource_buffer_copy(UNUSED struct vrend_context *ctx,
                                        uint32_t dstx, uint32_t srcx,
                                        uint32_t width)
 {
-   glBindBuffer(GL_COPY_READ_BUFFER, src_res->id);
-   glBindBuffer(GL_COPY_WRITE_BUFFER, dst_res->id);
+   glBindBuffer(GL_COPY_READ_BUFFER, src_res->gl_id);
+   glBindBuffer(GL_COPY_WRITE_BUFFER, dst_res->gl_id);
 
    glCopyBufferSubData(GL_COPY_READ_BUFFER, GL_COPY_WRITE_BUFFER, srcx, dstx, width);
    glBindBuffer(GL_COPY_READ_BUFFER, 0);
@@ -9986,7 +10681,7 @@ static void vrend_resource_copy_fallback(struct vrend_resource *src_res,
       cube_slice = 6;
 
    if (src_res->base.format != dst_res->base.format) {
-      vrend_printf( "copy fallback failed due to mismatched formats %d %d\n", src_res->base.format, dst_res->base.format);
+      virgl_error("Copy fallback failed due to mismatched formats %d %d\n", src_res->base.format, dst_res->base.format);
       return;
    }
 
@@ -10067,7 +10762,7 @@ static void vrend_resource_copy_fallback(struct vrend_resource *src_res,
          glPixelStorei(GL_PACK_ALIGNMENT, 8);
          break;
       }
-      glBindTexture(src_res->target, src_res->id);
+      glBindTexture(src_res->target, src_res->gl_id);
       slice_offset = 0;
       read_chunk_size = (src_res->target == GL_TEXTURE_CUBE_MAP) ? slice_size : total_size;
       for (i = 0; i < cube_slice; i++) {
@@ -10107,10 +10802,18 @@ static void vrend_resource_copy_fallback(struct vrend_resource *src_res,
       break;
    }
 
-   glBindTexture(dst_res->target, dst_res->id);
+   glBindTexture(dst_res->target, dst_res->gl_id);
    slice_offset = src_box->z * slice_size;
    cube_slice = (src_res->target == GL_TEXTURE_CUBE_MAP) ? src_box->z + src_box->depth : cube_slice;
    i = (src_res->target == GL_TEXTURE_CUBE_MAP) ? src_box->z : 0;
+   /* Allow depth==0 (treated as 1 slice) and avoid width/height product overflow. */
+   uint32_t slices_to_copy = src_box->depth ? src_box->depth : 1;
+   uint64_t required_size = (uint64_t)slice_offset + (uint64_t)slices_to_copy * slice_size;
+   if (required_size > total_size) {
+      virgl_error("Offset out of bound: %d\n", src_box->z);
+      goto cleanup;
+   }
+
    for (; i < cube_slice; i++) {
       GLenum ctarget = dst_res->target == GL_TEXTURE_CUBE_MAP ?
                           (GLenum)(GL_TEXTURE_CUBE_MAP_POSITIVE_X + i) : dst_res->target;
@@ -10138,9 +10841,10 @@ static void vrend_resource_copy_fallback(struct vrend_resource *src_res,
       slice_offset += slice_size;
    }
 
+cleanup:
    glPixelStorei(GL_UNPACK_ALIGNMENT, 4);
    free(tptr);
-   glBindTexture(GL_TEXTURE_2D, 0);
+   glBindTexture(dst_res->target, 0);
 }
 
 static inline void
@@ -10148,9 +10852,9 @@ vrend_copy_sub_image(struct vrend_resource* src_res, struct vrend_resource * dst
                      uint32_t src_level, const struct pipe_box *src_box,
                      uint32_t dst_level, uint32_t dstx, uint32_t dsty, uint32_t dstz)
 {
-   glCopyImageSubData(src_res->id, src_res->target, src_level,
+   glCopyImageSubData(src_res->gl_id, src_res->target, src_level,
                       src_box->x, src_box->y, src_box->z,
-                      dst_res->id, dst_res->target, dst_level,
+                      dst_res->gl_id, dst_res->target, dst_level,
                       dstx, dsty, dstz,
                       src_box->width, src_box->height,src_box->depth);
 
@@ -10159,10 +10863,54 @@ vrend_copy_sub_image(struct vrend_resource* src_res, struct vrend_resource * dst
    //   "ERROR: GL_INVALID_VALUE in glCopyImageSubData(srcX or srcWidth exceeds image bounds)"
    if (has_bit(src_res->storage_bits, VREND_STORAGE_GBM_BUFFER) &&
        glGetError() != GL_NO_ERROR) {
-      vrend_printf("glCopyImageSubData maybe fail\n");
+      virgl_warn("glCopyImageSubData maybe fail\n");
    }
 }
 
+static bool resource_dest_contains_box(struct vrend_resource *src_res,
+                                       struct vrend_resource *dst_res,
+                                       uint32_t dst_level,
+                                       uint32_t dstx,
+                                       uint32_t dsty,
+                                       uint32_t dstz,
+                                       const struct pipe_box *src_box)
+{
+   enum pipe_format src_format;
+   enum pipe_format dst_format;
+   struct pipe_box dst_box = *src_box;
+   unsigned src_bw, dst_bw, src_bh, dst_bh;
+
+   src_format = src_res->base.format;
+   dst_format = dst_res->base.format;
+
+   /* init dst box */
+   dst_box.x = dstx;
+   dst_box.y = dsty;
+   dst_box.z = dstz;
+
+   src_bw = util_format_get_blockwidth(src_format);
+   src_bh = util_format_get_blockheight(src_format);
+   dst_bw = util_format_get_blockwidth(dst_format);
+   dst_bh = util_format_get_blockheight(dst_format);
+
+   /* Note: all box positions and sizes are in pixels */
+   if (src_bw > 1 && dst_bw == 1) {
+      /* Copy from compressed to uncompressed.
+       * Shrink dest box by the src block size.
+       */
+      dst_box.width /= src_bw;
+      dst_box.height /= src_bh;
+   }
+   else if (src_bw == 1 && dst_bw > 1) {
+      /* Copy from uncompressed to compressed.
+       * Expand dest box by the dest block size.
+       */
+      dst_box.width *= dst_bw;
+      dst_box.height *= dst_bh;
+   }
+
+   return resource_contains_box(dst_res, &dst_box, dst_level);
+}
 
 void vrend_renderer_resource_copy_region(struct vrend_context *ctx,
                                          uint32_t dst_handle, uint32_t dst_level,
@@ -10190,6 +10938,17 @@ void vrend_renderer_resource_copy_region(struct vrend_context *ctx,
       return;
    }
 
+   if (!resource_contains_box(src_res, src_box, src_level)) {
+      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_CMD_BUFFER, src_handle);
+      return;
+   }
+
+   if (!resource_dest_contains_box(src_res, dst_res, dst_level,
+                                   dstx, dsty, dstz, src_box)) {
+      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_RESOURCE_OUT_OF_RANGE, dst_handle);
+      return;
+   }
+
    VREND_DEBUG(dbg_copy_resource, ctx, "COPY_REGION: From %s ms:%d [%d, %d, %d]+[%d, %d, %d] lvl:%d "
                                    "To %s ms:%d [%d, %d, %d]\n",
                                    util_format_name(src_res->base.format), src_res->base.nr_samples,
@@ -10214,9 +10973,32 @@ void vrend_renderer_resource_copy_region(struct vrend_context *ctx,
    if (dst_res->egl_image)
       comp_flags ^= VREND_COPY_COMPAT_FLAG_ONE_IS_EGL_IMAGE;
 
-   if (has_feature(feat_copy_image) &&
-       format_is_copy_compatible(src_res->base.format,dst_res->base.format, comp_flags) &&
-       src_res->base.nr_samples == dst_res->base.nr_samples) {
+   bool allow_copy_image = has_feature(feat_copy_image) &&
+                           format_is_copy_compatible(src_res->base.format,
+                                                     dst_res->base.format,
+                                                     comp_flags) &&
+                           src_res->base.nr_samples == dst_res->base.nr_samples;
+
+   /* ANGLE/Metal on macOS returns GL_INVALID_ENUM for multisample copy_image
+    * targets when running in GLES mode. Prefer the blit fallback instead.
+    */
+   if (allow_copy_image &&
+       (src_res->target == GL_TEXTURE_2D_MULTISAMPLE ||
+        src_res->target == GL_TEXTURE_2D_MULTISAMPLE_ARRAY ||
+        dst_res->target == GL_TEXTURE_2D_MULTISAMPLE ||
+        dst_res->target == GL_TEXTURE_2D_MULTISAMPLE_ARRAY)) {
+      allow_copy_image = false;
+   }
+#ifdef __APPLE__
+   /* ANGLE GLES on macOS can still advertise copy_image but fail at runtime;
+    * force shader blit when running GLES on Apple to avoid GL_INVALID_ENUM.
+    */
+   if (allow_copy_image && vrend_state.use_gles) {
+      allow_copy_image = false;
+   }
+#endif
+
+   if (allow_copy_image) {
       VREND_DEBUG(dbg_copy_resource, ctx, "COPY_REGION: use glCopyImageSubData\n");
       vrend_copy_sub_image(src_res, dst_res, src_level, src_box,
                            dst_level, dstx, dsty, dstz);
@@ -10287,6 +11069,13 @@ void vrend_renderer_resource_copy_region(struct vrend_context *ctx,
       glEnable(GL_SCISSOR_TEST);
 }
 
+
+static inline bool texture_view_compatible(enum virgl_formats src, enum virgl_formats dst)
+{
+   return (tex_conv_table[src].view_class != view_class_unsupported) &&
+         (tex_conv_table[src].view_class == tex_conv_table[dst].view_class);
+}
+
 static GLuint vrend_make_view(struct vrend_resource *res, enum virgl_formats format)
 {
    GLuint view_id;
@@ -10295,11 +11084,14 @@ static GLuint vrend_make_view(struct vrend_resource *res, enum virgl_formats for
    GLenum view_ifmt = tex_conv_table[format].internalformat;
 
    if (tex_ifmt == view_ifmt)
-      return res->id;
+      return res->gl_id;
+
+   if (!texture_view_compatible(res->base.format, format))
+      return res->gl_id;
 
    /* If the format doesn't support TextureStorage it is not immutable, so no TextureView*/
    if (!has_bit(res->storage_bits, VREND_STORAGE_GL_IMMUTABLE))
-      return res->id;
+      return res->gl_id;
 
    assert(vrend_resource_supports_view(res, format));
 
@@ -10314,7 +11106,7 @@ static GLuint vrend_make_view(struct vrend_resource *res, enum virgl_formats for
    }
 
    glGenTextures(1, &view_id);
-   glTextureView(view_id, res->target, res->id, view_ifmt, 0, res->base.last_level + 1,
+   glTextureView(view_id, res->target, res->gl_id, view_ifmt, 0, res->base.last_level + 1,
                  0, res->base.array_size);
    return view_id;
 }
@@ -10645,8 +11437,8 @@ static void vrend_renderer_blit_int(struct vrend_context *ctx,
 {
    struct vrend_blit_info blit_info = {
       .b = *info,
-      .src_view = src_res->id,
-      .dst_view = dst_res->id,
+      .src_view = src_res->gl_id,
+      .dst_view = dst_res->gl_id,
       .swizzle =  {0, 1, 2, 3}
    };
 
@@ -10674,10 +11466,10 @@ static void vrend_renderer_blit_int(struct vrend_context *ctx,
       vrend_sync_make_current(ctx->sub->gl_context);
    }
 
-   if (blit_info.src_view != src_res->id)
+   if (blit_info.src_view != src_res->gl_id)
       glDeleteTextures(1, &blit_info.src_view);
 
-   if (blit_info.dst_view != dst_res->id)
+   if (blit_info.dst_view != dst_res->gl_id)
       glDeleteTextures(1, &blit_info.dst_view);
 }
 
@@ -10703,16 +11495,40 @@ void vrend_renderer_blit(struct vrend_context *ctx,
    if (ctx->in_error)
       return;
 
-   if (!info->src.format || info->src.format >= VIRGL_FORMAT_MAX) {
+   if (unlikely(info->src.format >= VIRGL_FORMAT_MAX ||
+                tex_conv_table[info->src.format].format == VIRGL_FORMAT_NONE)) {
       vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_FORMAT, info->src.format);
       return;
    }
 
-   if (!info->dst.format || info->dst.format >= VIRGL_FORMAT_MAX) {
+   if (unlikely(info->dst.format >= VIRGL_FORMAT_MAX ||
+                tex_conv_table[info->dst.format].format == VIRGL_FORMAT_NONE)) {
       vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_FORMAT, info->dst.format);
       return;
    }
 
+   if ((src_res->base.target == PIPE_TEXTURE_CUBE) && (info->src.box.depth + info->src.box.z  - 1) >= 6) {
+      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_CUBE_MAP_FACE_OUT_OF_RANGE,
+                                 info->src.box.depth + info->src.box.z - 1);
+      return;
+   }
+
+   if (dst_res->base.target == PIPE_TEXTURE_CUBE && info->dst.box.depth + info->dst.box.z - 1 >= 6) {
+      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_CUBE_MAP_FACE_OUT_OF_RANGE,
+                                 info->src.box.depth + info->src.box.z - 1);
+      return;
+   }
+
+   /* Check that we are actually blitting into the destination texture, and make sure that we
+    * don't end up in a long loop because we have an invalid dst.box.z.
+    */
+   int dst_depth = MAX2(u_minify(dst_res->base.depth0, info->dst.level), dst_res->base.array_size);
+   if (info->dst.box.depth > dst_depth || info->dst.box.z > dst_depth) {
+      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_BLIT_AREA_OUT_OF_RANGE,
+                                 info->dst.box.depth);
+      return;
+   }
+
    if (info->render_condition_enable == false)
       vrend_pause_render_condition(ctx, true);
 
@@ -10762,7 +11578,7 @@ void vrend_renderer_blit(struct vrend_context *ctx,
     * to resource_copy_region, in this case and if no render states etx need
     * to be applied, forward the call to glCopyImageSubData, otherwise do a
     * normal blit. */
-   if (has_feature(feat_copy_image) &&
+   bool allow_copy_image = has_feature(feat_copy_image) &&
        (!info->render_condition_enable || !ctx->sub->cond_render_gl_mode) &&
        format_is_copy_compatible(info->src.format,info->dst.format, comp_flags) &&
        eglimage_copy_compatible &&
@@ -10775,7 +11591,26 @@ void vrend_renderer_blit(struct vrend_context *ctx,
        info->dst.box.y + info->dst.box.height <= dst_height &&
        info->src.box.width == info->dst.box.width &&
        info->src.box.height == info->dst.box.height &&
-       info->src.box.depth == info->dst.box.depth) {
+       info->src.box.depth == info->dst.box.depth;
+
+   /* ANGLE/Metal GLES path returns GL_INVALID_ENUM for copy_image on MSAA
+    * targets; avoid copy_image there. Also prefer shader blit when running
+    * GLES on macOS even for non-MSAA to steer clear of driver quirks.
+    */
+   if (allow_copy_image &&
+       (src_res->target == GL_TEXTURE_2D_MULTISAMPLE ||
+        src_res->target == GL_TEXTURE_2D_MULTISAMPLE_ARRAY ||
+        dst_res->target == GL_TEXTURE_2D_MULTISAMPLE ||
+        dst_res->target == GL_TEXTURE_2D_MULTISAMPLE_ARRAY)) {
+      allow_copy_image = false;
+   }
+#ifdef __APPLE__
+   if (allow_copy_image && vrend_state.use_gles) {
+      allow_copy_image = false;
+   }
+#endif
+
+   if (allow_copy_image) {
       VREND_DEBUG(dbg_blit, ctx,  "  Use glCopyImageSubData\n");
       vrend_copy_sub_image(src_res, dst_res, info->src.level, &info->src.box,
                            info->dst.level, info->dst.box.x, info->dst.box.y,
@@ -10804,9 +11639,6 @@ int vrend_renderer_create_fence(struct vrend_context *ctx,
 {
    struct vrend_fence *fence;
 
-   if (!ctx)
-      return EINVAL;
-
    fence = malloc(sizeof(struct vrend_fence));
    if (!fence)
       return ENOMEM;
@@ -10833,12 +11665,22 @@ int vrend_renderer_create_fence(struct vrend_context *ctx,
       list_addtail(&fence->fences, &vrend_state.fence_wait_list);
       cnd_signal(&vrend_state.fence_cond);
       mtx_unlock(&vrend_state.fence_mutex);
-   } else
+   } else {
       list_addtail(&fence->fences, &vrend_state.fence_list);
+   }
+
+#ifdef HAVE_EPOXY_EGL_H
+   int fence_fd = -1;
+   if (vrend_renderer_export_ctx0_fence(fence_id, &fence_fd) == 0 &&
+       virgl_fence_set_fd(fence_id, fence_fd))
+      virgl_error("failed to export fence sync object\n");
+   if (fence_fd != -1)
+      close(fence_fd);
+#endif
    return 0;
 
  fail:
-   vrend_printf( "failed to create fence sync object\n");
+   virgl_error("Failed to create fence sync object\n");
    free(fence);
    return ENOMEM;
 }
@@ -10867,7 +11709,6 @@ static bool need_fence_retire_signal_locked(struct vrend_fence *fence,
 void vrend_renderer_check_fences(void)
 {
    struct list_head retired_fences;
-   struct vrend_fence *fence, *stor;
 
    assert(!vrend_state.use_async_fence_cb);
 
@@ -10876,7 +11717,7 @@ void vrend_renderer_check_fences(void)
    if (vrend_state.sync_thread) {
       flush_eventfd(vrend_state.eventfd);
       mtx_lock(&vrend_state.fence_mutex);
-      LIST_FOR_EACH_ENTRY_SAFE(fence, stor, &vrend_state.fence_list, fences) {
+      list_for_each_entry_safe(struct vrend_fence, fence, &vrend_state.fence_list, fences) {
          /* vrend_free_fences_for_context might have marked the fence invalid
           * by setting fence->ctx to NULL
           */
@@ -10896,7 +11737,7 @@ void vrend_renderer_check_fences(void)
    } else {
       vrend_renderer_force_ctx_0();
 
-      LIST_FOR_EACH_ENTRY_SAFE(fence, stor, &vrend_state.fence_list, fences) {
+      list_for_each_entry_safe(struct vrend_fence, fence, &vrend_state.fence_list, fences) {
          if (do_wait(fence, /* can_block */ false)) {
             list_del(&fence->fences);
             list_addtail(&fence->fences, &retired_fences);
@@ -10906,18 +11747,18 @@ void vrend_renderer_check_fences(void)
          }
       }
 
-      LIST_FOR_EACH_ENTRY_SAFE(fence, stor, &retired_fences, fences) {
+      list_for_each_entry_safe(struct vrend_fence, fence, &retired_fences, fences) {
          if (!need_fence_retire_signal_locked(fence, &retired_fences))
             free_fence_locked(fence);
       }
    }
 
-   if (LIST_IS_EMPTY(&retired_fences))
+   if (list_is_empty(&retired_fences))
       return;
 
    vrend_renderer_check_queries();
 
-   LIST_FOR_EACH_ENTRY_SAFE(fence, stor, &retired_fences, fences) {
+   list_for_each_entry_safe(struct vrend_fence, fence, &retired_fences, fences) {
       struct vrend_context *ctx = fence->ctx;
       ctx->fence_retire(fence->fence_id, ctx->fence_retire_data);
 
@@ -10981,10 +11822,14 @@ static bool vrend_check_query(struct vrend_query *query)
    state.query_state = VIRGL_QUERY_STATE_DONE;
 
    if (query->res->iov) {
-      vrend_write_to_iovec(query->res->iov, query->res->num_iovs, 0,
-            (const void *) &state, sizeof(state));
+      if (vrend_write_to_iovec(query->res->iov, query->res->num_iovs, 0,
+                               (const void *) &state, sizeof(state)) != sizeof(state))
+         virgl_error("Query state does not fit IOV size\n");
    } else {
-      *((struct virgl_host_query_state *) query->res->ptr) = state;
+      if (query->res->base.width0 >= sizeof(state) )
+         *((struct virgl_host_query_state *) query->res->ptr) = state;
+      else
+         virgl_error("Query state does not fit buffer size\n");
    }
 
    return true;
@@ -10993,12 +11838,10 @@ static bool vrend_check_query(struct vrend_query *query)
 static struct vrend_sub_context *vrend_renderer_find_sub_ctx(struct vrend_context *ctx,
                                                              int sub_ctx_id)
 {
-   struct vrend_sub_context *sub;
-
    if (ctx->sub && ctx->sub->sub_ctx_id == sub_ctx_id)
       return ctx->sub;
 
-   LIST_FOR_EACH_ENTRY(sub, &ctx->sub_ctxs, head) {
+   list_for_each_entry_safe(struct vrend_sub_context, sub, &ctx->sub_ctxs, head) {
       if (sub->sub_ctx_id == sub_ctx_id)
          return sub;
    }
@@ -11038,11 +11881,9 @@ static bool vrend_hw_switch_context_with_sub(struct vrend_context *ctx, int sub_
 
 static void vrend_renderer_check_queries(void)
 {
-   struct vrend_query *query, *stor;
-
-   LIST_FOR_EACH_ENTRY_SAFE(query, stor, &vrend_state.waiting_query_list, waiting_queries) {
+   list_for_each_entry_safe(struct vrend_query, query, &vrend_state.waiting_query_list, waiting_queries) {
       if (!vrend_hw_switch_context_with_sub(query->ctx, query->sub_ctx_id)) {
-         vrend_printf("failed to switch to context (%d) with sub (%d) for query %u\n",
+         virgl_warn("Failed to switch to context (%d) with sub (%d) for query %u\n",
                       query->ctx->ctx_id, query->sub_ctx_id, query->id);
       }
       else if (!vrend_check_query(query)) {
@@ -11053,7 +11894,7 @@ static void vrend_renderer_check_queries(void)
    }
 
    atomic_store(&vrend_state.has_waiting_queries,
-                !LIST_IS_EMPTY(&vrend_state.waiting_query_list));
+                !list_is_empty(&vrend_state.waiting_query_list));
 }
 
 bool vrend_hw_switch_context(struct vrend_context *ctx, bool now)
@@ -11214,7 +12055,7 @@ int vrend_create_query(struct vrend_context *ctx, uint32_t handle,
          err = EINVAL;
       break;
    default:
-      vrend_printf("unknown query object received %d\n", q->type);
+      virgl_warn("Unknown query object received %d\n", q->type);
       break;
    }
 
@@ -11226,8 +12067,10 @@ int vrend_create_query(struct vrend_context *ctx, uint32_t handle,
       }
    }
 
-   if (err)
+   if (err) {
+      vrend_resource_reference(&q->res, NULL);
       FREE(q);
+   }
 
    return err;
 }
@@ -11298,7 +12141,7 @@ int vrend_end_query(struct vrend_context *ctx, uint32_t handle)
    return 0;
 }
 
-void vrend_get_query_result(struct vrend_context *ctx, uint32_t handle,
+int vrend_get_query_result(struct vrend_context *ctx, uint32_t handle,
                             UNUSED uint32_t wait)
 {
    struct vrend_query *q;
@@ -11306,17 +12149,18 @@ void vrend_get_query_result(struct vrend_context *ctx, uint32_t handle,
 
    q = vrend_object_lookup(ctx->sub->object_hash, handle, VIRGL_OBJECT_QUERY);
    if (!q)
-      return;
+      return EINVAL;
 
    ret = vrend_check_query(q);
    if (ret) {
       list_delinit(&q->waiting_queries);
-   } else if (LIST_IS_EMPTY(&q->waiting_queries)) {
+   } else if (list_is_empty(&q->waiting_queries)) {
       list_addtail(&q->waiting_queries, &vrend_state.waiting_query_list);
    }
 
    atomic_store(&vrend_state.has_waiting_queries,
-                !LIST_IS_EMPTY(&vrend_state.waiting_query_list));
+                !list_is_empty(&vrend_state.waiting_query_list));
+   return 0;
 }
 
 #define COPY_QUERY_RESULT_TO_BUFFER(resid, offset, pvalue, size, multiplier) \
@@ -11331,7 +12175,7 @@ static inline void *buffer_offset(intptr_t i)
    return (void *)i;
 }
 
-void vrend_get_query_result_qbo(struct vrend_context *ctx, uint32_t handle,
+int vrend_get_query_result_qbo(struct vrend_context *ctx, uint32_t handle,
                                 uint32_t qbo_handle,
                                 uint32_t wait, uint32_t result_type, uint32_t offset,
                                 int32_t index)
@@ -11340,16 +12184,16 @@ void vrend_get_query_result_qbo(struct vrend_context *ctx, uint32_t handle,
   struct vrend_resource *res;
 
   if (!has_feature(feat_qbo))
-     return;
+     return EINVAL;
 
   q = vrend_object_lookup(ctx->sub->object_hash, handle, VIRGL_OBJECT_QUERY);
   if (!q)
-     return;
+     return EINVAL;
 
   res = vrend_renderer_ctx_res_lookup(ctx, qbo_handle);
-  if (!res) {
+  if (!res || !res->gl_id) {
      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, qbo_handle);
-     return;
+     return EINVAL;
   }
 
   VREND_DEBUG(dbg_query, ctx, "Get query result from Query:%d\n", q->id);
@@ -11362,7 +12206,7 @@ void vrend_get_query_result_qbo(struct vrend_context *ctx, uint32_t handle,
      qtype = wait ? GL_QUERY_RESULT : GL_QUERY_RESULT_NO_WAIT;
 
   if (!q->fake_samples_passed) {
-     glBindBuffer(GL_QUERY_BUFFER, res->id);
+     glBindBuffer(GL_QUERY_BUFFER, res->gl_id);
      switch ((enum pipe_query_value_type)result_type) {
      case PIPE_QUERY_TYPE_I32:
         glGetQueryObjectiv(q->id, qtype, buffer_offset(offset));
@@ -11414,6 +12258,8 @@ void vrend_get_query_result_qbo(struct vrend_context *ctx, uint32_t handle,
   }
 
   glBindBuffer(GL_QUERY_BUFFER, 0);
+
+  return 0;
 }
 
 static void vrend_pause_render_condition(struct vrend_context *ctx, bool pause)
@@ -11440,16 +12286,18 @@ static void vrend_pause_render_condition(struct vrend_context *ctx, bool pause)
 void vrend_render_condition(struct vrend_context *ctx,
                             uint32_t handle,
                             bool condition,
-                            uint mode)
+                            uint32_t mode)
 {
    struct vrend_query *q;
    GLenum glmode = 0;
 
    if (handle == 0) {
-      if (has_feature(feat_gl_conditional_render))
-         glEndConditionalRender();
-      else if (has_feature(feat_nv_conditional_render))
-         glEndConditionalRenderNV();
+      if (ctx->sub->cond_render_q_id != 0) {
+         if (has_feature(feat_gl_conditional_render))
+            glEndConditionalRender();
+         else if (has_feature(feat_nv_conditional_render))
+            glEndConditionalRenderNV();
+      }
       ctx->sub->cond_render_q_id = 0;
       ctx->sub->cond_render_gl_mode = 0;
       return;
@@ -11475,7 +12323,7 @@ void vrend_render_condition(struct vrend_context *ctx,
       glmode = condition ? GL_QUERY_BY_REGION_NO_WAIT_INVERTED : GL_QUERY_BY_REGION_NO_WAIT;
       break;
    default:
-      vrend_printf( "unhandled condition %x\n", mode);
+      virgl_warn("Unhandled condition %x\n", mode);
    }
 
    ctx->sub->cond_render_q_id = q->id;
@@ -11496,7 +12344,7 @@ int vrend_create_so_target(struct vrend_context *ctx,
    struct vrend_resource *res;
    int ret_handle;
    res = vrend_renderer_ctx_res_lookup(ctx, res_handle);
-   if (!res) {
+   if (!res || !res->gl_id) {
       vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, res_handle);
       return EINVAL;
    }
@@ -11515,34 +12363,68 @@ int vrend_create_so_target(struct vrend_context *ctx,
    ret_handle = vrend_renderer_object_insert(ctx, target, handle,
                                              VIRGL_OBJECT_STREAMOUT_TARGET);
    if (ret_handle == 0) {
+      vrend_resource_reference(&target->buffer, NULL);
       FREE(target);
       return ENOMEM;
    }
    return 0;
 }
 
-static int vrender_get_glsl_version(void)
+static int get_glsl_version(void)
 {
    int major_local = 0, minor_local = 0;
    const GLubyte *version_str;
-   ASSERTED int c;
+   int count;
 
    version_str = glGetString(GL_SHADING_LANGUAGE_VERSION);
+   if (!version_str) {
+      virgl_error("GL_SHADING_LANGUAGE_VERSION query failed with empty output.");
+      return -1;
+   }
+
    if (vrend_state.use_gles) {
-      c = sscanf((const char *)version_str, "%*s %*s %*s %*s %i.%i",
-                  &major_local, &minor_local);
+      count = sscanf((const char *)version_str, "%*s %*s %*s %*s %i.%i",
+                          &major_local, &minor_local);
    } else {
-      c = sscanf((const char *)version_str, "%i.%i",
-                  &major_local, &minor_local);
+      count = sscanf((const char *)version_str, "%i.%i",
+                          &major_local, &minor_local);
+   }
+
+   if (count != 2) {
+      virgl_error("GL_SHADING_LANGUAGE_VERSION query failed with unexpected version format.");
+      return -1;
    }
-   assert(c == 2);
 
    return (major_local * 100) + minor_local;
 }
 
 static void vrend_fill_caps_glsl_version(int gl_ver, int gles_ver,
-					  union virgl_caps *caps)
-{
+                                         union virgl_caps *caps)
+{
+   static int debug_caps = -1;
+   if (debug_caps == -1)
+      debug_caps = getenv("VIRGL_DEBUG_CAPS") != NULL;
+   
+   if (debug_caps) {
+      fprintf(stderr, "DEBUG vrend_fill_caps_glsl_version: gl_ver=%d gles_ver=%d caps=%p\n", gl_ver, gles_ver, (void*)caps);
+      fflush(stderr);
+   }
+   
+#ifdef __APPLE__
+   /* macOS Metal reports GL 4.1 core but Mesa needs explicit GLSL 4.10.
+    * Force this for any desktop GL context on macOS to avoid fallback to GL 2.1.
+    */
+   if (gl_ver >= 30 && gles_ver == 0) {
+      caps->v1.glsl_level = 410;
+      if (debug_caps) {
+         fprintf(stderr, "DEBUG macOS override: set glsl_level=%d (caps->v1.glsl_level now=%d)\n", 
+                 410, caps->v1.glsl_level);
+         fflush(stderr);
+      }
+      return;
+   }
+#endif
+
    if (gles_ver > 0) {
       caps->v1.glsl_level = 120;
 
@@ -11588,7 +12470,7 @@ static void vrend_fill_caps_glsl_version(int gl_ver, int gles_ver,
          }
       }
    }
-   vrend_printf("GLSL feature level %d\n", caps->v1.glsl_level);
+      virgl_info("GLSL feature level %d\n", caps->v1.glsl_level);
 }
 
 static void set_format_bit(struct virgl_supported_format_mask *mask, enum virgl_formats fmt)
@@ -11609,6 +12491,10 @@ static void vrend_renderer_fill_caps_v1(int gl_ver, int gles_ver, union virgl_ca
 {
    int i;
    GLint max;
+   const char *gl_version_str = (const char *)glGetString(GL_VERSION);
+   const char *gl_renderer_str = (const char *)glGetString(GL_RENDERER);
+   const bool is_angle = ((gl_version_str && strstr(gl_version_str, "ANGLE")) ||
+                          (gl_renderer_str && strstr(gl_renderer_str, "ANGLE")));
 
    /*
     * We can't fully support this feature on GLES,
@@ -11624,7 +12510,6 @@ static void vrend_renderer_fill_caps_v1(int gl_ver, int gles_ver, union virgl_ca
 
    if (gl_ver > 0 && !vrend_state.use_core_profile) {
       caps->v1.bset.poly_stipple = 1;
-      caps->v1.bset.color_clamping = 1;
       caps->v1.prim_mask |= (1 << PIPE_PRIM_QUADS) |
                             (1 << PIPE_PRIM_QUAD_STRIP) |
                             (1 << PIPE_PRIM_POLYGON);
@@ -11654,9 +12539,27 @@ static void vrend_renderer_fill_caps_v1(int gl_ver, int gles_ver, union virgl_ca
 
    if (has_feature(feat_ubo)) {
       glGetIntegerv(GL_MAX_VERTEX_UNIFORM_BLOCKS, &max);
+      const char *version_str = (const char *)glGetString(GL_VERSION);
+      bool is_angle_local = (version_str && strstr(version_str, "ANGLE"));
       /* GL_MAX_VERTEX_UNIFORM_BLOCKS is omitting the ordinary uniform block, add it
-       * also reduce by 1 as we might generate a VirglBlock helper uniform block */
-      caps->v1.max_uniform_blocks = max + 1 - 1;
+       * also reduce by 1 as we might generate a VirglBlock helper uniform block.
+       * Mesa needs at least 12 per shader after its own adjustments, so report max+1.
+       * 
+       * Special handling for ANGLE/Metal: ANGLE clamps reported values to ES spec minimums
+       * (12 for UBOs) even though Metal backend supports 14. Detect ANGLE and report a
+       * higher value to ensure Mesa gets enough after its adjustments. */
+      if (is_angle_local && max <= 12) {
+         caps->v1.max_uniform_blocks = 14;  // ANGLE Metal internal limit
+         fprintf(stderr, "[VREND CAPS] ANGLE backend with UBO limit %d (ES spec minimum), "
+                         "overriding to 14 for Mesa compatibility\n", max);
+      } else {
+         caps->v1.max_uniform_blocks = max + 1;
+         fprintf(stderr, "[VREND CAPS] feat_ubo=YES, GL_MAX_VERTEX_UNIFORM_BLOCKS=%d, reporting max_uniform_blocks=%d\n", 
+                 max, caps->v1.max_uniform_blocks);
+      }
+   } else {
+      fprintf(stderr, "[VREND CAPS] feat_ubo=NO (gl_ver=%d, gles_ver=%d, epoxy_gl_version=%d, epoxy_is_desktop_gl=%d)\n",
+              gl_ver, gles_ver, epoxy_gl_version(), epoxy_is_desktop_gl());
    }
 
    if (has_feature(feat_depth_clamp))
@@ -11675,8 +12578,13 @@ static void vrend_renderer_fill_caps_v1(int gl_ver, int gles_ver, union virgl_ca
    if (has_feature(feat_seamless_cubemap_per_texture))
       caps->v1.bset.seamless_cube_map_per_texture = 1;
 
-   if (has_feature(feat_texture_multisample))
+   if (has_feature(feat_texture_multisample)) {
       caps->v1.bset.texture_multisample = 1;
+      fprintf(stderr, "[VREND CAPS] feat_texture_multisample enabled, setting caps->v1.bset.texture_multisample=1\n");
+   } else {
+      fprintf(stderr, "[VREND CAPS] feat_texture_multisample NOT enabled (gl_ver=%d, gles_ver=%d)\n",
+              vrend_state.use_gles ? 0 : gl_ver, vrend_state.use_gles ? gl_ver : 0);
+   }
 
    if (has_feature(feat_tessellation))
       caps->v1.bset.has_tessellation_shaders = 1;
@@ -11722,7 +12630,7 @@ static void vrend_renderer_fill_caps_v1(int gl_ver, int gles_ver, union virgl_ca
      if (has_feature(feat_cull_distance))
         caps->v1.bset.has_cull = 1;
      if (epoxy_has_gl_extension("GL_ARB_derivative_control"))
-	caps->v1.bset.derivative_control = 1;
+        caps->v1.bset.derivative_control = 1;
    }
 
    if (has_feature(feat_polygon_offset_clamp))
@@ -11731,11 +12639,9 @@ static void vrend_renderer_fill_caps_v1(int gl_ver, int gles_ver, union virgl_ca
    if (has_feature(feat_transform_feedback_overflow_query))
      caps->v1.bset.transform_feedback_overflow_query = 1;
 
-   if (epoxy_has_gl_extension("GL_EXT_texture_mirror_clamp") ||
-       epoxy_has_gl_extension("GL_ARB_texture_mirror_clamp_to_edge") ||
-       epoxy_has_gl_extension("GL_EXT_texture_mirror_clamp_to_edge")) {
+   if (has_feature(feat_texture_mirror_clamp) ||
+       has_feature(feat_texture_mirror_clamp_to_edge))
       caps->v1.bset.mirror_clamp = true;
-   }
 
    if (has_feature(feat_texture_array)) {
       glGetIntegerv(GL_MAX_ARRAY_TEXTURE_LAYERS, &max);
@@ -11795,6 +12701,7 @@ static void vrend_renderer_fill_caps_v1(int gl_ver, int gles_ver, union virgl_ca
 
    glGetIntegerv(GL_MAX_SAMPLES, &max);
    caps->v1.max_samples = max;
+   fprintf(stderr, "[VREND] GL_MAX_SAMPLES from glGetIntegerv: %d\n", max);
 
    /* All of the formats are common. */
    for (i = 0; i < VIRGL_FORMAT_MAX; i++) {
@@ -11822,16 +12729,141 @@ static void vrend_renderer_fill_caps_v2(int gl_ver, int gles_ver,  union virgl_c
    GLfloat range[2];
    uint32_t video_memory;
    const char *renderer = (const char *)glGetString(GL_RENDERER);
+   const bool angle_in_renderer = (renderer && strstr(renderer, "ANGLE"));
 
    /* Count this up when you add a feature flag that is used to set a CAP in
     * the guest that was set unconditionally before. Then check that flag and
     * this value to avoid regressions when a guest with a new mesa version is
     * run on an old virgl host. Use it also to indicate non-cap fixes on the
     * host that help enable features in the guest. */
-   caps->v2.host_feature_check_version = 17;
+   caps->v2.host_feature_check_version = 23;
+   if (gles_ver > 0 && angle_in_renderer)
+      caps->v2.host_feature_check_version = 4;
+
+   /* Forward host GL_RENDERER to the guest.
+    *
+    * Firefox has an ANGLE-specific GL_RENDERER parser that triggers on the
+    * substring "ANGLE" and may fail hard if the string doesn't match its
+    * expected formats.
+    *
+    * Additionally, Firefox's WebGL renderer sanitizer recognizes a limited set
+    * of device names (e.g. strings starting with "Apple"). The plain "virgl"
+    * renderer name does not match those heuristics.
+    *
+    * For ANGLE-on-Metal, extract the Apple SoC / device name and forward that
+    * (without the "ANGLE" token) so the guest GL_RENDERER can be both parse- and
+    * sanitize-friendly.
+    */
+   if (renderer && strstr(renderer, "ANGLE Metal Renderer:")) {
+      const char *metal_start = strstr(renderer, "ANGLE Metal Renderer:");
+      const char *device_start = metal_start + strlen("ANGLE Metal Renderer:");
+      while (*device_start == ' ')
+         device_start++;
+
+      const char *device_end = device_start;
+      while (*device_end && *device_end != ',' && *device_end != ')')
+         device_end++;
+
+      while (device_end > device_start && device_end[-1] == ' ')
+         device_end--;
+
+      /* Forward the real Metal device name.
+       * If it isn't vendor-prefixed, try to prefix it with the ANGLE vendor
+       * field from the leading "ANGLE (vendor, ...)" without including the
+       * "ANGLE" token itself.
+       */
+      if (device_end > device_start) {
+         const size_t device_len = (size_t)(device_end - device_start);
+
+         /* If already vendor-prefixed (common on Apple), keep as-is. */
+         if (device_len >= 5 && !strncmp(device_start, "Apple", 5)) {
+            snprintf(caps->v2.renderer, sizeof(caps->v2.renderer), "%.*s",
+                     (int)device_len, device_start);
+         } else {
+            const char *angle_prefix = "ANGLE (";
+            const char *vendor_start = strstr(renderer, angle_prefix);
+            if (vendor_start == renderer) {
+               vendor_start += strlen(angle_prefix);
+               const char *vendor_end = strchr(vendor_start, ',');
+               if (vendor_end && vendor_end > vendor_start) {
+                  while (*vendor_start == ' ')
+                     vendor_start++;
+                  while (vendor_end > vendor_start && vendor_end[-1] == ' ')
+                     vendor_end--;
+               }
+
+               if (vendor_end && vendor_end > vendor_start) {
+                  snprintf(caps->v2.renderer, sizeof(caps->v2.renderer), "%.*s %.*s",
+                           (int)(vendor_end - vendor_start), vendor_start,
+                           (int)device_len, device_start);
+               } else {
+                  snprintf(caps->v2.renderer, sizeof(caps->v2.renderer), "%.*s",
+                           (int)device_len, device_start);
+               }
+            } else {
+               snprintf(caps->v2.renderer, sizeof(caps->v2.renderer), "%.*s",
+                        (int)device_len, device_start);
+            }
+         }
+      } else {
+         strncpy(caps->v2.renderer, "Generic Renderer", sizeof(caps->v2.renderer) - 1);
+         caps->v2.renderer[sizeof(caps->v2.renderer) - 1] = '\0';
+      }
+   } else if (renderer && !strncmp(renderer, "ANGLE (", 7)) {
+      /* Common ANGLE format:
+       *   "ANGLE (Apple, Apple M4 Pro, OpenGL ES 3.2 ... )"
+       * Extract the renderer field (second CSV field) and forward it without
+       * the "ANGLE" token.
+       */
+      const char *p = renderer + 7; /* after "ANGLE (" */
+      while (*p == ' ')
+         p++;
+
+      const char *vendor_start = p;
+      const char *vendor_end = strchr(vendor_start, ',');
+      if (!vendor_end)
+         goto angle_generic;
+
+      const char *device_start = vendor_end + 1;
+      while (*device_start == ' ')
+         device_start++;
+
+      const char *device_end = strchr(device_start, ',');
+      if (!device_end)
+         goto angle_generic;
+
+      while (device_end > device_start && device_end[-1] == ' ')
+         device_end--;
+
+      const size_t device_len = (size_t)(device_end - device_start);
+      if (!device_len)
+         goto angle_generic;
+
+      if (device_len >= 5 && !strncmp(device_start, "Apple", 5)) {
+         snprintf(caps->v2.renderer, sizeof(caps->v2.renderer), "%.*s",
+                  (int)device_len, device_start);
+      } else {
+         /* If not Apple-prefixed, keep the renderer field as-is (still
+          * avoiding the "ANGLE" token).
+          */
+         snprintf(caps->v2.renderer, sizeof(caps->v2.renderer), "%.*s",
+                  (int)device_len, device_start);
+      }
+   } else {
+      if (renderer)
+         strncpy(caps->v2.renderer, renderer, sizeof(caps->v2.renderer) - 1);
+      else
+         strncpy(caps->v2.renderer, "(null)", sizeof(caps->v2.renderer) - 1);
+      caps->v2.renderer[sizeof(caps->v2.renderer) - 1] = '\0';
+   }
+
+   goto angle_done;
+
+angle_generic:
+   strncpy(caps->v2.renderer, "Generic Renderer", sizeof(caps->v2.renderer) - 1);
+   caps->v2.renderer[sizeof(caps->v2.renderer) - 1] = '\0';
 
-   /* Forward host GL_RENDERER to the guest. */
-   strncpy(caps->v2.renderer, renderer, sizeof(caps->v2.renderer) - 1);
+angle_done:
 
    /* glamor reject llvmpipe, and since the renderer string is
     * composed of "virgl" and this renderer string we have to
@@ -11859,9 +12891,10 @@ static void vrend_renderer_fill_caps_v2(int gl_ver, int gles_ver,  union virgl_c
    }
 
    glGetFloatv(GL_MAX_TEXTURE_LOD_BIAS, &caps->v2.max_texture_lod_bias);
-   glGetIntegerv(GL_MAX_VERTEX_ATTRIBS, (GLint*)&caps->v2.max_vertex_attribs);
 
-   if (gl_ver >= 32 || (vrend_state.use_gles && gl_ver >= 30))
+   caps->v2.max_vertex_attribs = vrend_state.max_vertex_attributes;
+
+   if (gl_ver >= 32 || gles_ver >= 30)
       glGetIntegerv(GL_MAX_VERTEX_OUTPUT_COMPONENTS, &max);
    else
       max = 64; // minimum required value
@@ -11909,13 +12942,29 @@ static void vrend_renderer_fill_caps_v2(int gl_ver, int gles_ver,  union virgl_c
       glGetIntegerv(GL_SHADER_STORAGE_BUFFER_OFFSET_ALIGNMENT, (GLint*)&caps->v2.shader_buffer_offset_alignment);
 
       glGetIntegerv(GL_MAX_VERTEX_SHADER_STORAGE_BLOCKS, &max);
-      if (max > PIPE_MAX_SHADER_BUFFERS)
-         max = PIPE_MAX_SHADER_BUFFERS;
-      caps->v2.max_shader_buffer_other_stages = max;
+      caps->v2.max_shader_buffer_other_stages = MIN2(max, PIPE_MAX_SHADER_BUFFERS);
+      caps->v2.max_shader_storage_blocks[PIPE_SHADER_VERTEX] = caps->v2.max_shader_buffer_other_stages;
       glGetIntegerv(GL_MAX_FRAGMENT_SHADER_STORAGE_BLOCKS, &max);
-      if (max > PIPE_MAX_SHADER_BUFFERS)
-         max = PIPE_MAX_SHADER_BUFFERS;
-      caps->v2.max_shader_buffer_frag_compute = max;
+      caps->v2.max_shader_buffer_frag_compute = MIN2(max, PIPE_MAX_SHADER_BUFFERS);
+      caps->v2.max_shader_storage_blocks[PIPE_SHADER_FRAGMENT] = caps->v2.max_shader_buffer_frag_compute;
+
+      if (has_feature(feat_geometry_shader)) {
+         glGetIntegerv(GL_MAX_GEOMETRY_SHADER_STORAGE_BLOCKS, &max);
+         caps->v2.max_shader_storage_blocks[PIPE_SHADER_GEOMETRY] = MIN2(max, PIPE_MAX_SHADER_BUFFERS);
+      }
+
+      if (has_feature(feat_tessellation)) {
+         glGetIntegerv(GL_MAX_TESS_CONTROL_SHADER_STORAGE_BLOCKS, &max);
+         caps->v2.max_shader_storage_blocks[PIPE_SHADER_TESS_CTRL] = MIN2(max, PIPE_MAX_SHADER_BUFFERS);
+         glGetIntegerv(GL_MAX_TESS_EVALUATION_SHADER_STORAGE_BLOCKS, &max);
+         caps->v2.max_shader_storage_blocks[PIPE_SHADER_TESS_EVAL] = MIN2(max, PIPE_MAX_SHADER_BUFFERS);
+      }
+
+      if (has_feature(feat_compute_shader)) {
+         glGetIntegerv(GL_MAX_COMPUTE_SHADER_STORAGE_BLOCKS, &max);
+         caps->v2.max_shader_storage_blocks[PIPE_SHADER_COMPUTE] = MIN2(max, PIPE_MAX_SHADER_BUFFERS);
+      }
+
       glGetIntegerv(GL_MAX_COMBINED_SHADER_STORAGE_BLOCKS, &max);
       /* We use a 32 bit mask for the binding points and the binding points
        * must be sufficient for all shader stages combined. */
@@ -11936,8 +12985,25 @@ static void vrend_renderer_fill_caps_v2(int gl_ver, int gles_ver,  union virgl_c
          glGetIntegerv(GL_MAX_IMAGE_SAMPLES, (GLint*)&caps->v2.max_image_samples);
    }
 
-   if (has_feature(feat_storage_multisample))
-      caps->v1.max_samples = vrend_renderer_query_multisample_caps(caps->v1.max_samples, &caps->v2);
+   /* Always call query_multisample_caps - it will handle the case where
+    * GL_ARB_texture_storage_multisample isn't available by skipping the test
+    * and returning the faked max_samples value with graceful downgrade. */
+   caps->v1.max_samples = vrend_renderer_query_multisample_caps(caps->v1.max_samples, &caps->v2);
+   fprintf(stderr, "[VREND CAPS] After query_multisample_caps: max_samples=%u\n", caps->v1.max_samples);
+   
+   /* For macOS Metal backend: Override to max_samples=1 to trigger Mesa's fake_sw_msaa.
+    * Even though MSAA tests confirm 4 samples work, Mesa's format queries can't verify
+    * multisample support, causing MaxSamples=0. Reporting 1 triggers fake_sw_msaa workaround.
+    * Apply to both desktop GL and GLES (ANGLE) modes. */
+   const char *version = (const char *)glGetString(GL_VERSION);
+   bool is_metal = (version && strstr(version, "Metal"));
+   bool is_angle = (version && strstr(version, "ANGLE"));
+   if ((is_metal || is_angle) && caps->v1.max_samples > 1) {
+      fprintf(stderr, "[VREND CAPS] %s backend: Overriding max_samples %u -> 1 for fake_sw_msaa\n", 
+              is_angle ? "ANGLE" : "Metal",
+              caps->v1.max_samples);
+      caps->v1.max_samples = 1;
+   }
 
    caps->v2.capability_bits |= VIRGL_CAP_TGSI_INVARIANT | VIRGL_CAP_SET_MIN_SAMPLES |
                                VIRGL_CAP_TGSI_PRECISE | VIRGL_CAP_APP_TWEAK_SUPPORT;
@@ -12007,6 +13073,12 @@ static void vrend_renderer_fill_caps_v2(int gl_ver, int gles_ver,  union virgl_c
                        (GLint*)(caps->v2.max_atomic_counter_buffers + PIPE_SHADER_TESS_CTRL));
          glGetIntegerv(GL_MAX_TESS_EVALUATION_ATOMIC_COUNTER_BUFFERS,
                        (GLint*)(caps->v2.max_atomic_counter_buffers + PIPE_SHADER_TESS_EVAL));
+
+         glGetIntegerv(GL_MAX_TESS_CONTROL_TOTAL_OUTPUT_COMPONENTS, &max);
+         caps->v2.max_tcs_outputs = max / 4;
+
+         glGetIntegerv(GL_MAX_TESS_EVALUATION_OUTPUT_COMPONENTS, &max);
+         caps->v2.max_tes_outputs = max / 4;
       }
 
       if (has_feature(feat_compute_shader)) {
@@ -12093,14 +13165,19 @@ static void vrend_renderer_fill_caps_v2(int gl_ver, int gles_ver,  union virgl_c
             readback_str = "readback";
             set_format_bit(&caps->v2.supported_readback_formats, fmt);
          }
+         /* Only report MSAA support for formats that actually support it.
+          * Even though we fake max_samples=4 for GL 3.0 requirements, we don't
+          * advertise MSAA format support to prevent Mesa from trying to use MSAA
+          * (which would fail on Metal backend). */
          if (vrend_format_can_multisample(fmt)) {
             log_texture_feature = true;
             multisample_str = "multisample";
             set_format_bit(&caps->v2.supported_multisample_formats, fmt);
          }
          if (log_texture_feature)
-            VREND_DEBUG(dbg_features, NULL, "%s: Supports %s %s\n",
-                        util_format_name(fmt), readback_str, multisample_str);
+            VREND_DEBUG(dbg_features, NULL, "%s: Supports %s %s tv_class:%d\n",
+                        util_format_name(fmt), readback_str, multisample_str,
+                        tex_conv_table[i].view_class);
       }
 
       if (vrend_format_can_scanout(fmt))
@@ -12154,7 +13231,7 @@ static void vrend_renderer_fill_caps_v2(int gl_ver, int gles_ver,  union virgl_c
          caps->v2.capability_bits |= VIRGL_CAP_ARB_BUFFER_STORAGE;
    }
 
-#ifdef ENABLE_MINIGBM_ALLOCATION
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
    if (gbm) {
       if (has_feature(feat_memory_object) && has_feature(feat_memory_object_fd)) {
          if ((!strcmp(gbm_device_get_backend_name(gbm->device), "i915") ||
@@ -12210,12 +13287,20 @@ static void vrend_renderer_fill_caps_v2(int gl_ver, int gles_ver,  union virgl_c
       caps->v2.max_anisotropy = MIN2(max_aniso, 16.0);
    }
 
-   glGetIntegerv(GL_MAX_TEXTURE_IMAGE_UNITS, &max);
-   caps->v2.max_texture_image_units = MIN2(max, PIPE_MAX_SHADER_SAMPLER_VIEWS);
+   /* BUGFIX: original support for this cap was broken. It was _supposed_ to
+    * inform a guest-side client's query for GL_MAX_TEXTURE_IMAGE_UNITS, but
+    * only ever informed gallium's internal query for
+    * PIPE_SHADER_CAP_MAX_TEXTURE_SAMPLERS. This caused crashes on Mali.
+    *
+    * Now we send the "correct" value, to ensure good behavior from old guest
+    * drivers, but it is not "useful" for any other purpose.
+    */
+   caps->v2.max_texture_samplers = PIPE_MAX_SAMPLERS;
 
    if (has_feature(feat_ubo)) {
       glGetIntegerv(GL_MAX_UNIFORM_BLOCK_SIZE, &max);
       caps->v2.max_uniform_block_size = max;
+      fprintf(stderr, "[VREND CAPS] GL_MAX_UNIFORM_BLOCK_SIZE=%d (Mesa needs >=16384 for UBO)\n", max);
    }
 
    /* Propagate the max of Uniform Components */
@@ -12260,8 +13345,17 @@ static void vrend_renderer_fill_caps_v2(int gl_ver, int gles_ver,  union virgl_c
    if (has_feature(feat_group_vote))
       caps->v2.capability_bits_v2 |= VIRGL_CAP_V2_GROUP_VOTE;
 
+   if (has_feature(feat_texture_mirror_clamp_to_edge))
+      caps->v2.capability_bits_v2 |= VIRGL_CAP_V2_MIRROR_CLAMP_TO_EDGE;
+
+   if (has_feature(feat_texture_mirror_clamp))
+      caps->v2.capability_bits_v2 |= VIRGL_CAP_V2_MIRROR_CLAMP;
+
 #ifdef ENABLE_VIDEO
-   vrend_video_fill_caps(caps);
+   /* Disable video caps entirely to prevent guest CREATE_VIDEO_BUFFER commands
+    * that the host cannot service in this configuration.
+    */
+   caps->v2.num_video_caps = 0;
 #else
    caps->v2.num_video_caps = 0;
 #endif
@@ -12274,19 +13368,30 @@ void vrend_renderer_fill_caps(uint32_t set, uint32_t version,
    GLenum err;
    bool fill_capset2 = false;
 
+   if (getenv("VIRGL_DEBUG_CAPS")) {
+      fprintf(stderr, "DEBUG vrend_renderer_fill_caps called: set=%u version=%u caps=%p\n", set, version, (void*)caps);
+      fflush(stderr);
+   }
+
    if (!caps)
       return;
 
    switch (set) {
-   case VIRGL_RENDERER_CAPSET_VIRGL:
+   case VIRTGPU_DRM_CAPSET_VIRGL:
       if (version > VREND_CAPSET_VIRGL_MAX_VERSION)
          return;
+      fprintf(stderr, "DEBUG: VIRGL capset (v1), zeroing caps\n");
+      fflush(stderr);
       memset(caps, 0, sizeof(struct virgl_caps_v1));
       caps->max_version = VREND_CAPSET_VIRGL_MAX_VERSION;
       break;
-   case VIRGL_RENDERER_CAPSET_VIRGL2:
+   case VIRTGPU_DRM_CAPSET_VIRGL2:
       if (version > VREND_CAPSET_VIRGL2_MAX_VERSION)
          return;
+      if (getenv("VIRGL_DEBUG_CAPS")) {
+         fprintf(stderr, "DEBUG: VIRGL2 capset (v2), zeroing caps\n");
+         fflush(stderr);
+      }
       memset(caps, 0, sizeof(*caps));
       caps->max_version = VREND_CAPSET_VIRGL2_MAX_VERSION;
       fill_capset2 = true;
@@ -12299,7 +13404,7 @@ void vrend_renderer_fill_caps(uint32_t set, uint32_t version,
     * have cleaned up propperly, so read the error state until we are okay.
     */
    while ((err = glGetError()) != GL_NO_ERROR)
-      vrend_printf("%s: Entering with stale GL error: %d\n", __func__, err);
+      virgl_warn("%s: Entering with stale GL error: %d\n", __func__, err);
 
    if (vrend_state.use_gles) {
       gles_ver = epoxy_gl_version();
@@ -12311,13 +13416,33 @@ void vrend_renderer_fill_caps(uint32_t set, uint32_t version,
 
    vrend_fill_caps_glsl_version(gl_ver, gles_ver, caps);
    VREND_DEBUG(dbg_features, NULL, "GLSL support level: %d", caps->v1.glsl_level);
+   
+   if (getenv("VIRGL_DEBUG_CAPS")) {
+      fprintf(stderr, "DEBUG after vrend_fill_caps_glsl_version: glsl_level=%d\n", caps->v1.glsl_level);
+      fflush(stderr);
+   }
 
    vrend_renderer_fill_caps_v1(gl_ver, gles_ver, caps);
+   
+   if (getenv("VIRGL_DEBUG_CAPS")) {
+      fprintf(stderr, "DEBUG after vrend_renderer_fill_caps_v1: glsl_level=%d\n", caps->v1.glsl_level);
+      fflush(stderr);
+   }
 
    if (!fill_capset2)
       return;
 
    vrend_renderer_fill_caps_v2(gl_ver, gles_ver, caps);
+   
+   if (getenv("VIRGL_DEBUG_CAPS")) {
+      fprintf(stderr, "DEBUG after vrend_renderer_fill_caps_v2: glsl_level=%d\n", caps->v1.glsl_level);
+      fflush(stderr);
+   }
+
+   /* Final caps report */
+   fprintf(stderr, "[VREND CAPS] FINAL VALUES: glsl_level=%u, max_samples=%u\n", 
+           caps->v1.glsl_level, caps->v1.max_samples);
+
 }
 
 GLint64 vrend_renderer_get_timestamp(void)
@@ -12336,7 +13461,7 @@ void *vrend_renderer_get_cursor_contents(struct pipe_resource *pres,
    int blsize;
    char *data, *data2;
    int size;
-   uint h;
+   unsigned h;
 
    if (res->base.width0 > 128 || res->base.height0 > 128)
       return NULL;
@@ -12364,12 +13489,12 @@ void *vrend_renderer_get_cursor_contents(struct pipe_resource *pres,
    }
 
    if (has_feature(feat_arb_robustness)) {
-      glBindTexture(res->target, res->id);
+      glBindTexture(res->target, res->gl_id);
       glGetnTexImageARB(res->target, 0, format, type, size, data);
    } else if (vrend_state.use_gles) {
       do_readpixels(res, 0, 0, 0, 0, 0, *width, *height, format, type, size, data);
    } else {
-      glBindTexture(res->target, res->id);
+      glBindTexture(res->target, res->gl_id);
       glGetTexImage(res->target, 0, format, type, data);
    }
 
@@ -12427,8 +13552,7 @@ void vrend_renderer_get_rect(struct pipe_resource *pres,
 static struct vrend_untyped_resource *
 vrend_renderer_find_untyped_resource(struct vrend_context *ctx, uint32_t res_id)
 {
-   struct vrend_untyped_resource *iter;
-   LIST_FOR_EACH_ENTRY(iter, &ctx->untyped_resources, head) {
+   list_for_each_entry(struct vrend_untyped_resource, iter, &ctx->untyped_resources, head) {
       if (iter->resource->res_id == res_id)
          return iter;
    }
@@ -12451,7 +13575,7 @@ void vrend_renderer_attach_res_ctx(struct vrend_context *ctx,
             wrapper->resource = last;
             list_add(&wrapper->head, &ctx->untyped_resources);
          } else {
-            vrend_printf("dropping attached resource %d due to OOM\n", last->res_id);
+            virgl_warn("Dropping attached resource %d due to OOM\n", last->res_id);
          }
       }
 
@@ -12508,7 +13632,7 @@ void vrend_renderer_resource_get_info(struct pipe_resource *pres,
 
    elsize = util_format_get_blocksize(res->base.format);
 
-   info->tex_id = res->id;
+   info->tex_id = res->gl_id;
    info->width = res->base.width0;
    info->height = res->base.height0;
    info->depth = res->base.depth0;
@@ -12517,12 +13641,44 @@ void vrend_renderer_resource_get_info(struct pipe_resource *pres,
    info->stride = util_format_get_nblocksx(res->base.format, u_minify(res->base.width0, 0)) * elsize;
 }
 
+void vrend_renderer_borrow_texture_for_scanout(struct pipe_resource *pres)
+{
+   struct vrend_texture *tex = (struct vrend_texture *)pres;
+   struct vrend_format_table *tex_conv = &tex_conv_table[tex->base.base.format];
+
+   assert(tex->base.target == GL_TEXTURE_2D);
+   assert(!util_format_is_depth_or_stencil(tex->base.base.format));
+
+   glBindTexture(GL_TEXTURE_2D, tex->base.gl_id);
+
+   if (tex_conv->flags & VIRGL_TEXTURE_NEED_SWIZZLE) {
+      for (unsigned i = 0; i < ARRAY_SIZE(tex->cur_swizzle); ++i) {
+         GLint next_swizzle = to_gl_swizzle(tex_conv->swizzle[i]);
+         if (tex->cur_swizzle[i] != next_swizzle) {
+            glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_SWIZZLE_R + i, next_swizzle);
+            tex->cur_swizzle[i] = next_swizzle;
+         }
+      }
+   }
+
+   if (tex->cur_srgb_decode != GL_DECODE_EXT && util_format_is_srgb(tex->base.base.format)) {
+      if (has_feature(feat_texture_srgb_decode)) {
+         glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_SRGB_DECODE_EXT,
+                         GL_DECODE_EXT);
+         tex->cur_srgb_decode = GL_DECODE_EXT;
+      }
+   }
+}
+
 int
 vrend_renderer_resource_d3d11_texture2d(struct pipe_resource *pres, void **d3d_tex2d)
 {
 #ifdef WIN32
    struct vrend_resource *res = (struct vrend_resource *)pres;
 
+   if (!vrend_state.d3d_share_texture)
+      return 0;
+
    if (!res->d3d_tex2d)
       return EINVAL;
 
@@ -12539,11 +13695,11 @@ void vrend_renderer_get_cap_set(uint32_t cap_set, uint32_t *max_ver,
                                 uint32_t *max_size)
 {
    switch (cap_set) {
-   case VIRGL_RENDERER_CAPSET_VIRGL:
+   case VIRTGPU_DRM_CAPSET_VIRGL:
       *max_ver = VREND_CAPSET_VIRGL_MAX_VERSION;
       *max_size = sizeof(struct virgl_caps_v1);
       break;
-   case VIRGL_RENDERER_CAPSET_VIRGL2:
+   case VIRTGPU_DRM_CAPSET_VIRGL2:
       *max_ver = VREND_CAPSET_VIRGL2_MAX_VERSION;
       *max_size = sizeof(struct virgl_caps_v2);
       break;
@@ -12552,27 +13708,28 @@ void vrend_renderer_get_cap_set(uint32_t cap_set, uint32_t *max_ver,
       *max_size = 0;
       break;
    }
+
 }
 
 void vrend_renderer_create_sub_ctx(struct vrend_context *ctx, int sub_ctx_id)
 {
-   struct vrend_sub_context *sub;
    struct virgl_gl_ctx_param ctx_params;
    GLuint i;
 
-   LIST_FOR_EACH_ENTRY(sub, &ctx->sub_ctxs, head) {
+   list_for_each_entry(struct vrend_sub_context, sub, &ctx->sub_ctxs, head) {
       if (sub->sub_ctx_id == sub_ctx_id) {
          return;
       }
    }
 
-   sub = CALLOC_STRUCT(vrend_sub_context);
+   struct vrend_sub_context *sub = CALLOC_STRUCT(vrend_sub_context);
    if (!sub)
       return;
 
    ctx_params.shared = (ctx->ctx_id == 0 && sub_ctx_id == 0) ? false : true;
    ctx_params.major_ver = vrend_state.gl_major_ver;
    ctx_params.minor_ver = vrend_state.gl_minor_ver;
+   ctx_params.compat_ctx = !vrend_state.use_core_profile && !vrend_state.use_gles;
    sub->gl_context = vrend_clicbs->create_gl_context(0, &ctx_params);
    sub->parent = ctx;
    vrend_clicbs->make_current(sub->gl_context);
@@ -12596,8 +13753,8 @@ void vrend_renderer_create_sub_ctx(struct vrend_context *ctx, int sub_ctx_id)
       sub->hw_blend_state.rt[i].colormask = 0xf;
    }
 
+   glGenVertexArrays(1, &sub->vaoid);
    if (!has_feature(feat_gles31_vertex_attrib_binding)) {
-      glGenVertexArrays(1, &sub->vaoid);
       glBindVertexArray(sub->vaoid);
    }
 
@@ -12630,32 +13787,27 @@ unsigned vrend_context_has_debug_flag(const struct vrend_context *ctx, enum virg
 void vrend_print_context_name(const struct vrend_context *ctx)
 {
    if (ctx)
-      vrend_printf("%s: ", ctx->debug_name);
+      virgl_debug("%s: ", ctx->debug_name);
    else
-      vrend_printf("HOST: ");
+      virgl_debug("HOST: ");
 }
 
 
 void vrend_renderer_destroy_sub_ctx(struct vrend_context *ctx, int sub_ctx_id)
 {
-   struct vrend_sub_context *sub, *tofree = NULL;
-
    /* never destroy sub context id 0 */
    if (sub_ctx_id == 0)
       return;
 
-   LIST_FOR_EACH_ENTRY(sub, &ctx->sub_ctxs, head) {
+   list_for_each_entry(struct vrend_sub_context, sub, &ctx->sub_ctxs, head) {
       if (sub->sub_ctx_id == sub_ctx_id) {
-         tofree = sub;
-      }
-   }
-
-   if (tofree) {
-      if (ctx->sub == tofree) {
-         ctx->sub = ctx->sub0;
+         if (ctx->sub == sub) {
+            ctx->sub = ctx->sub0;
+         }
+         vrend_destroy_sub_context(sub);
+         vrend_clicbs->make_current(ctx->sub->gl_context);
+         break;
       }
-      vrend_destroy_sub_context(tofree);
-      vrend_clicbs->make_current(ctx->sub->gl_context);
    }
 }
 
@@ -12683,14 +13835,16 @@ void vrend_renderer_reset(void)
    vrend_destroy_context(vrend_state.ctx0);
 
    vrend_state.ctx0 = vrend_create_context(0, strlen("HOST"), "HOST");
-   /* TODO respawn sync thread */
+
+   if (vrend_state.use_async_fence_cb)
+      vrend_renderer_use_threaded_sync();
 }
 
 int vrend_renderer_get_poll_fd(void)
 {
    int fd = vrend_state.eventfd;
    if (vrend_state.use_async_fence_cb && fd < 0)
-      vrend_printf("failed to duplicate eventfd: error=%d\n", errno);
+      virgl_error("Failed to duplicate eventfd: error=%d\n", errno);
    return fd;
 }
 
@@ -12699,7 +13853,7 @@ int vrend_renderer_export_query(struct pipe_resource *pres,
 {
    struct vrend_resource *res = (struct vrend_resource *)pres;
 
-#ifdef ENABLE_MINIGBM_ALLOCATION
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
    if (res->gbm_bo)
       return virgl_gbm_export_query(res->gbm_bo, export_query);
 #else
@@ -12735,9 +13889,8 @@ int vrend_renderer_pipe_resource_create(struct vrend_context *ctx, uint32_t blob
 struct pipe_resource *vrend_get_blob_pipe(struct vrend_context *ctx, uint64_t blob_id)
 {
    uint32_t id = (uint32_t)blob_id;
-   struct vrend_resource *res, *stor;
 
-   LIST_FOR_EACH_ENTRY_SAFE(res, stor, &ctx->vrend_resources, head) {
+   list_for_each_entry_safe(struct vrend_resource, res, &ctx->vrend_resources, head) {
       if (res->blob_id != id)
          continue;
 
@@ -12784,7 +13937,6 @@ vrend_renderer_pipe_resource_set_type(struct vrend_context *ctx,
 
    /* resource is still untyped */
    if (!res->pipe_resource) {
-#ifdef ENABLE_GBM
       const struct vrend_renderer_resource_create_args create_args = {
          .target = PIPE_TEXTURE_2D,
          .format = args->format,
@@ -12797,61 +13949,105 @@ vrend_renderer_pipe_resource_set_type(struct vrend_context *ctx,
          .nr_samples = 0,
          .flags = 0,
       };
-      int plane_fds[VIRGL_GBM_MAX_PLANES];
       struct vrend_resource *gr;
-      uint32_t virgl_format;
-      uint32_t drm_format;
-      int ret;
 
       if (res->fd_type != VIRGL_RESOURCE_FD_DMABUF)
          return EINVAL;
 
-      for (uint32_t i = 0; i < args->plane_count; i++)
-         plane_fds[i] = res->fd;
-
       gr = vrend_resource_create(&create_args);
       if (!gr)
          return ENOMEM;
 
-      virgl_format = gr->base.format;
-      drm_format = 0;
-      if (virgl_gbm_convert_format(&virgl_format, &drm_format)) {
-         vrend_printf("%s: unsupported format %d\n", __func__, virgl_format);
-         FREE(gr);
-         return EINVAL;
-      }
+#ifdef HAVE_EPOXY_EGL_H
+      if (egl) {
+#ifdef ENABLE_GBM
+         int plane_fds[VIRGL_GBM_MAX_PLANES];
+         uint32_t virgl_format;
+         uint32_t drm_format;
+         int ret;
+
+         for (uint32_t i = 0; i < args->plane_count; i++)
+            plane_fds[i] = res->fd;
+
+         virgl_format = gr->base.format;
+         drm_format = 0;
+         if (virgl_gbm_convert_format(&virgl_format, &drm_format)) {
+            virgl_error("%s: unsupported format %d\n", __func__, virgl_format);
+            FREE(gr);
+            return EINVAL;
+         }
 
-      gr->egl_image = virgl_egl_image_from_dmabuf(egl,
-                                                  args->width,
-                                                  args->height,
-                                                  drm_format,
-                                                  args->modifier,
-                                                  args->plane_count,
-                                                  plane_fds,
-                                                  args->plane_strides,
-                                                  args->plane_offsets);
-      if (!gr->egl_image) {
-         vrend_printf("%s: failed to create egl image\n", __func__);
-         FREE(gr);
-         return EINVAL;
-      }
+         gr->egl_image = virgl_egl_image_from_dmabuf(egl,
+                                                     args->width,
+                                                     args->height,
+                                                     drm_format,
+                                                     args->modifier,
+                                                     args->plane_count,
+                                                     plane_fds,
+                                                     args->plane_strides,
+                                                     args->plane_offsets);
+         if (!gr->egl_image) {
+            virgl_error("%s: failed to create egl image\n", __func__);
+            FREE(gr);
+            return EINVAL;
+         }
 
-      gr->storage_bits |= VREND_STORAGE_EGL_IMAGE;
+         gr->storage_bits |= VREND_STORAGE_EGL_IMAGE;
 
-      ret = vrend_resource_alloc_texture(gr, virgl_format, gr->egl_image);
-      if (ret) {
-         virgl_egl_image_destroy(egl, gr->egl_image);
+         ret = vrend_resource_alloc_texture(gr, virgl_format, gr->egl_image);
+         if (ret) {
+            virgl_egl_image_destroy(egl, gr->egl_image);
+            FREE(gr);
+            return ret;
+         }
+
+#else /* ENABLE_GBM */
          FREE(gr);
-         return ret;
-      }
+         virgl_error("%s: no EGL/GBM support \n", __func__);
+         return EINVAL;
 
-      /* "promote" the fd to pipe_resource */
-      res->pipe_resource = &gr->base;
+#endif /* ENABLE_GBM */
+      } else {
 #else /* HAVE_EPOXY_EGL_H */
-      (void)args;
-      vrend_printf("%s: no EGL/GBM support \n", __func__);
-      return EINVAL;
+      {
 #endif /* HAVE_EPOXY_EGL_H */
+         int fd = -1;
+         GLenum internalformat = tex_conv_table[gr->base.format].internalformat;
+
+         if (!has_feature(feat_memory_object_fd) || !has_feature(feat_memory_object)) {
+            FREE(gr);
+            return EINVAL;
+         }
+
+         enum virgl_resource_fd_type fd_type = virgl_resource_export_fd(res, &fd);
+         if (fd_type == VIRGL_RESOURCE_FD_INVALID) {
+            FREE(gr);
+            return EINVAL;
+         }
+
+         /* Create a GL memory object importing memory from a FD */
+         GLuint mem_object;
+         glCreateMemoryObjectsEXT(1, &mem_object);
+         GLint params = GL_TRUE;
+         glMemoryObjectParameterivEXT(mem_object, GL_DEDICATED_MEMORY_OBJECT_EXT, &params);
+         glImportMemoryFdEXT(mem_object, res->map_size, GL_HANDLE_TYPE_OPAQUE_FD_EXT, fd);
+
+         struct pipe_resource *pr = &gr->base;
+         gr->target = tgsitargettogltarget(pr->target, pr->nr_samples);
+         gr->memobj = mem_object;
+         gr->storage_bits |= VREND_STORAGE_GL_TEXTURE | VREND_STORAGE_GL_MEMOBJ;
+
+         /* Create a GL texture which uses that memory as storage */
+         glGenTextures(1, &gr->gl_id);
+         glBindTexture(gr->target, gr->gl_id);
+         GLsizei width = (GLsizei)args->width;
+         GLsizei height = (GLsizei)args->height;
+         glTexParameteri(gr->target, GL_TEXTURE_TILING_EXT, GL_LINEAR_TILING_EXT);
+         glTexStorageMem2DEXT(gr->target, 1, internalformat, width, height, mem_object, 0);
+         glBindTexture(gr->target, 0);
+         gr->is_imported = true;
+      }
+      res->pipe_resource = &gr->base;
    }
 
    vrend_ctx_resource_insert(ctx->res_hash,
@@ -12873,7 +14069,7 @@ int vrend_renderer_resource_map(struct pipe_resource *pres, void **map, uint64_t
    if (!has_bits(res->storage_bits, VREND_STORAGE_GL_BUFFER | VREND_STORAGE_GL_IMMUTABLE))
       return -EINVAL;
 
-   glBindBufferARB(res->target, res->id);
+   glBindBufferARB(res->target, res->gl_id);
    *map = glMapBufferRange(res->target, 0, res->size, res->buffer_storage_flags);
    if (!*map)
       return -EINVAL;
@@ -12889,7 +14085,7 @@ int vrend_renderer_resource_unmap(struct pipe_resource *pres)
    if (!has_bits(res->storage_bits, VREND_STORAGE_GL_BUFFER | VREND_STORAGE_GL_IMMUTABLE))
       return -EINVAL;
 
-   glBindBufferARB(res->target, res->id);
+   glBindBufferARB(res->target, res->gl_id);
    glUnmapBuffer(res->target);
    glBindBufferARB(res->target, 0);
    return 0;
@@ -12897,6 +14093,8 @@ int vrend_renderer_resource_unmap(struct pipe_resource *pres)
 
 int vrend_renderer_create_ctx0_fence(uint32_t fence_id)
 {
+   if (!vrend_state.ctx0)
+         return EINVAL;
    return vrend_renderer_create_fence(vrend_state.ctx0,
          VIRGL_RENDERER_FENCE_FLAG_MERGEABLE, fence_id);
 }
@@ -12907,9 +14105,7 @@ static bool find_ctx0_fence_locked(struct list_head *fence_list,
                                    bool *seen_first,
                                    struct vrend_fence **fence)
 {
-   struct vrend_fence *iter;
-
-   LIST_FOR_EACH_ENTRY(iter, fence_list, fences) {
+   list_for_each_entry(struct vrend_fence, iter, fence_list, fences) {
       /* only consider ctx0 fences */
       if (iter->ctx != vrend_state.ctx0)
          continue;
@@ -12932,6 +14128,8 @@ static bool find_ctx0_fence_locked(struct list_head *fence_list,
 
 int vrend_renderer_export_ctx0_fence(uint32_t fence_id, int* out_fd) {
 #ifdef HAVE_EPOXY_EGL_H
+   int ret = 0;
+
    if (!vrend_state.use_egl_fence) {
       return -EINVAL;
    }
@@ -12955,15 +14153,17 @@ int vrend_renderer_export_ctx0_fence(uint32_t fence_id, int* out_fd) {
          found = true;
    }
 
-   if (vrend_state.sync_thread)
-      mtx_unlock(&vrend_state.fence_mutex);
-
    if (found) {
       if (fence)
-         return virgl_egl_export_fence(egl, fence->eglsyncobj, out_fd) ? 0 : -EINVAL;
+         ret = virgl_egl_export_fence(egl, fence->eglsyncobj, out_fd) ? 0 : -EINVAL;
       else
-         return virgl_egl_export_signaled_fence(egl, out_fd) ? 0 : -EINVAL;
+         ret = virgl_egl_export_signaled_fence(egl, out_fd) ? 0 : -EINVAL;
    }
+
+   if (vrend_state.sync_thread)
+      mtx_unlock(&vrend_state.fence_mutex);
+
+   return ret;
 #else
    (void)fence_id;
    (void)out_fd;
@@ -12982,6 +14182,11 @@ void vrend_renderer_get_meminfo(struct vrend_context *ctx, uint32_t res_handle)
       return;
    }
 
+   if (!res->iov || res->iov->iov_len < sizeof(struct virgl_memory_info)) {
+       vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, res_handle);
+       return;
+   }
+
    info = (struct virgl_memory_info *)res->iov->iov_base;
 
    if (has_feature(feat_nvx_gpu_memory_info)) {
@@ -13055,3 +14260,8 @@ struct vrend_video_context *vrend_context_get_video_ctx(struct vrend_context *ct
     return ctx->video;
 }
 #endif
+
+bool vrend_renderer_video_available(void)
+{
+   return vrend_state.video_available;
+}
diff --git a/src/vrend_renderer.h b/src/vrend_renderer.h
index f5773f8..b6cf725 100644
--- a/src/vrend_renderer.h
+++ b/src/vrend_renderer.h
@@ -26,7 +26,7 @@
 #define VREND_RENDERER_H
 
 #include "pipe/p_state.h"
-#include "util/u_double_list.h"
+#include "util/list.h"
 #include "util/u_inlines.h"
 #include "virgl_protocol.h"
 #include "vrend_debug.h"
@@ -43,6 +43,13 @@
 #include <d3d11.h>
 #endif
 
+#ifdef ENABLE_TESTS
+/* With this flag set, the transfer will not try to use GBM mappings.
+ * Intended to be used in unit tests only. */
+#define VIRGL_COPY_TRANSFER3D_FLAGS_DEBUG_TEST_NO_GBM_MAPPING (1 << 2)
+#endif
+
+
 typedef void *virgl_gl_context;
 typedef void *virgl_gl_drawable;
 
@@ -50,6 +57,7 @@ struct virgl_gl_ctx_param {
    int major_ver;
    int minor_ver;
    bool shared;
+   bool compat_ctx;
 };
 
 struct virgl_context;
@@ -76,7 +84,7 @@ struct vrend_resource {
    uint32_t storage_bits;
    uint32_t map_info;
 
-   GLuint id;
+   GLuint gl_id;
    GLenum target;
 
    GLuint tbo_tex_id;/* tbos have two ids to track */
@@ -106,6 +114,7 @@ struct vrend_resource {
 
    uint32_t blob_id;
    struct list_head head;
+   bool is_imported;
 };
 
 #define VIRGL_TEXTURE_NEED_SWIZZLE        (1 << 0)
@@ -114,12 +123,54 @@ struct vrend_resource {
 #define VIRGL_TEXTURE_CAN_TARGET_RECTANGLE (1 << 3)
 #define VIRGL_TEXTURE_CAN_MULTISAMPLE      (1 << 4)
 
+enum view_class {
+   view_class_unsupported,
+   view_class_128,
+   view_class_96,
+   view_class_64,
+   view_class_48,
+   view_class_32,
+   view_class_24,
+   view_class_16,
+   view_class_8,
+   view_class_rgtc1_red,
+   view_class_rgtc2_rg,
+   view_class_bptc_unorm,
+   view_class_bptc_float,
+   view_class_dxt1_rgb,
+   view_class_dxt1_rgba,
+   view_class_dxt3_rgba,
+   view_class_dxt5_rgba,
+   view_class_eac_r11,
+   view_class_eac_rg11,
+   view_class_etc2_r11,
+   view_class_etc2_rgb,
+   view_class_etc2_rgba,
+   view_class_etc2_eac_rgba,
+   view_class_astc_4x4_rgba,
+   view_class_astc_5x4_rgba,
+   view_class_astc_5x5_rgba,
+   view_class_astc_6x5_rgba,
+   view_class_astc_6x6_rgba,
+   view_class_astc_8x5_rgba,
+   view_class_astc_8x6_rgba,
+   view_class_astc_8x8_rgba,
+   view_class_astc_10x5_rgba,
+   view_class_astc_10x6_rgba,
+   view_class_astc_10x8_rgba,
+   view_class_astc_10x10_rgba,
+   view_class_astc_12x10_rgba,
+   view_class_astc_12x12_rgba
+};
+
+
 struct vrend_format_table {
    enum virgl_formats format;
    GLenum internalformat;
    GLenum glformat;
    GLenum gltype;
    enum pipe_swizzle swizzle[4];
+   enum view_class view_class;
    uint32_t bindings;
    uint32_t flags;
 };
@@ -144,6 +195,8 @@ struct vrend_if_cbs {
 #define VREND_USE_ASYNC_FENCE_CB (1 << 2)
 #define VREND_USE_VIDEO          (1 << 3)
 #define VREND_D3D11_SHARE_TEXTURE (1 << 4)
+#define VREND_USE_COMPAT_CONTEXT (1 << 5)
+#define VREND_USE_GLES (1 << 6)
 
 bool vrend_check_no_error(struct vrend_context *ctx);
 
@@ -180,10 +233,18 @@ void vrend_clear(struct vrend_context *ctx,
                  double depth, unsigned stencil);
 
 int vrend_clear_texture(struct vrend_context* ctx,
-                        uint32_t handle, uint32_t level,
+                        struct vrend_resource *res, uint32_t level,
                         const struct pipe_box *box,
                         const void * data);
 
+void vrend_clear_surface(struct vrend_context *ctx,
+                         uint32_t surf_handle,
+                         unsigned buffers,
+                         const union pipe_color_union *color,
+                         unsigned dstx, unsigned dsty,
+                         unsigned width, unsigned height,
+                         bool render_condition_enabled);
+
 int vrend_draw_vbo(struct vrend_context *ctx,
                    const struct pipe_draw_info *info,
                    uint32_t cso, uint32_t indirect_handle, uint32_t indirect_draw_count_handle);
@@ -228,15 +289,14 @@ struct pipe_resource *
 vrend_renderer_resource_create(const struct vrend_renderer_resource_create_args *args,
                                void *image_eos);
 
-int vrend_create_surface(struct vrend_context *ctx,
-                         uint32_t handle,
-                         uint32_t res_handle, uint32_t format,
-                         uint32_t val0, uint32_t val1,
-                         uint32_t nr_samples);
-int vrend_create_sampler_view(struct vrend_context *ctx,
-                              uint32_t handle,
-                              uint32_t res_handle, uint32_t format,
-                              uint32_t val0, uint32_t val1, uint32_t swizzle_packed);
+int vrend_create_surface(struct vrend_context *ctx, uint32_t handle,
+                         struct vrend_resource *res,
+                         enum virgl_formats format, uint32_t level, uint32_t first_layer,
+                         uint32_t last_layer, uint32_t nr_samples);
+int vrend_create_sampler_view(struct vrend_context *ctx, uint32_t handle,
+                              struct vrend_resource *res, enum virgl_formats format,
+                              enum pipe_texture_target pipe_target, uint32_t val0,
+                              uint32_t val1, uint32_t swizzle_packed);
 
 int vrend_create_sampler_state(struct vrend_context *ctx,
                                uint32_t handle,
@@ -274,13 +334,13 @@ int vrend_transfer_inline_write(struct vrend_context *ctx,
 
 int vrend_renderer_copy_transfer3d(struct vrend_context *ctx,
                                    uint32_t dst_handle,
-                                   uint32_t src_handle,
+                                   struct vrend_resource *dst_res, struct vrend_resource *src_res,
                                    const struct vrend_transfer_info *info);
 
-int vrend_renderer_copy_transfer3d_from_host(struct vrend_context *ctx,
-                                   uint32_t dst_handle,
-                                   uint32_t src_handle,
-                                   const struct vrend_transfer_info *info);
+int vrend_renderer_copy_transfer3d_from_host(struct vrend_context *ctx, uint32_t dst_handle,
+                                             uint32_t src_handle,
+                                             struct vrend_resource *src_res, struct vrend_resource *dst_res,
+                                             const struct vrend_transfer_info *info);
 
 void vrend_set_viewport_states(struct vrend_context *ctx,
                                uint32_t start_slot, uint32_t num_viewports,
@@ -313,7 +373,7 @@ void vrend_set_index_buffer(struct vrend_context *ctx,
                             uint32_t res_handle,
                             uint32_t index_size,
                             uint32_t offset);
-void vrend_set_single_image_view(struct vrend_context *ctx,
+int vrend_set_single_image_view(struct vrend_context *ctx,
                                  uint32_t shader_type,
                                  uint32_t index,
                                  uint32_t format, uint32_t access,
@@ -383,8 +443,8 @@ void vrend_set_uniform_buffer(struct vrend_context *ctx, uint32_t shader,
                               uint32_t res_handle);
 
 void vrend_fb_bind_texture_id(struct vrend_resource *res,
-                              int id, int idx, uint32_t level,
-                              uint32_t layer, uint32_t samples);
+                              int id, GLuint idx, GLint level,
+                              GLint layer, uint32_t samples);
 
 void vrend_set_tess_state(struct vrend_context *ctx, const float tess_factors[6]);
 
@@ -414,16 +474,16 @@ int vrend_create_query(struct vrend_context *ctx, uint32_t handle,
 
 int vrend_begin_query(struct vrend_context *ctx, uint32_t handle);
 int vrend_end_query(struct vrend_context *ctx, uint32_t handle);
-void vrend_get_query_result(struct vrend_context *ctx, uint32_t handle,
+int vrend_get_query_result(struct vrend_context *ctx, uint32_t handle,
                             uint32_t wait);
-void vrend_get_query_result_qbo(struct vrend_context *ctx, uint32_t handle,
+int vrend_get_query_result_qbo(struct vrend_context *ctx, uint32_t handle,
                                 uint32_t qbo_handle,
                                 uint32_t wait, uint32_t result_type, uint32_t offset,
                                 int32_t index);
 void vrend_render_condition(struct vrend_context *ctx,
                             uint32_t handle,
                             bool condtion,
-                            uint mode);
+                            uint32_t mode);
 void *vrend_renderer_get_cursor_contents(struct pipe_resource *pres,
                                          uint32_t *width,
                                          uint32_t *height);
@@ -451,7 +511,11 @@ vrend_resource_reference(struct vrend_resource **ptr, struct vrend_resource *tex
 {
    struct vrend_resource *old_tex = *ptr;
 
-   if (pipe_reference(&(*ptr)->base.reference, &tex->base.reference))
+   /**
+    * Check the comment in vrend_sampler_view_reference for more information.
+    * Like above here only the address of the first element of *ptr.
+    */
+   if (pipe_reference((struct pipe_reference *)*ptr, (struct pipe_reference *)tex))
       vrend_renderer_resource_destroy(old_tex);
    *ptr = tex;
 }
@@ -499,6 +563,8 @@ struct vrend_blit_info {
 void vrend_renderer_resource_get_info(struct pipe_resource *pres,
                                       struct vrend_renderer_resource_info *info);
 
+void vrend_renderer_borrow_texture_for_scanout(struct pipe_resource *pres);
+
 void vrend_renderer_get_cap_set(uint32_t cap_set, uint32_t *max_ver,
                                 uint32_t *max_size);
 
@@ -516,15 +582,15 @@ void vrend_report_context_error_internal(const char *fname, struct vrend_context
     vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_CMD_BUFFER, cmd)
 
 void vrend_fb_bind_texture(struct vrend_resource *res,
-                           int idx,
-                           uint32_t level, uint32_t layer);
+                           GLuint idx,
+                           GLint level, GLint layer);
 bool vrend_format_is_emulated_alpha(enum virgl_formats format);
 bool vrend_format_is_bgra(enum virgl_formats format);
 
 #define VREND_COPY_COMPAT_FLAG_ALLOW_COMPRESSED (1u << 0)
 #define VREND_COPY_COMPAT_FLAG_ONE_IS_EGL_IMAGE (1u << 1)
-boolean format_is_copy_compatible(enum virgl_formats src, enum virgl_formats dst,
-                                  unsigned int flags);
+bool format_is_copy_compatible(enum virgl_formats src, enum virgl_formats dst,
+                               unsigned int flags);
 
 void vrend_renderer_prepare_reset(void);
 void vrend_renderer_reset(void);
@@ -547,6 +613,8 @@ static const struct gl_version gl_versions[] = { {4,6}, {4,5}, {4,4}, {4,3}, {4,
 
 extern const struct vrend_if_cbs *vrend_clicbs;
 
+bool vrend_renderer_video_available(void);
+
 int vrend_renderer_export_query(struct pipe_resource *pres,
                                 struct virgl_renderer_export_query *export_query);
 
diff --git a/src/vrend_shader.c b/src/vrend_shader.c
index c2bee98..4b47045 100644
--- a/src/vrend_shader.c
+++ b/src/vrend_shader.c
@@ -28,6 +28,7 @@
 #include "util/u_memory.h"
 #include "util/u_math.h"
 #include <string.h>
+#include <stdint.h>
 #include <stdio.h>
 #include <math.h>
 #include <errno.h>
@@ -206,10 +207,10 @@ struct dump_ctx {
    enum tgsi_processor_type prog_type;
    int size;
    struct vrend_glsl_strbufs glsl_strbufs;
-   uint instno;
+   unsigned instno;
 
-   struct vrend_strbuf src_bufs[4];
-   struct vrend_strbuf dst_bufs[3];
+   struct vrend_strbuf src_bufs[TGSI_FULL_MAX_SRC_REGISTERS];
+   struct vrend_strbuf dst_bufs[TGSI_FULL_MAX_DST_REGISTERS];
 
    uint64_t interp_input_mask;
    uint32_t num_inputs;
@@ -389,7 +390,7 @@ struct dest_info {
   enum vrend_type_qualifier dstconv;
   enum vrend_type_qualifier udstconv;
   enum vrend_type_qualifier idstconv;
-  bool dst_override_no_wm[2];
+  bool dst_override_no_wm[TGSI_FULL_MAX_DST_REGISTERS];
   int32_t dest_index;
 };
 
@@ -397,8 +398,8 @@ struct source_info {
    enum vrend_type_qualifier svec4;
    int32_t sreg_index;
    bool tg4_has_component;
-   bool override_no_wm[3];
-   bool override_no_cast[3];
+   bool override_no_wm[TGSI_FULL_MAX_SRC_REGISTERS];
+   bool override_no_cast[TGSI_FULL_MAX_SRC_REGISTERS];
    int imm_value;
 };
 
@@ -512,12 +513,12 @@ static inline const char *get_wm_string(unsigned wm)
 static inline const char *get_swizzle_string(uint8_t swizzle)
 {
    switch (swizzle) {
-   case PIPE_SWIZZLE_RED: return ".x";
-   case PIPE_SWIZZLE_GREEN: return ".y";
-   case PIPE_SWIZZLE_BLUE: return ".z";
-   case PIPE_SWIZZLE_ALPHA: return ".w";
-   case PIPE_SWIZZLE_ZERO: 
-   case PIPE_SWIZZLE_ONE: return ".0";
+   case PIPE_SWIZZLE_X: return ".x";
+   case PIPE_SWIZZLE_Y: return ".y";
+   case PIPE_SWIZZLE_Z: return ".z";
+   case PIPE_SWIZZLE_W: return ".w";
+   case PIPE_SWIZZLE_0:
+   case PIPE_SWIZZLE_1: return ".0";
    default:
       assert(0);
       return "";
@@ -861,10 +862,10 @@ static bool add_images(struct dump_ctx *ctx, int first, int last,
 
    const struct util_format_description *descr = util_format_description(img_decl->Format);
    if (descr->nr_channels == 2 &&
-       descr->swizzle[0] == UTIL_FORMAT_SWIZZLE_X &&
-       descr->swizzle[1] == UTIL_FORMAT_SWIZZLE_Y &&
-       descr->swizzle[2] == UTIL_FORMAT_SWIZZLE_0 &&
-       descr->swizzle[3] == UTIL_FORMAT_SWIZZLE_1) {
+       descr->swizzle[0] == PIPE_SWIZZLE_X &&
+       descr->swizzle[1] == PIPE_SWIZZLE_Y &&
+       descr->swizzle[2] == PIPE_SWIZZLE_0 &&
+       descr->swizzle[3] == PIPE_SWIZZLE_1) {
       ctx->shader_req_bits |= SHADER_REQ_NV_IMAGE_FORMATS;
    } else if (img_decl->Format == PIPE_FORMAT_R11G11B10_FLOAT ||
               img_decl->Format == PIPE_FORMAT_R10G10B10A2_UINT ||
@@ -873,10 +874,10 @@ static bool add_images(struct dump_ctx *ctx, int first, int last,
               img_decl->Format == PIPE_FORMAT_R16G16B16A16_SNORM)
       ctx->shader_req_bits |= SHADER_REQ_NV_IMAGE_FORMATS;
    else if (descr->nr_channels == 1 &&
-            descr->swizzle[0] == UTIL_FORMAT_SWIZZLE_X &&
-            descr->swizzle[1] == UTIL_FORMAT_SWIZZLE_0 &&
-            descr->swizzle[2] == UTIL_FORMAT_SWIZZLE_0 &&
-            descr->swizzle[3] == UTIL_FORMAT_SWIZZLE_1 &&
+            descr->swizzle[0] == PIPE_SWIZZLE_X &&
+            descr->swizzle[1] == PIPE_SWIZZLE_0 &&
+            descr->swizzle[2] == PIPE_SWIZZLE_0 &&
+            descr->swizzle[3] == PIPE_SWIZZLE_1 &&
             (descr->channel[0].size == 8 || descr->channel[0].size ==16))
       ctx->shader_req_bits |= SHADER_REQ_NV_IMAGE_FORMATS;
 
@@ -1089,6 +1090,11 @@ varying_bit_from_semantic_and_index(enum tgsi_semantic semantic, int index)
    case TGSI_SEMANTIC_PSIZE:
       return VARYING_SLOT_PSIZ;
    case TGSI_SEMANTIC_GENERIC:
+      if (unlikely(index >= MAX_VARYING)) {
+         virgl_warn("Warning: Out of range TGSI_SEMANTIC_GENERIC index: %d\n", index);
+         return VARYING_SLOT_VAR0;
+      }
+
       return VARYING_SLOT_VAR0 + index;
    case TGSI_SEMANTIC_FACE:
       return VARYING_SLOT_FACE;
@@ -1104,7 +1110,11 @@ varying_bit_from_semantic_and_index(enum tgsi_semantic semantic, int index)
    case TGSI_SEMANTIC_CLIPVERTEX:
       return VARYING_SLOT_CLIP_VERTEX;
    case TGSI_SEMANTIC_TEXCOORD:
-      assert(index < 8);
+      if (unlikely(index >= 8)) {
+         virgl_warn("Warning: Out of range TGSI_SEMANTIC_TEXCOORD index: %d\n", index);
+         return VARYING_SLOT_TEX0;
+      }
+
       return (VARYING_SLOT_TEX0 + index);
    case TGSI_SEMANTIC_PCOORD:
       return VARYING_SLOT_PNTC;
@@ -1117,9 +1127,14 @@ varying_bit_from_semantic_and_index(enum tgsi_semantic semantic, int index)
    case TGSI_SEMANTIC_TESSOUTER:
       return VARYING_SLOT_TESS_LEVEL_OUTER;
    case TGSI_SEMANTIC_PATCH:
+      if (unlikely(index >= MAX_VARYING)) {
+         virgl_warn("Warning: Out of range TGSI_SEMANTIC_PATCH index: %d\n", index);
+         return VARYING_SLOT_PATCH0;
+      }
+
       return VARYING_SLOT_PATCH0 + index;
    default:
-      vrend_printf("Warning: Bad TGSI semantic: %d/%d\n", semantic, index);
+      virgl_warn("Warning: Bad TGSI semantic: %d/%d\n", semantic, index);
       return 0;
    }
 }
@@ -1142,7 +1157,7 @@ static int lookup_image_array(const struct dump_ctx *ctx, int index)
    return image ? image->first : -1;
 }
 
-static boolean
+static bool
 iter_decls(struct tgsi_iterate_context *iter,
            struct tgsi_full_declaration *decl)
 {
@@ -1155,6 +1170,11 @@ iter_decls(struct tgsi_iterate_context *iter,
       }
 
       if (ctx->prog_type == TGSI_PROCESSOR_FRAGMENT) {
+         if (ctx->num_inputs >= ARRAY_SIZE(ctx->inputs)) {
+               virgl_error( "Number of inputs exceeded, max is %zd\n", ARRAY_SIZE(ctx->inputs));
+            return false;
+         }
+
          for (uint32_t j = 0; j < ctx->num_inputs; j++) {
             if (ctx->inputs[j].name == decl->Semantic.Name &&
                 ctx->inputs[j].sid == decl->Semantic.Index &&
@@ -1262,6 +1282,7 @@ struct syvalue_prop_map {
    [TGSI_SEMANTIC_SAMPLEID] = {"gl_SampleID", SHADER_REQ_SAMPLE_SHADING | SHADER_REQ_INTS, true},
    [TGSI_SEMANTIC_SAMPLEPOS] = { "gl_SamplePosition", SHADER_REQ_SAMPLE_SHADING, true},
    [TGSI_SEMANTIC_INVOCATIONID] = { "gl_InvocationID", SHADER_REQ_INTS | SHADER_REQ_GPU_SHADER5, true},
+   [TGSI_SEMANTIC_VERTEXID_NOBASE] = {"(gl_VertexID - gl_BaseVertexARB)", SHADER_REQ_SHADER_DRAW_PARAMETERS | SHADER_REQ_INTS | SHADER_REQ_GPU_SHADER5, true},
    [TGSI_SEMANTIC_SAMPLEMASK] = {"gl_SampleMaskIn[0]", SHADER_REQ_INTS | SHADER_REQ_GPU_SHADER5, true},
    [TGSI_SEMANTIC_PRIMID] = {"gl_PrimitiveID", SHADER_REQ_INTS | SHADER_REQ_GPU_SHADER5, true},
    [TGSI_SEMANTIC_TESSCOORD] = {"gl_TessCoord", SHADER_REQ_NONE, false},
@@ -1277,12 +1298,12 @@ struct syvalue_prop_map {
 };
 
 
-static boolean
+static bool
 iter_declaration(struct tgsi_iterate_context *iter,
                  struct tgsi_full_declaration *decl)
 {
    struct dump_ctx *ctx = (struct dump_ctx *)iter;
-   int i;
+   uint32_t i;
    int color_offset = 0;
    const char *name_prefix;
    bool add_two_side = false;
@@ -1301,12 +1322,12 @@ iter_declaration(struct tgsi_iterate_context *iter,
 
       i = ctx->num_inputs++;
       if (ctx->num_inputs > ARRAY_SIZE(ctx->inputs)) {
-         vrend_printf( "Number of inputs exceeded, max is %lu\n", ARRAY_SIZE(ctx->inputs));
+         virgl_error( "Number of inputs exceeded, max is %zd\n", ARRAY_SIZE(ctx->inputs));
          return false;
       }
 
       if (unlikely(decl->Range.First > decl->Range.Last)) {
-         vrend_printf("Wrong range: First (%u) > Last (%u)\n", decl->Range.First, decl->Range.Last);
+         virgl_error("Wrong range: First (%u) > Last (%u)\n", decl->Range.First, decl->Range.Last);
          return false;
       }
 
@@ -1378,13 +1399,13 @@ iter_declaration(struct tgsi_iterate_context *iter,
                else if (decl->Semantic.Index == 1)
                   name_prefix = "gl_SecondaryColor";
                else
-                  vrend_printf( "got illegal color semantic index %d\n", decl->Semantic.Index);
+                  virgl_error( "got illegal color semantic index %d\n", decl->Semantic.Index);
                ctx->inputs[i].glsl_no_index = true;
             } else {
                if (ctx->key->color_two_side) {
-                  int j = ctx->num_inputs++;
-                  if (ctx->num_inputs > ARRAY_SIZE(ctx->inputs)) {
-                     vrend_printf( "Number of inputs exceeded, max is %lu\n", ARRAY_SIZE(ctx->inputs));
+                  uint32_t j = ctx->num_inputs++;
+                  if (ctx->num_inputs >= ARRAY_SIZE(ctx->inputs)) {
+                     virgl_error( "Number of inputs exceeded, max is %zd\n", ARRAY_SIZE(ctx->inputs));
                      return false;
                   }
 
@@ -1401,9 +1422,9 @@ iter_declaration(struct tgsi_iterate_context *iter,
                   ctx->color_in_mask |= (1 << decl->Semantic.Index);
 
                   if (ctx->front_face_emitted == false) {
-                     int k = ctx->num_inputs++;
+                     uint32_t k = ctx->num_inputs++;
                      if (ctx->num_inputs >= ARRAY_SIZE(ctx->inputs)) {
-                        vrend_printf( "Number of inputs exceeded, max is %lu\n", ARRAY_SIZE(ctx->inputs));
+                        virgl_error( "Number of inputs exceeded, max is %zd\n", ARRAY_SIZE(ctx->inputs));
                         return false;
                      }
 
@@ -1444,8 +1465,7 @@ iter_declaration(struct tgsi_iterate_context *iter,
             ctx->inputs[i].type = VEC_INT;
             ctx->inputs[i].override_no_wm = true;
             name_prefix = "gl_ViewportIndex";
-            if (ctx->glsl_ver_required >= 140)
-               ctx->shader_req_bits |= SHADER_REQ_LAYER;
+            ctx->shader_req_bits |= SHADER_REQ_LAYER;
             if (ctx->cfg->use_gles)
                ctx->shader_req_bits |= SHADER_REQ_VIEWPORT_IDX;
          }
@@ -1570,7 +1590,7 @@ iter_declaration(struct tgsi_iterate_context *iter,
          }
          break;
       default:
-         vrend_printf("unhandled input semantic: %x\n", ctx->inputs[i].name);
+         virgl_warn("Unhandled input semantic: %x\n", ctx->inputs[i].name);
          break;
       }
 
@@ -1614,12 +1634,12 @@ iter_declaration(struct tgsi_iterate_context *iter,
       }
       i = ctx->num_outputs++;
       if (ctx->num_outputs > ARRAY_SIZE(ctx->outputs)) {
-         vrend_printf( "Number of outputs exceeded, max is %lu\n", ARRAY_SIZE(ctx->outputs));
+         virgl_error("Number of outputs exceeded, max is %zd\n", ARRAY_SIZE(ctx->outputs));
          return false;
       }
 
       if (unlikely(decl->Range.First > decl->Range.Last)) {
-         vrend_printf("Wrong range: First (%u) > Last (%u)\n", decl->Range.First, decl->Range.Last);
+         virgl_error("Wrong range: First (%u) > Last (%u)\n", decl->Range.First, decl->Range.Last);
          return false;
       }
 
@@ -1651,7 +1671,7 @@ iter_declaration(struct tgsi_iterate_context *iter,
              iter->processor.Processor == TGSI_PROCESSOR_TESS_CTRL ||
              iter->processor.Processor == TGSI_PROCESSOR_TESS_EVAL) {
             if (ctx->outputs[i].first > 0)
-               vrend_printf("Illegal position input\n");
+               virgl_warn("Illegal position input\n");
             name_prefix = "gl_Position";
             ctx->outputs[i].glsl_predefined_no_emit = true;
             ctx->outputs[i].glsl_no_index = true;
@@ -1788,12 +1808,11 @@ iter_declaration(struct tgsi_iterate_context *iter,
             ctx->outputs[i].override_no_wm = true;
             ctx->outputs[i].is_int = true;
             name_prefix = "gl_ViewportIndex";
-            if (ctx->glsl_ver_required >= 140 || ctx->cfg->use_gles) {
-               if (iter->processor.Processor == TGSI_PROCESSOR_GEOMETRY)
-                  ctx->shader_req_bits |= SHADER_REQ_VIEWPORT_IDX;
-               else {
-                  ctx->shader_req_bits |= SHADER_REQ_AMD_VIEWPORT_IDX;
-               }
+            if (iter->processor.Processor == TGSI_PROCESSOR_GEOMETRY) {
+               ctx->shader_req_bits |= SHADER_REQ_VIEWPORT_IDX;
+               ctx->glsl_ver_required = require_glsl_ver(ctx, 140);
+            } else {
+               ctx->shader_req_bits |= SHADER_REQ_AMD_VIEWPORT_IDX;
             }
          }
          break;
@@ -1836,7 +1855,7 @@ iter_declaration(struct tgsi_iterate_context *iter,
          }
          break;
       default:
-         vrend_printf("unhandled output semantic: %x\n", ctx->outputs[i].name);
+         virgl_warn("Unhandled output semantic: %x\n", ctx->outputs[i].name);
          break;
       }
 
@@ -1865,7 +1884,7 @@ iter_declaration(struct tgsi_iterate_context *iter,
       break;
    case TGSI_FILE_TEMPORARY:
       if (unlikely(decl->Range.First > decl->Range.Last)) {
-         vrend_printf("Wrong range: First (%u) > Last (%u)\n", decl->Range.First, decl->Range.Last);
+         virgl_error("Wrong range: First (%u) > Last (%u)\n", decl->Range.First, decl->Range.Last);
          return false;
       }
 
@@ -1878,12 +1897,12 @@ iter_declaration(struct tgsi_iterate_context *iter,
       break;
    case TGSI_FILE_SAMPLER_VIEW:
       if (unlikely(decl->Range.First > decl->Range.Last)) {
-         vrend_printf("Wrong range: First (%u) > Last (%u)\n", decl->Range.First, decl->Range.Last);
+         virgl_error("Wrong range: First (%u) > Last (%u)\n", decl->Range.First, decl->Range.Last);
          return false;
       }
 
       if (decl->Range.Last >= ARRAY_SIZE(ctx->samplers)) {
-         vrend_printf( "Sampler view exceeded, max is %lu\n", ARRAY_SIZE(ctx->samplers));
+         virgl_error("Sampler view exceeded, max is %zd\n", ARRAY_SIZE(ctx->samplers));
          return false;
       }
       if (!add_samplers(ctx, decl->Range.First, decl->Range.Last, decl->SamplerView.Resource, decl->SamplerView.ReturnTypeX))
@@ -1891,7 +1910,7 @@ iter_declaration(struct tgsi_iterate_context *iter,
       break;
    case TGSI_FILE_IMAGE:
       if (unlikely(decl->Range.First > decl->Range.Last)) {
-         vrend_printf("Wrong range: First (%u) > Last (%u)\n", decl->Range.First, decl->Range.Last);
+         virgl_error("Wrong range: First (%u) > Last (%u)\n", decl->Range.First, decl->Range.Last);
          return false;
       }
 
@@ -1899,7 +1918,7 @@ iter_declaration(struct tgsi_iterate_context *iter,
       ctx->shader_req_bits |= SHADER_REQ_EXPLICIT_UNIFORM_LOCATION;
       ctx->shader_req_bits |= SHADER_REQ_EXPLICIT_ATTRIB_LOCATION;
       if (decl->Range.Last >= ARRAY_SIZE(ctx->images)) {
-         vrend_printf( "Image view exceeded, max is %lu\n", ARRAY_SIZE(ctx->images));
+         virgl_error("Image view exceeded, max is %zd\n", ARRAY_SIZE(ctx->images));
          return false;
       }
       if (!add_images(ctx, decl->Range.First, decl->Range.Last, &decl->Image))
@@ -1907,7 +1926,7 @@ iter_declaration(struct tgsi_iterate_context *iter,
       break;
    case TGSI_FILE_BUFFER:
       if (decl->Range.First + ctx->key->ssbo_binding_offset >= VREND_MAX_COMBINED_SSBO_BINDING_POINTS) {
-         vrend_printf( "Buffer view exceeded, max is %d\n", VREND_MAX_COMBINED_SSBO_BINDING_POINTS);
+         virgl_error("Buffer view exceeded, max is %d\n", VREND_MAX_COMBINED_SSBO_BINDING_POINTS);
          return false;
       }
       ctx->ssbo_used_mask |= (1 << decl->Range.First);
@@ -1922,15 +1941,16 @@ iter_declaration(struct tgsi_iterate_context *iter,
       }
       if (ctx->ssbo_last_binding < decl->Range.Last)
          ctx->ssbo_last_binding = decl->Range.Last;
+      ctx->glsl_ver_required = require_glsl_ver(ctx, 140);
       break;
    case TGSI_FILE_CONSTANT:
       if (decl->Declaration.Dimension && decl->Dim.Index2D != 0) {
          if (decl->Dim.Index2D > 31) {
-            vrend_printf( "Number of uniforms exceeded, max is 32\n");
+            virgl_error("Number of uniforms exceeded, max is 32\n");
             return false;
          }
          if (ctx->ubo_used_mask & (1 << decl->Dim.Index2D)) {
-            vrend_printf( "UBO #%d is already defined\n", decl->Dim.Index2D);
+            virgl_error("UBO #%d is already defined\n", decl->Dim.Index2D);
             return false;
          }
          ctx->ubo_used_mask |= (1 << decl->Dim.Index2D);
@@ -1951,7 +1971,7 @@ iter_declaration(struct tgsi_iterate_context *iter,
    case TGSI_FILE_SYSTEM_VALUE:
       i = ctx->num_system_values++;
       if (ctx->num_system_values > ARRAY_SIZE(ctx->system_values)) {
-         vrend_printf( "Number of system values exceeded, max is %lu\n", ARRAY_SIZE(ctx->system_values));
+         virgl_error("Number of system values exceeded, max is %zd\n", ARRAY_SIZE(ctx->system_values));
          return false;
       }
 
@@ -1965,7 +1985,7 @@ iter_declaration(struct tgsi_iterate_context *iter,
          struct syvalue_prop_map *svmap = &sysvalue_map[decl->Semantic.Name];
          name_prefix = svmap->glsl_name;
          if (!name_prefix) {
-            vrend_printf("Error: unsupported system value %d\n", decl->Semantic.Name);
+            virgl_error("Unsupported system value %d\n", decl->Semantic.Name);
             return false;
          }
          ctx->shader_req_bits |= svmap->required_ext;
@@ -1975,7 +1995,7 @@ iter_declaration(struct tgsi_iterate_context *iter,
             ctx->glsl_strbufs.required_sysval_uniform_decls |= BIT(UNIFORM_DRAWID_BASE);
          break;
       } else {
-         vrend_printf("Error: system value %d out of range\n", decl->Semantic.Name);
+         virgl_error("System value %d out of range\n", decl->Semantic.Name);
          return false;
       }
    case TGSI_FILE_MEMORY:
@@ -1983,28 +2003,29 @@ iter_declaration(struct tgsi_iterate_context *iter,
       break;
    case TGSI_FILE_HW_ATOMIC:
       if (unlikely(decl->Range.First > decl->Range.Last)) {
-         vrend_printf("Wrong range: First (%u) > Last (%u)\n", decl->Range.First, decl->Range.Last);
+         virgl_error("Wrong range: First (%u) > Last (%u)\n", decl->Range.First, decl->Range.Last);
          return false;
       }
 
       if (ctx->num_abo >= ARRAY_SIZE(ctx->abo_idx)) {
-         vrend_printf( "Number of atomic counter buffers exceeded, max is %lu\n", ARRAY_SIZE(ctx->abo_idx));
+         virgl_error("Number of atomic counter buffers exceeded, max is %zd\n", ARRAY_SIZE(ctx->abo_idx));
          return false;
       }
       ctx->abo_idx[ctx->num_abo] = decl->Dim.Index2D;
       ctx->abo_sizes[ctx->num_abo] = decl->Range.Last - decl->Range.First + 1;
       ctx->abo_offsets[ctx->num_abo] = decl->Range.First;
       ctx->num_abo++;
+      ctx->glsl_ver_required = require_glsl_ver(ctx, 140);
       break;
    default:
-      vrend_printf("unsupported file %d declaration\n", decl->Declaration.File);
+      virgl_error("Unsupported file %d declaration\n", decl->Declaration.File);
       break;
    }
 
    return true;
 }
 
-static boolean
+static bool
 iter_property(struct tgsi_iterate_context *iter,
               struct tgsi_full_property *prop)
 {
@@ -2098,14 +2119,14 @@ iter_property(struct tgsi_iterate_context *iter,
       }
       break;
    default:
-      vrend_printf("unhandled property: %x\n", prop->Property.PropertyName);
+      virgl_error("Unhandled property: %x\n", prop->Property.PropertyName);
       return false;
    }
 
    return true;
 }
 
-static boolean
+static bool
 iter_immediate(struct tgsi_iterate_context *iter,
                struct tgsi_full_immediate *imm)
 {
@@ -2113,22 +2134,29 @@ iter_immediate(struct tgsi_iterate_context *iter,
    int i;
    uint32_t first = ctx->num_imm;
 
-   if (first >= ARRAY_SIZE(ctx->imm)) {
-      vrend_printf( "Number of immediates exceeded, max is: %lu\n", ARRAY_SIZE(ctx->imm));
+   if (unlikely(first >= MAX_IMMEDIATE)) {
+      virgl_error("Number of immediates exceeded, max is: %u\n", MAX_IMMEDIATE);
       return false;
    }
 
    ctx->imm[first].type = imm->Immediate.DataType;
    for (i = 0; i < 4; i++) {
-      if (imm->Immediate.DataType == TGSI_IMM_FLOAT32) {
+      switch (imm->Immediate.DataType) {
+      case TGSI_IMM_FLOAT32:
          ctx->imm[first].val[i].f = imm->u[i].Float;
-      } else if (imm->Immediate.DataType == TGSI_IMM_UINT32 ||
-                 imm->Immediate.DataType == TGSI_IMM_FLOAT64) {
+         break;
+      case TGSI_IMM_UINT32:
+      case TGSI_IMM_FLOAT64:
          ctx->shader_req_bits |= SHADER_REQ_INTS;
          ctx->imm[first].val[i].ui = imm->u[i].Uint;
-      } else if (imm->Immediate.DataType == TGSI_IMM_INT32) {
+         break;
+      case TGSI_IMM_INT32:
          ctx->shader_req_bits |= SHADER_REQ_INTS;
          ctx->imm[first].val[i].i = imm->u[i].Int;
+         break;
+      default:
+         virgl_error("Unhandled immediate type, ignoring: %x\n", imm->Immediate.DataType);
+         break;
       }
    }
    ctx->num_imm++;
@@ -2200,7 +2228,7 @@ static void emit_alpha_test(const struct dump_ctx *ctx,
       glsl_strbufs->required_sysval_uniform_decls |= BIT(UNIFORM_ALPHA_REF_VAL);
       break;
    default:
-      vrend_printf( "invalid alpha-test: %x\n", ctx->key->alpha_test);
+      virgl_error("Invalid alpha-test: %x\n", ctx->key->alpha_test);
       set_buf_error(glsl_strbufs);
       return;
    }
@@ -2269,10 +2297,10 @@ static void prepare_so_movs(struct dump_ctx *ctx)
    }
 }
 
-static const struct vrend_shader_io *get_io_slot(const struct vrend_shader_io *slots, unsigned nslots, int idx)
+static const struct vrend_shader_io *get_io_slot(const struct vrend_shader_io *slots, uint32_t nslots, int idx)
 {
    const struct vrend_shader_io *result = slots;
-   for (unsigned i = 0; i < nslots; ++i, ++result) {
+   for (uint32_t i = 0; i < nslots; ++i, ++result) {
       if ((result->first <=  idx) && (result->last >=  idx))
          return result;
    }
@@ -2322,7 +2350,7 @@ static void emit_so_movs(const struct dump_ctx *ctx,
    char writemask[6];
 
    if (ctx->so->num_outputs >= PIPE_MAX_SO_OUTPUTS) {
-      vrend_printf( "Num outputs exceeded, max is %u\n", PIPE_MAX_SO_OUTPUTS);
+      virgl_error("Num outputs exceeded, max is %u\n", PIPE_MAX_SO_OUTPUTS);
       set_buf_error(glsl_strbufs);
       return;
    }
@@ -2365,9 +2393,11 @@ static void emit_so_movs(const struct dump_ctx *ctx,
             ctx->so_names[i] = strdup(out_var);
          }
       } else {
-         char ntemp[8];
-         snprintf(ntemp, 8, "tfout%d", i);
-         ctx->so_names[i] = strdup(ntemp);
+         if (!ctx->so_names[i]) {
+            char ntemp[8];
+            snprintf(ntemp, 8, "tfout%d", i);
+            ctx->so_names[i] = strdup(ntemp);
+         }
       }
       if (ctx->so->output[i].num_components == 1) {
          if (output->is_int)
@@ -2377,9 +2407,6 @@ static void emit_so_movs(const struct dump_ctx *ctx,
       } else
          snprintf(outtype, 15, "vec%d", ctx->so->output[i].num_components);
 
-      if (ctx->so->output[i].register_index >= 255)
-         continue;
-
       if (output->name == TGSI_SEMANTIC_CLIPDIST) {
          if (output->first == output->last)
             emit_buff(glsl_strbufs, "tfout%d = %s(clip_dist_temp[%d]%s);\n", i, outtype, output->sid,
@@ -2537,7 +2564,7 @@ static void emit_fragment_logicop(const struct dump_ctx *ctx,
    }
 
 
-   for (unsigned i = 0; i < ctx->num_outputs; i++) {
+   for (uint32_t i = 0; i < ctx->num_outputs; i++) {
       mask[i] = (1 << ctx->key->fs.surface_component_bits[i]) - 1;
       scale[i] = mask[i];
       switch (ctx->key->fs.logicop_func) {
@@ -2570,7 +2597,7 @@ static void emit_fragment_logicop(const struct dump_ctx *ctx,
       }
    }
 
-   for (unsigned i = 0; i < ctx->num_outputs; i++) {
+   for (uint32_t i = 0; i < ctx->num_outputs; i++) {
       switch (ctx->key->fs.logicop_func) {
       case PIPE_LOGICOP_CLEAR:
          strbuf_fmt(&full_op_buf[i], "%s", "vec4(0)");
@@ -2623,7 +2650,7 @@ static void emit_fragment_logicop(const struct dump_ctx *ctx,
       }
    }
 
-   for (unsigned i = 0; i < ctx->num_outputs; i++) {
+   for (uint32_t i = 0; i < ctx->num_outputs; i++) {
       switch (ctx->key->fs.logicop_func) {
       case PIPE_LOGICOP_NOOP:
          break;
@@ -2642,7 +2669,7 @@ static void emit_cbuf_swizzle(const struct dump_ctx *ctx,
                               struct vrend_glsl_strbufs *glsl_strbufs)
 {
    int cbuf_id = 0;
-   for (uint i = 0; i < ctx->num_outputs; i++) {
+   for (uint32_t i = 0; i < ctx->num_outputs; i++) {
       if (ctx->outputs[i].name == TGSI_SEMANTIC_COLOR) {
          if (ctx->key->fs.swizzle_output_rgb_to_bgr & (1 << cbuf_id)) {
             emit_buff(glsl_strbufs, "fsout_c%d = fsout_c%d.zyxw;\n", cbuf_id, cbuf_id);
@@ -2655,7 +2682,7 @@ static void emit_cbuf_swizzle(const struct dump_ctx *ctx,
 static void emit_cbuf_colorspace_convert(const struct dump_ctx *ctx,
                                          struct vrend_glsl_strbufs *glsl_strbufs)
 {
-   for (uint i = 0; i < ctx->num_outputs; i++) {
+   for (uint32_t i = 0; i < ctx->num_outputs; i++) {
       if (ctx->key->fs.needs_manual_srgb_encode_bitmask & (1 << i)) {
          emit_buff(glsl_strbufs,
                    "{\n"
@@ -2703,7 +2730,7 @@ static bool set_texture_reqs(struct dump_ctx *ctx,
                              uint32_t sreg_index)
 {
    if (sreg_index >= ARRAY_SIZE(ctx->samplers)) {
-      vrend_printf( "Sampler view exceeded, max is %lu\n", ARRAY_SIZE(ctx->samplers));
+      virgl_error("Sampler view exceeded, max is %zd\n", ARRAY_SIZE(ctx->samplers));
       return false;
    }
    ctx->samplers[sreg_index].tgsi_sampler_type = inst->Texture.Texture;
@@ -2930,6 +2957,10 @@ static bool fill_offset_buffer(const struct dump_ctx *ctx,
                                bool *require_dummy_value)
 {
    if (inst->TexOffsets[0].File == TGSI_FILE_IMMEDIATE) {
+      if (unlikely((unsigned) inst->TexOffsets[0].Index >= MAX_IMMEDIATE)) {
+         virgl_error("Immediate exceeded, max is %u\n", MAX_IMMEDIATE);
+         return false;
+      }
       const struct immed *imd = &ctx->imm[inst->TexOffsets[0].Index];
       switch (inst->Texture.Texture) {
       case TGSI_TEXTURE_1D:
@@ -2954,7 +2985,7 @@ static bool fill_offset_buffer(const struct dump_ctx *ctx,
                   imd->val[inst->TexOffsets[0].SwizzleZ].i);
          break;
       default:
-         vrend_printf( "unhandled texture: %x\n", inst->Texture.Texture);
+         virgl_error("Unhandled texture: %x\n", inst->Texture.Texture);
          return false;
       }
    } else if (inst->TexOffsets[0].File == TGSI_FILE_TEMPORARY) {
@@ -2992,7 +3023,7 @@ static bool fill_offset_buffer(const struct dump_ctx *ctx,
                   get_swiz_char(inst->TexOffsets[0].SwizzleZ));
          break;
       default:
-         vrend_printf( "unhandled texture: %x\n", inst->Texture.Texture);
+         virgl_error("Unhandled texture: %x\n", inst->Texture.Texture);
          return false;
          break;
       }
@@ -3031,7 +3062,7 @@ static bool fill_offset_buffer(const struct dump_ctx *ctx,
                      get_swiz_char(inst->TexOffsets[0].SwizzleZ));
             break;
          default:
-            vrend_printf( "unhandled texture: %x\n", inst->Texture.Texture);
+            virgl_error("Unhandled texture: %x\n", inst->Texture.Texture);
             return false;
             break;
          }
@@ -3303,8 +3334,8 @@ static void translate_tex(struct dump_ctx *ctx,
 
    bool exchange_bias_offset = false;
    if (inst->Texture.NumOffsets == 1) {
-      if (inst->TexOffsets[0].Index >= (int)ARRAY_SIZE(ctx->imm)) {
-         vrend_printf( "Immediate exceeded, max is %lu\n", ARRAY_SIZE(ctx->imm));
+      if (unlikely((unsigned) inst->TexOffsets[0].Index >= MAX_IMMEDIATE)) {
+         virgl_error("Immediate exceeded, max is %u\n", MAX_IMMEDIATE);
          set_buf_error(&ctx->glsl_strbufs);
          goto cleanup;
       }
@@ -3434,8 +3465,8 @@ static void translate_tex(struct dump_ctx *ctx,
 
                int swz = (packed_swizzles >> (i * 3)) & 7;
                switch (swz) {
-               case PIPE_SWIZZLE_ZERO : emit_buf(&ctx->glsl_strbufs,  "0.0"); break;
-               case PIPE_SWIZZLE_ONE :
+               case PIPE_SWIZZLE_0 : emit_buf(&ctx->glsl_strbufs,  "0.0"); break;
+               case PIPE_SWIZZLE_1 :
                   switch (dtypeprefix) {
                   case UINT_BITS_TO_FLOAT:
                      emit_buf(&ctx->glsl_strbufs,  "uintBitsToFloat(1u)");
@@ -3685,7 +3716,7 @@ static bool is_integer_memory(const struct dump_ctx *ctx, enum tgsi_file_type fi
    case TGSI_FILE_MEMORY:
       return ctx->integer_memory;
    default:
-      vrend_printf( "Invalid file type");
+      virgl_error("Invalid file type");
    }
 
    return false;
@@ -3737,7 +3768,7 @@ static void make_ssbo_varstring(const struct dump_ctx *ctx, char result[128],
    const char *cname = tgsi_proc_to_prefix(ctx->prog_type);
    bool atomic_ssbo = ctx->ssbo_atomic_mask & (1 << register_index);
    const char *atomic_str = atomic_ssbo ? "atomic" : "";
-   uint base = atomic_ssbo ? ctx->ssbo_atomic_array_base : ctx->ssbo_array_base;
+   unsigned base = atomic_ssbo ? ctx->ssbo_atomic_array_base : ctx->ssbo_array_base;
 
    if (ctx->info.indirect_files & (1 << TGSI_FILE_BUFFER)) {
       if (indirect && !ctx->cfg->use_gles)
@@ -3764,7 +3795,10 @@ translate_store(const struct dump_ctx *ctx,
 {
    const struct tgsi_full_dst_register *dst_reg = &inst->Dst[0];
 
-   assert(dinfo->dest_index >= 0);
+   if (dinfo->dest_index < 0) {
+      set_buf_error(glsl_strbufs);
+      return;
+   }
    if (dst_reg->Register.File == TGSI_FILE_IMAGE) {
 
       /* bail out if we want to write to a non-existing image */
@@ -3879,7 +3913,9 @@ translate_load(const struct dump_ctx *ctx,
    if (src->Register.File == TGSI_FILE_IMAGE) {
 
       /* Bail out if we want to load from an image that is not actually used */
-      assert(sinfo->sreg_index >= 0);
+      if (sinfo->sreg_index < 0) {
+         return false;
+      }
       if (!((1 << sinfo->sreg_index) & ctx->images_used_mask))
             return false;
 
@@ -3975,7 +4011,7 @@ translate_load(const struct dump_ctx *ctx,
          char src[128] = "";
 
          bool atomic_ssbo = ctx->ssbo_atomic_mask & (1 << inst->Src[0].Register.Index);
-         uint base = atomic_ssbo ? ctx->ssbo_atomic_array_base : ctx->ssbo_array_base;
+         unsigned base = atomic_ssbo ? ctx->ssbo_atomic_array_base : ctx->ssbo_array_base;
          int start, array_count;
          uint32_t mask = ctx->ssbo_used_mask;
          u_bit_scan_consecutive_range(&mask, &start, &array_count);
@@ -4032,7 +4068,7 @@ static const char *get_atomic_opname(int tgsi_opcode, bool *is_cas)
       opname = "Max";
       break;
    default:
-      vrend_printf( "illegal atomic opcode");
+      virgl_error("Illegal atomic opcode");
       return NULL;
    }
    return opname;
@@ -4165,13 +4201,13 @@ translate_atomic(struct dump_ctx *ctx,
    if (src->Register.File == TGSI_FILE_BUFFER || src->Register.File == TGSI_FILE_MEMORY) {
       enum vrend_type_qualifier type;
       if ((is_integer_memory(ctx, src->Register.File, src->Register.Index))) {
-	 type = INT;
-	 dtypeprefix = INT_BITS_TO_FLOAT;
-	 stypeprefix = FLOAT_BITS_TO_INT;
+         type = INT;
+         dtypeprefix = INT_BITS_TO_FLOAT;
+         stypeprefix = FLOAT_BITS_TO_INT;
       } else {
-	 type = UINT;
-	 dtypeprefix = UINT_BITS_TO_FLOAT;
-	 stypeprefix = FLOAT_BITS_TO_UINT;
+         type = UINT;
+         dtypeprefix = UINT_BITS_TO_FLOAT;
+         stypeprefix = FLOAT_BITS_TO_UINT;
       }
 
       if (is_cas)
@@ -4258,10 +4294,13 @@ static bool
 get_destination_info(struct dump_ctx *ctx,
                      const struct tgsi_full_instruction *inst,
                      struct dest_info *dinfo,
-                     struct vrend_strbuf dst_bufs[3],
-                     char fp64_dsts[3][255],
+                     struct vrend_strbuf dst_bufs[TGSI_FULL_MAX_DST_REGISTERS],
+                     char fp64_dsts[TGSI_FULL_MAX_DST_REGISTERS][255],
                      char *writemask)
 {
+   if (inst->Instruction.NumDstRegs > TGSI_FULL_MAX_DST_REGISTERS)
+      return false;
+
    const struct tgsi_full_dst_register *dst_reg;
    enum tgsi_opcode_type dtype = tgsi_opcode_infer_dst_type(inst->Instruction.Opcode);
 
@@ -4332,7 +4371,8 @@ get_destination_info(struct dump_ctx *ctx,
          dinfo->idstconv = IVEC4;
       }
 
-      if (dst_reg->Register.File == TGSI_FILE_OUTPUT) {
+      switch (dst_reg->Register.File) {
+      case TGSI_FILE_OUTPUT: {
          int j = find_io_index(ctx->num_outputs, ctx->outputs,
                                dst_reg->Register.Index);
 
@@ -4432,8 +4472,9 @@ get_destination_info(struct dump_ctx *ctx,
                break;
             }
          }
+         break;
       }
-      else if (dst_reg->Register.File == TGSI_FILE_TEMPORARY) {
+      case TGSI_FILE_TEMPORARY: {
          char temp_buf[64];
          get_temp(ctx, dst_reg->Register.Indirect, 0, dst_reg->Register.Index,
                   temp_buf, &ctx->require_dummy_value);
@@ -4445,32 +4486,43 @@ get_destination_info(struct dump_ctx *ctx,
                ctx->shader_req_bits |= SHADER_REQ_GPU_SHADER5;
             }
          }
+         break;
       }
-      else if (dst_reg->Register.File == TGSI_FILE_IMAGE) {
+      case TGSI_FILE_IMAGE: {
          const char *cname = tgsi_proc_to_prefix(ctx->prog_type);
-	 if (ctx->info.indirect_files & (1 << TGSI_FILE_IMAGE)) {
+         if (ctx->info.indirect_files & (1 << TGSI_FILE_IMAGE)) {
             int basearrayidx = lookup_image_array(ctx, dst_reg->Register.Index);
             if (dst_reg->Register.Indirect) {
-               assert(dst_reg->Indirect.File == TGSI_FILE_ADDRESS);
+               if (dst_reg->Indirect.File != TGSI_FILE_ADDRESS)
+                  return false;
                strbuf_fmt(&dst_bufs[i], "%simg%d[addr%d + %d]", cname, basearrayidx, dst_reg->Indirect.Index, dst_reg->Register.Index - basearrayidx);
             } else
                strbuf_fmt(&dst_bufs[i], "%simg%d[%d]", cname, basearrayidx, dst_reg->Register.Index - basearrayidx);
          } else
             strbuf_fmt(&dst_bufs[i], "%simg%d", cname, dst_reg->Register.Index);
          dinfo->dest_index = dst_reg->Register.Index;
-      } else if (dst_reg->Register.File == TGSI_FILE_BUFFER) {
+         break;
+      }
+      case TGSI_FILE_BUFFER: {
          char dst[128];
          make_ssbo_varstring(ctx, dst, dst_reg->Register.Index, dst_reg->Register.Indirect, dst_reg->Indirect.Index);
          strbuf_fmt(&dst_bufs[i], "%s", dst);
          dinfo->dest_index = dst_reg->Register.Index;
-      } else if (dst_reg->Register.File == TGSI_FILE_MEMORY) {
+         break;
+      }
+      case TGSI_FILE_MEMORY:
          strbuf_fmt(&dst_bufs[i], "values");
-      } else if (dst_reg->Register.File == TGSI_FILE_ADDRESS) {
+         break;
+      case TGSI_FILE_ADDRESS:
          strbuf_fmt(&dst_bufs[i], "addr%d", dst_reg->Register.Index);
+         break;
+      default:
+         /* illegal destination type */
+         return false;
       }
 
       if (dtype == TGSI_TYPE_DOUBLE) {
-         strcpy(fp64_dsts[i], dst_bufs[i].buf);
+         snprintf(fp64_dsts[i], sizeof(fp64_dsts[i]), "%s", dst_bufs[i].buf);
          strbuf_fmt(&dst_bufs[i], "fp64_dst[%d]%s", i, fp64_writemask);
          writemask[0] = 0;
       }
@@ -4604,8 +4656,11 @@ static bool
 get_source_info(struct dump_ctx *ctx,
                 const struct tgsi_full_instruction *inst,
                 struct source_info *sinfo,
-                struct vrend_strbuf srcs[4], char src_swizzle0[16])
+                struct vrend_strbuf srcs[TGSI_FULL_MAX_SRC_REGISTERS], char src_swizzle0[16])
 {
+   if (inst->Instruction.NumSrcRegs > TGSI_FULL_MAX_SRC_REGISTERS)
+      return false;
+
    bool stprefix = false;
 
    enum vrend_type_qualifier stypeprefix = TYPE_CONVERSION_NONE;
@@ -4645,7 +4700,7 @@ get_source_info(struct dump_ctx *ctx,
       char arrayname[16] = "";
       char fp64_src[255];
       int swz_idx = 0, pre_idx = 0;
-      boolean isfloatabsolute = src->Register.Absolute && stype != TGSI_TYPE_DOUBLE;
+      bool isfloatabsolute = src->Register.Absolute && stype != TGSI_TYPE_DOUBLE;
 
       sinfo->override_no_wm[i] = false;
       sinfo->override_no_cast[i] = false;
@@ -4657,7 +4712,8 @@ get_source_info(struct dump_ctx *ctx,
 
       if (src->Register.Dimension) {
          if (src->Dimension.Indirect) {
-            assert(src->DimIndirect.File == TGSI_FILE_ADDRESS);
+            if (src->DimIndirect.File != TGSI_FILE_ADDRESS)
+               return false;
             sprintf(arrayname, "[addr%d]", src->DimIndirect.Index);
          } else
             sprintf(arrayname, "[%d]", src->Dimension.Index);
@@ -4677,7 +4733,8 @@ get_source_info(struct dump_ctx *ctx,
 
       get_source_swizzle(src, swizzle_writer + swz_idx);
 
-      if (src->Register.File == TGSI_FILE_INPUT) {
+      switch (src->Register.File) {
+      case TGSI_FILE_INPUT: {
          int j = find_io_index(ctx->num_inputs, ctx->inputs, src->Register.Index);
          if (j < 0)
             return false;
@@ -4754,7 +4811,9 @@ get_source_info(struct dump_ctx *ctx,
                strbuf_fmt(src_buf, "%s(%s%s%s%s)", get_string(srcstypeprefix), prefix, input->glsl_name, arrayname, input->is_int ? "" : swizzle);
          }
          sinfo->override_no_wm[i] = input->override_no_wm;
-      } else if (src->Register.File == TGSI_FILE_OUTPUT) {
+         break;
+      }
+      case TGSI_FILE_OUTPUT: {
          int j = find_io_index(ctx->num_outputs, ctx->outputs, src->Register.Index);
          if (j < 0)
             return false;
@@ -4793,7 +4852,9 @@ get_source_info(struct dump_ctx *ctx,
             strbuf_fmt(src_buf, "%s(%s%s%s%s)", get_string(srcstypeprefix), prefix, output->glsl_name, arrayname, output->is_int ? "" : swizzle);
          }
          sinfo->override_no_wm[i] = output->override_no_wm;
-      } else if (src->Register.File == TGSI_FILE_TEMPORARY) {
+         break;
+         }
+      case TGSI_FILE_TEMPORARY: {
          struct vrend_temp_range *range = find_temp_range(ctx, src->Register.Index);
          if (!range)
             return false;
@@ -4804,17 +4865,21 @@ get_source_info(struct dump_ctx *ctx,
          char temp_buf[64];
          get_temp(ctx, src->Register.Indirect, src->Indirect.Index, src->Register.Index,
                   temp_buf, &ctx->require_dummy_value);
-         strbuf_fmt(src_buf, "%s%c%s%s%s%c", get_string(stypeprefix), stprefix ? '(' : ' ', prefix, temp_buf, swizzle, stprefix ? ')' : ' ');
-      } else if (src->Register.File == TGSI_FILE_CONSTANT) {
+         strbuf_fmt(src_buf, "%s%cvec4(%s%s)%s%c", get_string(stypeprefix), stprefix ? '(' : ' ', prefix, temp_buf, swizzle, stprefix ? ')' : ' ');
+         break;
+      }
+      case TGSI_FILE_CONSTANT: {
          const char *cname = tgsi_proc_to_prefix(ctx->prog_type);
          int dim = 0;
          if (src->Register.Dimension && src->Dimension.Index != 0) {
             dim = src->Dimension.Index;
             if (src->Dimension.Indirect) {
-               assert(src->DimIndirect.File == TGSI_FILE_ADDRESS);
+               if (src->DimIndirect.File != TGSI_FILE_ADDRESS)
+                  return false;
                ctx->shader_req_bits |= SHADER_REQ_GPU_SHADER5;
                if (src->Register.Indirect) {
-                  assert(src->Indirect.File == TGSI_FILE_ADDRESS);
+                  if (src->Indirect.File != TGSI_FILE_ADDRESS)
+                     return false;
                   strbuf_fmt(src_buf, "%s(%s%suboarr[addr%d].ubocontents[addr%d + %d]%s)", get_string(stypeprefix), prefix, cname, src->DimIndirect.Index, src->Indirect.Index, src->Register.Index, swizzle);
                } else
                   strbuf_fmt(src_buf, "%s(%s%suboarr[addr%d].ubocontents[%d]%s)", get_string(stypeprefix), prefix, cname, src->DimIndirect.Index, src->Register.Index, swizzle);
@@ -4826,7 +4891,12 @@ get_source_info(struct dump_ctx *ctx,
                      strbuf_fmt(src_buf, "%s(%s%suboarr[%d].ubocontents[%d]%s)", get_string(stypeprefix), prefix, cname, dim - ctx->ubo_base, src->Register.Index, swizzle);
                } else {
                   if (src->Register.Indirect) {
-                     strbuf_fmt(src_buf, "%s(%s%subo%dcontents[addr0 + %d]%s)", get_string(stypeprefix), prefix, cname, dim, src->Register.Index, swizzle);
+                     if (src->Indirect.File != TGSI_FILE_ADDRESS)
+                        return false;
+                     strbuf_fmt(src_buf, "%s(%s%subo%dcontents[addr%d + %d]%s)",
+                                get_string(stypeprefix), prefix, cname, dim,
+                                src->Indirect.Index, src->Register.Index,
+                                swizzle);
                   } else
                      strbuf_fmt(src_buf, "%s(%s%subo%dcontents[%d]%s)", get_string(stypeprefix), prefix, cname, dim, src->Register.Index, swizzle);
                }
@@ -4842,11 +4912,17 @@ get_source_info(struct dump_ctx *ctx,
                csp = IVEC4;
 
             if (src->Register.Indirect) {
-               strbuf_fmt(src_buf, "%s%s(%sconst%d[addr0 + %d]%s)", prefix, get_string(csp), cname, dim, src->Register.Index, swizzle);
+               if (src->Indirect.File != TGSI_FILE_ADDRESS)
+                  return false;
+               strbuf_fmt(src_buf, "%s%s(%sconst%d[addr%d + %d]%s)", prefix,
+                          get_string(csp), cname, dim, src->Indirect.Index,
+                          src->Register.Index, swizzle);
             } else
                strbuf_fmt(src_buf, "%s%s(%sconst%d[%d]%s)", prefix, get_string(csp), cname, dim, src->Register.Index, swizzle);
          }
-      } else if (src->Register.File == TGSI_FILE_SAMPLER) {
+         break;
+      }
+      case TGSI_FILE_SAMPLER: {
          const char *cname = tgsi_proc_to_prefix(ctx->prog_type);
          if (ctx->info.indirect_files & (1 << TGSI_FILE_SAMPLER)) {
             int basearrayidx = lookup_sampler_array(ctx, src->Register.Index);
@@ -4859,142 +4935,161 @@ get_source_info(struct dump_ctx *ctx,
             strbuf_fmt(src_buf, "%ssamp%d%s", cname, src->Register.Index, swizzle);
          }
          sinfo->sreg_index = src->Register.Index;
-      } else if (src->Register.File == TGSI_FILE_IMAGE) {
+      } break;
+      case TGSI_FILE_IMAGE: {
          const char *cname = tgsi_proc_to_prefix(ctx->prog_type);
          if (ctx->info.indirect_files & (1 << TGSI_FILE_IMAGE)) {
             int basearrayidx = lookup_image_array(ctx, src->Register.Index);
             if (src->Register.Indirect) {
-               assert(src->Indirect.File == TGSI_FILE_ADDRESS);
+               if (src->Indirect.File != TGSI_FILE_ADDRESS)
+                  return false;
                strbuf_fmt(src_buf, "%simg%d[addr%d + %d]", cname, basearrayidx, src->Indirect.Index, src->Register.Index - basearrayidx);
             } else
                strbuf_fmt(src_buf, "%simg%d[%d]", cname, basearrayidx, src->Register.Index - basearrayidx);
          } else
             strbuf_fmt(src_buf, "%simg%d%s", cname, src->Register.Index, swizzle);
          sinfo->sreg_index = src->Register.Index;
-      } else if (src->Register.File == TGSI_FILE_BUFFER) {
+      } break;
+      case  TGSI_FILE_BUFFER: {
          char src_str[128];
          make_ssbo_varstring(ctx, src_str, src->Register.Index, src->Register.Indirect, src->Indirect.Index);
          strbuf_fmt(src_buf, "%s", src_str);
          sinfo->sreg_index = src->Register.Index;
-      } else if (src->Register.File == TGSI_FILE_MEMORY) {
+      } break;
+      case  TGSI_FILE_MEMORY:
          strbuf_fmt(src_buf, "values");
          sinfo->sreg_index = src->Register.Index;
-      } else if (src->Register.File == TGSI_FILE_IMMEDIATE) {
-         if (src->Register.Index >= (int)ARRAY_SIZE(ctx->imm)) {
-            vrend_printf( "Immediate exceeded, max is %lu\n", ARRAY_SIZE(ctx->imm));
-            return false;
-         }
-         struct immed *imd = &ctx->imm[src->Register.Index];
-         int idx = src->Register.SwizzleX;
-         char temp[48];
-         enum vrend_type_qualifier vtype = VEC4;
-         enum vrend_type_qualifier imm_stypeprefix = stypeprefix;
-
-         if ((inst->Instruction.Opcode == TGSI_OPCODE_TG4 && i == 1) ||
-             (inst->Instruction.Opcode == TGSI_OPCODE_INTERP_SAMPLE && i == 1))
-            stype = TGSI_TYPE_SIGNED;
-
-         if (imd->type == TGSI_IMM_UINT32 || imd->type == TGSI_IMM_INT32) {
-            if (imd->type == TGSI_IMM_UINT32)
-               vtype = UVEC4;
-            else
-               vtype = IVEC4;
-
-            if (stype == TGSI_TYPE_UNSIGNED && imd->type == TGSI_IMM_INT32)
-               imm_stypeprefix = UVEC4;
-            else if (stype == TGSI_TYPE_SIGNED && imd->type == TGSI_IMM_UINT32)
-               imm_stypeprefix = IVEC4;
-            else if (stype == TGSI_TYPE_FLOAT || stype == TGSI_TYPE_UNTYPED) {
-               if (imd->type == TGSI_IMM_INT32)
-                  imm_stypeprefix = INT_BITS_TO_FLOAT;
-               else
-                  imm_stypeprefix = UINT_BITS_TO_FLOAT;
-            } else if (stype == TGSI_TYPE_UNSIGNED || stype == TGSI_TYPE_SIGNED)
-               imm_stypeprefix = TYPE_CONVERSION_NONE;
-         } else if (imd->type == TGSI_IMM_FLOAT64) {
-            vtype = UVEC4;
-            if (stype == TGSI_TYPE_DOUBLE)
-               imm_stypeprefix = TYPE_CONVERSION_NONE;
-            else
-               imm_stypeprefix = UINT_BITS_TO_FLOAT;
-         }
-
-         /* build up a vec4 of immediates */
-         strbuf_fmt(src_buf, "%s%s(%s(", prefix,
-                    get_string(imm_stypeprefix), get_string(vtype));
-
-         for (uint32_t j = 0; j < 4; j++) {
-            if (j == 0)
-               idx = src->Register.SwizzleX;
-            else if (j == 1)
-               idx = src->Register.SwizzleY;
-            else if (j == 2)
-               idx = src->Register.SwizzleZ;
-            else if (j == 3)
-               idx = src->Register.SwizzleW;
-
-            if (inst->Instruction.Opcode == TGSI_OPCODE_TG4 && i == 1 && j == 0) {
-               if (imd->val[idx].ui > 0) {
-                  sinfo->tg4_has_component = true;
-                  if (!ctx->cfg->use_gles)
-                     ctx->shader_req_bits |= SHADER_REQ_GPU_SHADER5;
-               }
+         break;
+      case TGSI_FILE_IMMEDIATE: {
+            if (unlikely((unsigned) src->Register.Index >= MAX_IMMEDIATE)) {
+               virgl_error("Immediate exceeded, max is %u\n", MAX_IMMEDIATE);
+               return false;
             }
+            struct immed *imd = &ctx->imm[src->Register.Index];
+            int idx = src->Register.SwizzleX;
+            char temp[48];
+            enum vrend_type_qualifier vtype = VEC4;
+            enum vrend_type_qualifier imm_stypeprefix = stypeprefix;
+
+            if ((inst->Instruction.Opcode == TGSI_OPCODE_TG4 && i == 1) ||
+                (inst->Instruction.Opcode == TGSI_OPCODE_INTERP_SAMPLE && i == 1))
+               stype = TGSI_TYPE_SIGNED;
 
             switch (imd->type) {
-            case TGSI_IMM_FLOAT32:
-               if (isinf(imd->val[idx].f) || isnan(imd->val[idx].f)) {
-                  ctx->shader_req_bits |= SHADER_REQ_INTS;
-                  snprintf(temp, 48, "uintBitsToFloat(%uU)", imd->val[idx].ui);
-               } else
-                  snprintf(temp, 25, "%.8g", imd->val[idx].f);
+            case TGSI_IMM_INT32:
+               vtype = IVEC4;
+               if (stype == TGSI_TYPE_SIGNED)
+                  imm_stypeprefix = TYPE_CONVERSION_NONE;
+               else if (stype == TGSI_TYPE_UNSIGNED)
+                  imm_stypeprefix = UVEC4;
+               else if (stype == TGSI_TYPE_FLOAT || stype == TGSI_TYPE_UNTYPED)
+                  imm_stypeprefix = INT_BITS_TO_FLOAT;
                break;
             case TGSI_IMM_UINT32:
-               snprintf(temp, 25, "%uU", imd->val[idx].ui);
-               break;
-            case TGSI_IMM_INT32:
-               snprintf(temp, 25, "%d", imd->val[idx].i);
-               sinfo->imm_value = imd->val[idx].i;
+               vtype = UVEC4;
+               if (stype == TGSI_TYPE_UNSIGNED)
+                  imm_stypeprefix = TYPE_CONVERSION_NONE;
+               else if (stype == TGSI_TYPE_SIGNED)
+                  imm_stypeprefix = IVEC4;
+               else if (stype == TGSI_TYPE_FLOAT || stype == TGSI_TYPE_UNTYPED)
+                  imm_stypeprefix = UINT_BITS_TO_FLOAT;
                break;
             case TGSI_IMM_FLOAT64:
-               snprintf(temp, 48, "%uU", imd->val[idx].ui);
+               vtype = UVEC4;
+               if (stype == TGSI_TYPE_DOUBLE)
+                  imm_stypeprefix = TYPE_CONVERSION_NONE;
+               else
+                  imm_stypeprefix = UINT_BITS_TO_FLOAT;
+               break;
+            case TGSI_IMM_INT64:
+            case TGSI_IMM_UINT64:
+            case TGSI_IMM_FLOAT32:
                break;
-            default:
-               vrend_printf( "unhandled imm type: %x\n", imd->type);
-               return false;
             }
-            strbuf_append(src_buf, temp);
-            if (j < 3)
-               strbuf_append(src_buf, ",");
-            else {
-               snprintf(temp, 4, "))%c", isfloatabsolute ? ')' : 0);
+
+            /* build up a vec4 of immediates */
+            strbuf_fmt(src_buf, "%s%s(%s(", prefix,
+                       get_string(imm_stypeprefix), get_string(vtype));
+
+            for (uint32_t j = 0; j < 4; j++) {
+               if (j == 0)
+                  idx = src->Register.SwizzleX;
+               else if (j == 1)
+                  idx = src->Register.SwizzleY;
+               else if (j == 2)
+                  idx = src->Register.SwizzleZ;
+               else if (j == 3)
+                  idx = src->Register.SwizzleW;
+
+               if (inst->Instruction.Opcode == TGSI_OPCODE_TG4 && i == 1 && j == 0) {
+                  if (imd->val[idx].ui > 0) {
+                     sinfo->tg4_has_component = true;
+                     if (!ctx->cfg->use_gles)
+                        ctx->shader_req_bits |= SHADER_REQ_GPU_SHADER5;
+                  }
+               }
+
+               switch (imd->type) {
+               case TGSI_IMM_FLOAT32:
+                  if (isinf(imd->val[idx].f) || isnan(imd->val[idx].f)) {
+                     ctx->shader_req_bits |= SHADER_REQ_INTS;
+                     snprintf(temp, 48, "uintBitsToFloat(%uU)", imd->val[idx].ui);
+                  } else
+                     snprintf(temp, 25, "%.8g", imd->val[idx].f);
+                  break;
+               case TGSI_IMM_UINT32:
+                  snprintf(temp, 25, "%uU", imd->val[idx].ui);
+                  break;
+               case TGSI_IMM_INT32:
+                  snprintf(temp, 25, "%d", imd->val[idx].i);
+                  sinfo->imm_value = imd->val[idx].i;
+                  break;
+               case TGSI_IMM_FLOAT64:
+                  snprintf(temp, 48, "%uU", imd->val[idx].ui);
+                  break;
+               default:
+                  virgl_error("Unhandled imm type: %x\n", imd->type);
+                  return false;
+               }
                strbuf_append(src_buf, temp);
+               if (j < 3)
+                  strbuf_append(src_buf, ",");
+               else {
+                  snprintf(temp, 4, "))%c", isfloatabsolute ? ')' : 0);
+                  strbuf_append(src_buf, temp);
+               }
             }
-         }
-      } else if (src->Register.File == TGSI_FILE_SYSTEM_VALUE) {
-         for (uint32_t j = 0; j < ctx->num_system_values; j++)
+      }  break;
+      case  TGSI_FILE_SYSTEM_VALUE: {
+         bool sysvalue_found = false;
+         for (uint32_t j = 0; j < ctx->num_system_values; j++) {
             if (ctx->system_values[j].first == src->Register.Index) {
-               if (ctx->system_values[j].name == TGSI_SEMANTIC_VERTEXID ||
-                   ctx->system_values[j].name == TGSI_SEMANTIC_INSTANCEID ||
-                   ctx->system_values[j].name == TGSI_SEMANTIC_PRIMID ||
-                   ctx->system_values[j].name == TGSI_SEMANTIC_VERTICESIN ||
-                   ctx->system_values[j].name == TGSI_SEMANTIC_INVOCATIONID ||
-                   ctx->system_values[j].name == TGSI_SEMANTIC_SAMPLEID) {
+               switch (ctx->system_values[j].name) {
+               case TGSI_SEMANTIC_VERTEXID:
+               case TGSI_SEMANTIC_VERTEXID_NOBASE:
+               case TGSI_SEMANTIC_INSTANCEID:
+               case TGSI_SEMANTIC_PRIMID:
+               case TGSI_SEMANTIC_VERTICESIN:
+               case TGSI_SEMANTIC_INVOCATIONID:
+               case TGSI_SEMANTIC_SAMPLEID:
                   if (inst->Instruction.Opcode == TGSI_OPCODE_INTERP_SAMPLE && i == 1)
                      strbuf_fmt(src_buf, "ivec4(%s)", ctx->system_values[j].glsl_name);
                   else
                      strbuf_fmt(src_buf, "%s(vec4(intBitsToFloat(%s)))", get_string(stypeprefix), ctx->system_values[j].glsl_name);
-               } else if (ctx->system_values[j].name == TGSI_SEMANTIC_HELPER_INVOCATION) {
+                  break;
+               case TGSI_SEMANTIC_HELPER_INVOCATION:
                   strbuf_fmt(src_buf, "uvec4(%s)", ctx->system_values[j].glsl_name);
-               } else if (ctx->system_values[j].name == TGSI_SEMANTIC_TESSINNER ||
-                        ctx->system_values[j].name == TGSI_SEMANTIC_TESSOUTER) {
+                  break;
+               case TGSI_SEMANTIC_TESSINNER:
+               case TGSI_SEMANTIC_TESSOUTER:
                   strbuf_fmt(src_buf, "%s(vec4(%s[%d], %s[%d], %s[%d], %s[%d]))",
                              prefix,
                              ctx->system_values[j].glsl_name, src->Register.SwizzleX,
                              ctx->system_values[j].glsl_name, src->Register.SwizzleY,
                              ctx->system_values[j].glsl_name, src->Register.SwizzleZ,
                              ctx->system_values[j].glsl_name, src->Register.SwizzleW);
-               } else if (ctx->system_values[j].name == TGSI_SEMANTIC_SAMPLEPOS) {
+                  break;
+               case TGSI_SEMANTIC_SAMPLEPOS: {
                   /* gl_SamplePosition is a vec2, but TGSI_SEMANTIC_SAMPLEPOS
                    * is a vec4 with z = w = 0
                    */
@@ -5007,27 +5102,32 @@ get_source_info(struct dump_ctx *ctx,
                              components[src->Register.SwizzleY],
                              components[src->Register.SwizzleZ],
                              components[src->Register.SwizzleW]);
-               } else if (ctx->system_values[j].name == TGSI_SEMANTIC_TESSCOORD) {
+                  break;
+               }
+               case TGSI_SEMANTIC_TESSCOORD:
                   strbuf_fmt(src_buf, "%s(vec4(%s.%c, %s.%c, %s.%c, %s.%c))",
                              prefix,
                              ctx->system_values[j].glsl_name, get_swiz_char(src->Register.SwizzleX),
                              ctx->system_values[j].glsl_name, get_swiz_char(src->Register.SwizzleY),
                              ctx->system_values[j].glsl_name, get_swiz_char(src->Register.SwizzleZ),
                              ctx->system_values[j].glsl_name, get_swiz_char(src->Register.SwizzleW));
-               } else if (ctx->system_values[j].name == TGSI_SEMANTIC_GRID_SIZE ||
-                          ctx->system_values[j].name == TGSI_SEMANTIC_THREAD_ID ||
-                          ctx->system_values[j].name == TGSI_SEMANTIC_BLOCK_ID) {
+                  break;
+               case TGSI_SEMANTIC_GRID_SIZE:
+               case TGSI_SEMANTIC_THREAD_ID:
+               case TGSI_SEMANTIC_BLOCK_ID: {
                   enum vrend_type_qualifier mov_conv = TYPE_CONVERSION_NONE;
                   if (inst->Instruction.Opcode == TGSI_OPCODE_MOV &&
                       inst->Dst[0].Register.File == TGSI_FILE_TEMPORARY)
-                    mov_conv = UINT_BITS_TO_FLOAT;
+                     mov_conv = UINT_BITS_TO_FLOAT;
                   strbuf_fmt(src_buf, "%s(uvec4(%s.%c, %s.%c, %s.%c, %s.%c))", get_string(mov_conv),
                              ctx->system_values[j].glsl_name, get_swiz_char(src->Register.SwizzleX),
                              ctx->system_values[j].glsl_name, get_swiz_char(src->Register.SwizzleY),
                              ctx->system_values[j].glsl_name, get_swiz_char(src->Register.SwizzleZ),
                              ctx->system_values[j].glsl_name, get_swiz_char(src->Register.SwizzleW));
                   sinfo->override_no_cast[i] = true;
-               } else if (ctx->system_values[j].name == TGSI_SEMANTIC_SAMPLEMASK) {
+                  break;
+               }
+               case TGSI_SEMANTIC_SAMPLEMASK: {
                   const char *vec_type = "ivec4";
                   enum vrend_type_qualifier srcstypeprefix = TYPE_CONVERSION_NONE;
                   if (stypeprefix == TYPE_CONVERSION_NONE)
@@ -5043,12 +5143,20 @@ get_source_info(struct dump_ctx *ctx,
                      src->Register.SwizzleY == TGSI_SWIZZLE_X ? ctx->system_values[j].glsl_name : "0",
                      src->Register.SwizzleZ == TGSI_SWIZZLE_X ? ctx->system_values[j].glsl_name : "0",
                      src->Register.SwizzleW == TGSI_SWIZZLE_X ? ctx->system_values[j].glsl_name : "0");
-               } else
+                  break;
+               }
+               default:
                   strbuf_fmt(src_buf, "%s%s", prefix, ctx->system_values[j].glsl_name);
-               sinfo->override_no_wm[i] = ctx->system_values[j].override_no_wm;
+                  sinfo->override_no_wm[i] = ctx->system_values[j].override_no_wm;
+               }
+               sysvalue_found = true;
                break;
             }
-      } else if (src->Register.File == TGSI_FILE_HW_ATOMIC) {
+         }
+         if (!sysvalue_found)
+            return false;
+      } break;
+      case TGSI_FILE_HW_ATOMIC: {
          for (uint32_t j = 0; j < ctx->num_abo; j++) {
             if (src->Dimension.Index == ctx->abo_idx[j] &&
                 src->Register.Index >= ctx->abo_offsets[j] &&
@@ -5058,7 +5166,8 @@ get_source_info(struct dump_ctx *ctx,
                if (ctx->abo_sizes[j] > 1) {
                   int offset = src->Register.Index - ctx->abo_offsets[j];
                   if (src->Register.Indirect) {
-                     assert(src->Indirect.File == TGSI_FILE_ADDRESS);
+                     if (src->Indirect.File != TGSI_FILE_ADDRESS)
+                        return false;
                      strbuf_fmt(src_buf, "ac%d_%d[addr%d + %d]", abo_idx, abo_offset, src->Indirect.Index, offset);
                   } else
                      strbuf_fmt(src_buf, "ac%d_%d[%d]", abo_idx, abo_offset, offset);
@@ -5068,11 +5177,14 @@ get_source_info(struct dump_ctx *ctx,
             }
          }
          sinfo->sreg_index = src->Register.Index;
+      } break;
+      default:
+         return false;
       }
 
       if (stype == TGSI_TYPE_DOUBLE) {
-         boolean isabsolute = src->Register.Absolute;
-         strcpy(fp64_src, src_buf->buf);
+         bool isabsolute = src->Register.Absolute;
+         snprintf(fp64_src, sizeof(fp64_src), "%s", src_buf->buf);
          strbuf_fmt(src_buf, "fp64_src[%d]", i);
          emit_buff(&ctx->glsl_strbufs, "%s.x = %spackDouble2x32(uvec2(%s%s))%s;\n", src_buf->buf, isabsolute ? "abs(" : "", fp64_src, swizzle, isabsolute ? ")" : "");
       }
@@ -5193,7 +5305,7 @@ void rewrite_vs_pos_array(struct dump_ctx *ctx)
    int range_end = 0;
    int io_idx = 0;
 
-   for (uint i = 0; i < ctx->num_inputs; ++i) {
+   for (uint32_t i = 0; i < ctx->num_inputs; ++i) {
       if (ctx->inputs[i].name == TGSI_SEMANTIC_POSITION) {
          ctx->inputs[i].glsl_predefined_no_emit = true;
          if (ctx->inputs[i].first < range_start) {
@@ -5257,10 +5369,10 @@ void emit_fs_clipdistance_load(const struct dump_ctx *ctx,
 }
 
 static
-void renumber_io_arrays(unsigned nio, struct vrend_shader_io *io)
+void renumber_io_arrays(uint32_t nio, struct vrend_shader_io *io)
 {
    int next_array_id = 1;
-   for (unsigned i = 0; i < nio; ++i) {
+   for (uint32_t i = 0; i < nio; ++i) {
       if (io[i].name != TGSI_SEMANTIC_GENERIC &&
           io[i].name != TGSI_SEMANTIC_PATCH)
          continue;
@@ -5393,7 +5505,7 @@ add_missing_inputs(const struct dump_ctx *ctx, struct vrend_shader_io *inputs,
    return num_inputs;
 }
 
-static boolean
+static bool
 iter_instruction(struct tgsi_iterate_context *iter,
                  struct tgsi_full_instruction *inst)
 {
@@ -5401,9 +5513,9 @@ iter_instruction(struct tgsi_iterate_context *iter,
    struct dest_info dinfo = { 0 };
    struct source_info sinfo = { 0 };
    const char *srcs[4];
-   char *dsts[3];
-   char fp64_dsts[3][255];
-   uint instno = ctx->instno++;
+   char *dsts[TGSI_FULL_MAX_DST_REGISTERS];
+   char fp64_dsts[TGSI_FULL_MAX_DST_REGISTERS][255];
+   unsigned instno = ctx->instno++;
    char writemask[6] = "";
    char src_swizzle0[16];
 
@@ -5436,7 +5548,7 @@ iter_instruction(struct tgsi_iterate_context *iter,
       /* GLES doesn't allow invariant specifiers on inputs, but on GL with
        * GLSL < 4.30 it is required to match the output of the previous stage */
       if (!ctx->cfg->use_gles) {
-         for (unsigned i = 0; i < ctx->num_inputs; ++i) {
+         for (uint32_t i = 0; i < ctx->num_inputs; ++i) {
             uint32_t bit_pos = varying_bit_from_semantic_and_index(ctx->inputs[i].name, ctx->inputs[i].sid);
             uint32_t slot = bit_pos / 32;
             uint32_t bit = 1u << (bit_pos & 0x1f);
@@ -5802,7 +5914,7 @@ iter_instruction(struct tgsi_iterate_context *iter,
       } else if (iter->processor.Processor == TGSI_PROCESSOR_TESS_CTRL && ctx->cfg->has_cull_distance) {
          emit_clip_dist_movs(ctx, &ctx->glsl_strbufs);
       } else if (iter->processor.Processor == TGSI_PROCESSOR_TESS_EVAL && ctx->cfg->has_cull_distance) {
-	 if (ctx->so && !ctx->key->gs_present)
+         if (ctx->so && !ctx->key->gs_present)
             emit_so_movs(ctx, &ctx->glsl_strbufs, &ctx->has_clipvertex_so);
          emit_clip_dist_movs(ctx, &ctx->glsl_strbufs);
          if (!ctx->key->gs_present) {
@@ -5841,29 +5953,37 @@ iter_instruction(struct tgsi_iterate_context *iter,
    case TGSI_OPCODE_BRK:
       emit_buf(&ctx->glsl_strbufs, "break;\n");
       break;
-   case TGSI_OPCODE_EMIT: {
-      struct immed *imd = &ctx->imm[(inst->Src[0].Register.Index)];
-      if (ctx->so && ctx->key->gs_present)
-         emit_so_movs(ctx, &ctx->glsl_strbufs, &ctx->has_clipvertex_so);
-      if (ctx->cfg->has_cull_distance && ctx->key->gs.emit_clip_distance)
-         emit_clip_dist_movs(ctx, &ctx->glsl_strbufs);
-      emit_prescale(&ctx->glsl_strbufs);
-      if (imd->val[inst->Src[0].Register.SwizzleX].ui > 0) {
-         ctx->shader_req_bits |= SHADER_REQ_GPU_SHADER5;
-         emit_buff(&ctx->glsl_strbufs, "EmitStreamVertex(%d);\n", imd->val[inst->Src[0].Register.SwizzleX].ui);
-      } else
-         emit_buf(&ctx->glsl_strbufs, "EmitVertex();\n");
-      break;
-   }
-   case TGSI_OPCODE_ENDPRIM: {
-      struct immed *imd = &ctx->imm[(inst->Src[0].Register.Index)];
-      if (imd->val[inst->Src[0].Register.SwizzleX].ui > 0) {
-         ctx->shader_req_bits |= SHADER_REQ_GPU_SHADER5;
-         emit_buff(&ctx->glsl_strbufs, "EndStreamPrimitive(%d);\n", imd->val[inst->Src[0].Register.SwizzleX].ui);
-      } else
-         emit_buf(&ctx->glsl_strbufs, "EndPrimitive();\n");
-      break;
-   }
+   case TGSI_OPCODE_EMIT:
+      if (likely((unsigned) inst->Src[0].Register.Index < MAX_IMMEDIATE)) {
+         struct immed *imd = &ctx->imm[inst->Src[0].Register.Index];
+         if (ctx->so && ctx->key->gs_present)
+            emit_so_movs(ctx, &ctx->glsl_strbufs, &ctx->has_clipvertex_so);
+         if (ctx->cfg->has_cull_distance && ctx->key->gs.emit_clip_distance)
+            emit_clip_dist_movs(ctx, &ctx->glsl_strbufs);
+         emit_prescale(&ctx->glsl_strbufs);
+         if (imd->val[inst->Src[0].Register.SwizzleX].ui > 0) {
+            ctx->shader_req_bits |= SHADER_REQ_GPU_SHADER5;
+            emit_buff(&ctx->glsl_strbufs, "EmitStreamVertex(%d);\n", imd->val[inst->Src[0].Register.SwizzleX].ui);
+         } else
+            emit_buf(&ctx->glsl_strbufs, "EmitVertex();\n");
+         break;
+      } else {
+         virgl_error("Immediate range exceeded, max is %u\n", MAX_IMMEDIATE);
+         return false;
+      }
+   case TGSI_OPCODE_ENDPRIM:
+      if (likely((unsigned) inst->Src[0].Register.Index < MAX_IMMEDIATE)) {
+         struct immed *imd = &ctx->imm[inst->Src[0].Register.Index];
+         if (imd->val[inst->Src[0].Register.SwizzleX].ui > 0) {
+            ctx->shader_req_bits |= SHADER_REQ_GPU_SHADER5;
+            emit_buff(&ctx->glsl_strbufs, "EndStreamPrimitive(%d);\n", imd->val[inst->Src[0].Register.SwizzleX].ui);
+         } else
+            emit_buf(&ctx->glsl_strbufs, "EndPrimitive();\n");
+         break;
+      } else {
+         virgl_error("Immediate range exceeded, max is %u\n", MAX_IMMEDIATE);
+         return false;
+      }
    case TGSI_OPCODE_INTERP_CENTROID:
       emit_buff(&ctx->glsl_strbufs, "%s = %s(%s(vec4(interpolateAtCentroid(%s)%s)));\n", dsts[0], get_string(dinfo.dstconv), get_string(dinfo.dtypeprefix), srcs[0], src_swizzle0);
       ctx->shader_req_bits |= SHADER_REQ_GPU_SHADER5;
@@ -5931,37 +6051,41 @@ iter_instruction(struct tgsi_iterate_context *iter,
    case TGSI_OPCODE_BARRIER:
       emit_buf(&ctx->glsl_strbufs, "barrier();\n");
       break;
-   case TGSI_OPCODE_MEMBAR: {
-      struct immed *imd = &ctx->imm[(inst->Src[0].Register.Index)];
-      uint32_t val = imd->val[inst->Src[0].Register.SwizzleX].ui;
-      uint32_t all_val = (TGSI_MEMBAR_SHADER_BUFFER |
-                          TGSI_MEMBAR_ATOMIC_BUFFER |
-                          TGSI_MEMBAR_SHADER_IMAGE |
-                          TGSI_MEMBAR_SHARED);
-
-      if (val & TGSI_MEMBAR_THREAD_GROUP) {
-         emit_buf(&ctx->glsl_strbufs, "groupMemoryBarrier();\n");
-      } else {
-         if ((val & all_val) == all_val) {
-            emit_buf(&ctx->glsl_strbufs, "memoryBarrier();\n");
-            ctx->shader_req_bits |= SHADER_REQ_IMAGE_LOAD_STORE;
+   case TGSI_OPCODE_MEMBAR:
+      if (likely((unsigned) inst->Src[0].Register.Index < MAX_IMMEDIATE)) {
+         struct immed *imd = &ctx->imm[inst->Src[0].Register.Index];
+         uint32_t val = imd->val[inst->Src[0].Register.SwizzleX].ui;
+         uint32_t all_val = (TGSI_MEMBAR_SHADER_BUFFER |
+                             TGSI_MEMBAR_ATOMIC_BUFFER |
+                             TGSI_MEMBAR_SHADER_IMAGE |
+                             TGSI_MEMBAR_SHARED);
+
+         if (val & TGSI_MEMBAR_THREAD_GROUP) {
+            emit_buf(&ctx->glsl_strbufs, "groupMemoryBarrier();\n");
          } else {
-            if (val & TGSI_MEMBAR_SHADER_BUFFER) {
-               emit_buf(&ctx->glsl_strbufs, "memoryBarrierBuffer();\n");
-            }
-            if (val & TGSI_MEMBAR_ATOMIC_BUFFER) {
-               emit_buf(&ctx->glsl_strbufs, "memoryBarrierAtomicCounter();\n");
-            }
-            if (val & TGSI_MEMBAR_SHADER_IMAGE) {
-               emit_buf(&ctx->glsl_strbufs, "memoryBarrierImage();\n");
-            }
-            if (val & TGSI_MEMBAR_SHARED) {
+            if ((val & all_val) == all_val) {
+               emit_buf(&ctx->glsl_strbufs, "memoryBarrier();\n");
+               ctx->shader_req_bits |= SHADER_REQ_IMAGE_LOAD_STORE;
+            } else {
+               if (val & TGSI_MEMBAR_SHADER_BUFFER) {
+                  emit_buf(&ctx->glsl_strbufs, "memoryBarrierBuffer();\n");
+               }
+               if (val & TGSI_MEMBAR_ATOMIC_BUFFER) {
+                  emit_buf(&ctx->glsl_strbufs, "memoryBarrierAtomicCounter();\n");
+               }
+               if (val & TGSI_MEMBAR_SHADER_IMAGE) {
+                  emit_buf(&ctx->glsl_strbufs, "memoryBarrierImage();\n");
+               }
+               if (val & TGSI_MEMBAR_SHARED) {
                emit_buf(&ctx->glsl_strbufs, "memoryBarrierShared();\n");
+               }
             }
          }
+         break;
+      } else {
+         virgl_error("Immediate range exceeded, max is %u\n", MAX_IMMEDIATE);
+         return false;
       }
-      break;
-   }
    case TGSI_OPCODE_STORE:
       if (ctx->cfg->use_gles) {
          if (!rewrite_1d_image_coordinate(ctx->src_bufs + 1, inst))
@@ -6011,7 +6135,7 @@ iter_instruction(struct tgsi_iterate_context *iter,
       emit_buff(&ctx->glsl_strbufs, "%s = uintBitsToFloat(clock2x32ARB());\n", dsts[0]);
       break;
    default:
-      vrend_printf("failed to convert opcode %d\n", inst->Instruction.Opcode);
+      virgl_warn("Failed to convert opcode %d\n", inst->Instruction.Opcode);
       break;
    }
 
@@ -6030,7 +6154,7 @@ iter_instruction(struct tgsi_iterate_context *iter,
    return true;
 }
 
-static boolean
+static bool
 prolog(struct tgsi_iterate_context *iter)
 {
    struct dump_ctx *ctx = (struct dump_ctx *)iter;
@@ -6177,10 +6301,12 @@ static void emit_header(const struct dump_ctx *ctx, struct vrend_glsl_strbufs *g
 
       if (ctx->prog_type == TGSI_PROCESSOR_VERTEX && ctx->cfg->use_explicit_locations)
          emit_ext(glsl_strbufs, "ARB_explicit_attrib_location", "require");
-      if (ctx->prog_type == TGSI_PROCESSOR_FRAGMENT && fs_emit_layout(ctx))
+      /* Core GLSL 150 already includes fragment coord layouts; avoid extension require on hosts that omit the ARB string (e.g., macOS core GL). */
+      if (ctx->prog_type == TGSI_PROCESSOR_FRAGMENT && fs_emit_layout(ctx) && ctx->glsl_ver_required < 150)
          emit_ext(glsl_strbufs, "ARB_fragment_coord_conventions", "require");
 
-      if (ctx->ubo_used_mask)
+      /* Uniform buffers are core in GLSL 1.40+; only request the ARB extension when targeting older versions. */
+      if (ctx->ubo_used_mask && ctx->glsl_ver_required < 140)
          emit_ext(glsl_strbufs, "ARB_uniform_buffer_object", "require");
 
       if (ctx->num_cull_dist_prop || ctx->key->num_in_cull || ctx->key->num_out_cull)
@@ -6445,7 +6571,7 @@ const char *get_internalformat_string(int virgl_format, enum tgsi_return_type *s
       return "";
    default:
       *stype = TGSI_RETURN_TYPE_UNORM;
-      vrend_printf( "illegal format %d\n", virgl_format);
+      virgl_warn("Illegal format %d\n", virgl_format);
       return "";
    }
 }
@@ -6508,7 +6634,7 @@ static int emit_ios_common(const struct dump_ctx *ctx,
                            struct vrend_glsl_strbufs *glsl_strbufs,
                            uint32_t *shadow_samp_mask)
 {
-   uint i;
+   uint32_t i;
    const char *sname = tgsi_proc_to_prefix(ctx->prog_type);
    int glsl_ver_required = ctx->glsl_ver_required;
 
@@ -6577,7 +6703,7 @@ static int emit_ios_common(const struct dump_ctx *ctx,
          emit_sampler_decl(ctx, glsl_strbufs, shadow_samp_mask, first, range, ctx->samplers + first);
       }
    } else {
-      uint nsamp = util_last_bit(ctx->samplers_used);
+      unsigned nsamp = util_last_bit(ctx->samplers_used);
       for (i = 0; i < nsamp; i++) {
 
          if ((ctx->samplers_used & (1 << i)) == 0)
@@ -6641,7 +6767,7 @@ static void emit_ios_streamout(const struct dump_ctx *ctx,
 {
    if (ctx->so) {
       char outtype[6] = "";
-      for (uint i = 0; i < ctx->so->num_outputs; i++) {
+      for (uint32_t i = 0; i < ctx->so->num_outputs; i++) {
          if (!ctx->write_so_outputs[i])
             continue;
          if (ctx->so->output[i].num_components == 1)
@@ -6707,7 +6833,10 @@ static void emit_ios_indirect_generics_input(const struct dump_ctx *ctx,
       if (size > 1)
          snprintf(array_handle, sizeof(array_handle), "[%d]", size);
 
-      assert(size < 256 && size >= 0);
+      if (size >= 256 || size < 0) {
+         set_buf_error(glsl_strbufs);
+         return;
+      }
 
       if (prefer_generic_io_block(ctx, io_in)) {
 
@@ -6768,14 +6897,21 @@ emit_ios_generic(const struct dump_ctx *ctx,
                 postfix);
 
       if (io->name == TGSI_SEMANTIC_GENERIC) {
-         assert(io->sid < 64);
+         if (io->sid >= 64) {
+            set_buf_error(glsl_strbufs);
+            return;
+         }
+
          if (iot == io_in) {
             generic_ios->match.inputs_emitted_mask |= 1ull << io->sid;
          } else {
             generic_ios->match.outputs_emitted_mask |= 1ull << io->sid;
          }
       } else if (io->name == TGSI_SEMANTIC_TEXCOORD) {
-         assert(io->sid < 8);
+         if (io->sid >= 8)  {
+            set_buf_error(glsl_strbufs);
+            return;
+         }
          if (iot == io_in) {
             texcoord_ios->match.inputs_emitted_mask |= 1ull << io->sid;
          } else {
@@ -6819,14 +6955,20 @@ emit_ios_generic(const struct dump_ctx *ctx,
 
          uint64_t mask = ((1ull << array_size) - 1) << io->sid;
          if (io->name == TGSI_SEMANTIC_GENERIC) {
-            assert(io->sid + array_size < 64);
+            if (io->sid + array_size >= 64)  {
+               set_buf_error(glsl_strbufs);
+               return;
+            }
             if (iot == io_in) {
                generic_ios->match.inputs_emitted_mask |= mask;
             } else {
                generic_ios->match.outputs_emitted_mask |= mask;
             }
          } else if (io->name == TGSI_SEMANTIC_TEXCOORD) {
-            assert(io->sid + array_size < 8);
+            if (io->sid + array_size > 8)  {
+               set_buf_error(glsl_strbufs);
+               return;
+            }
             if (iot == io_in) {
                texcoord_ios->match.inputs_emitted_mask |= mask;
             } else {
@@ -6909,7 +7051,7 @@ emit_ios_generic_outputs(const struct dump_ctx *ctx,
 
          if (ctx->outputs[i].name == TGSI_SEMANTIC_COLOR) {
             if (ctx->outputs[i].sid >= 64) {
-               vrend_printf("Number of output id exceeded, max is 64\n");
+               virgl_error("Number of output id exceeded, max is 64\n");
                set_buf_error(glsl_strbufs);
                return;
             }
@@ -6920,7 +7062,7 @@ emit_ios_generic_outputs(const struct dump_ctx *ctx,
 
          if (ctx->outputs[i].name == TGSI_SEMANTIC_BCOLOR) {
             if (ctx->outputs[i].sid >= 64) {
-               vrend_printf("Number of output id exceeded, max is 64\n");
+               virgl_error("Number of output id exceeded, max is 64\n");
                set_buf_error(glsl_strbufs);
                return;
             }
@@ -7154,7 +7296,7 @@ static void emit_ios_fs(const struct dump_ctx *ctx,
             if (!prefix)
                prefix = "";
             auxprefix = get_aux_string(ctx->inputs[i].location);
-            *interp_input_mask |= 1 << i;
+            *interp_input_mask |= UINT64_C(1) << i;
          }
 
          char prefixes[64];
@@ -7193,7 +7335,7 @@ static void emit_ios_fs(const struct dump_ctx *ctx,
                emit_hdrf(glsl_strbufs, "layout (location=%d%s) inout highp %s fsout_c%d;\n", i, noncoherent, type, i);
             } else
                emit_hdrf(glsl_strbufs, "layout (location=%d) out %s fsout_c%d;\n", i,
-			 type, i);
+                         type, i);
          } else
             emit_hdrf(glsl_strbufs, "out %s fsout_c%d;\n", type, i);
       }
@@ -7345,7 +7487,7 @@ static void emit_ios_geom(const struct dump_ctx *ctx,
          if (ctx->outputs[i].name == TGSI_SEMANTIC_GENERIC ||
              ctx->outputs[i].name == TGSI_SEMANTIC_COLOR ||
              ctx->outputs[i].name == TGSI_SEMANTIC_BCOLOR) {
-            *interp_input_mask |= 1 << i;
+            *interp_input_mask |= UINT64_C(1) << i;
          }
 
          emit_hdrf(glsl_strbufs, "layout (stream = %d) %s%s%sout vec4 %s;\n", ctx->outputs[i].stream, prefix,
@@ -7569,7 +7711,7 @@ static int emit_ios(const struct dump_ctx *ctx,
    int glsl_ver_required = ctx->glsl_ver_required;
 
    if (ctx->so && ctx->so->num_outputs >= PIPE_MAX_SO_OUTPUTS) {
-      vrend_printf( "Num outputs exceeded, max is %u\n", PIPE_MAX_SO_OUTPUTS);
+      virgl_error("Num outputs exceeded, max is %u\n", PIPE_MAX_SO_OUTPUTS);
       set_hdr_error(glsl_strbufs);
       return glsl_ver_required;
    }
@@ -7594,7 +7736,7 @@ static int emit_ios(const struct dump_ctx *ctx,
       emit_ios_cs(ctx, glsl_strbufs);
       break;
    default:
-      vrend_printf("Unknown shader processor %d\n", ctx->prog_type);
+      virgl_error("Unknown shader processor %d\n", ctx->prog_type);
       set_hdr_error(glsl_strbufs);
       return glsl_ver_required;
    }
@@ -7619,7 +7761,7 @@ static int emit_ios(const struct dump_ctx *ctx,
    return glsl_ver_required;
 }
 
-static boolean fill_fragment_interpolants(const struct dump_ctx *ctx, struct vrend_fs_shader_info *fs_info)
+static bool fill_fragment_interpolants(const struct dump_ctx *ctx, struct vrend_fs_shader_info *fs_info)
 {
    uint32_t i, index = 0;
 
@@ -7636,7 +7778,7 @@ static boolean fill_fragment_interpolants(const struct dump_ctx *ctx, struct vre
    return true;
 }
 
-static boolean fill_interpolants(const struct dump_ctx *ctx, struct vrend_variable_shader_info *sinfo)
+static bool fill_interpolants(const struct dump_ctx *ctx, struct vrend_variable_shader_info *sinfo)
 {
    if (!ctx->interp_input_mask)
       return true;
@@ -7646,7 +7788,7 @@ static boolean fill_interpolants(const struct dump_ctx *ctx, struct vrend_variab
    return fill_fragment_interpolants(ctx, &sinfo->fs_info);
 }
 
-static boolean analyze_instruction(struct tgsi_iterate_context *iter,
+static bool analyze_instruction(struct tgsi_iterate_context *iter,
                                    struct tgsi_full_instruction *inst)
 {
    struct dump_ctx *ctx = (struct dump_ctx *)iter;
@@ -7663,7 +7805,7 @@ static boolean analyze_instruction(struct tgsi_iterate_context *iter,
       for (int i = 0; i < inst->Instruction.NumSrcRegs; ++i) {
          if (inst->Src[i].Register.File == TGSI_FILE_INPUT) {
             int idx = inst->Src[i].Register.Index;
-            for (unsigned j = 0; j < ctx->num_inputs; ++j) {
+            for (uint32_t j = 0; j < ctx->num_inputs; ++j) {
                if (ctx->inputs[j].first <= idx && ctx->inputs[j].last >= idx &&
                    ctx->inputs[j].name == TGSI_SEMANTIC_CLIPDIST) {
                   ctx->fs_uses_clipdist_input = true;
@@ -7706,7 +7848,7 @@ static void fill_sinfo(const struct dump_ctx *ctx, struct vrend_shader_info *sin
    sinfo->fog_input_mask = ctx->fog_input_mask;
    sinfo->fog_output_mask = ctx->fog_output_mask;
 
-   sinfo->ssbo_used_mask = ctx->ssbo_used_mask >> ctx->ssbo_first_binding;
+   sinfo->ssbo_used_mask = ctx->ssbo_used_mask >> (ctx->ssbo_first_binding != UINT32_MAX ? ctx->ssbo_first_binding : 0);
    sinfo->ssbo_binding_offset  = ctx->key->ssbo_binding_offset;
 
    sinfo->ssbo_last_binding = ctx->key->ssbo_binding_offset + ctx->ssbo_last_binding -
@@ -7733,7 +7875,7 @@ static void fill_sinfo(const struct dump_ctx *ctx, struct vrend_shader_info *sin
 
    if (sinfo->so_names || ctx->so_names) {
       if (sinfo->so_names) {
-         for (unsigned i = 0; i < sinfo->so_info.num_outputs; ++i)
+         for (uint32_t i = 0; i < sinfo->so_info.num_outputs; ++i)
             free(sinfo->so_names[i]);
          free(sinfo->so_names);
       }
@@ -7743,7 +7885,7 @@ static void fill_sinfo(const struct dump_ctx *ctx, struct vrend_shader_info *sin
     * to the next shader stage. mesa/tgsi doesn't provide this information for
     * TCS, TES, and GEOM shaders.
     */
-   for(unsigned i = 0; i < ctx->num_outputs; i++) {
+   for(uint32_t i = 0; i < ctx->num_outputs; i++) {
       if (ctx->prog_type == TGSI_PROCESSOR_FRAGMENT) {
          if (ctx->outputs[i].name == TGSI_SEMANTIC_COLOR)
             sinfo->fs_output_layout[i] = ctx->outputs[i].sid;
@@ -7765,7 +7907,7 @@ static void fill_sinfo(const struct dump_ctx *ctx, struct vrend_shader_info *sin
    sinfo->in_generic_emitted_mask = ctx->generic_ios.match.inputs_emitted_mask;
    sinfo->in_texcoord_emitted_mask = ctx->texcoord_ios.match.inputs_emitted_mask;
 
-   for (unsigned i = 0; i < ctx->num_outputs; ++i) {
+   for (uint32_t i = 0; i < ctx->num_outputs; ++i) {
       if (ctx->outputs[i].invariant) {
          uint32_t bit_pos = varying_bit_from_semantic_and_index(ctx->outputs[i].name, ctx->outputs[i].sid);
          uint32_t slot = bit_pos / 32;
@@ -7777,7 +7919,7 @@ static void fill_sinfo(const struct dump_ctx *ctx, struct vrend_shader_info *sin
 
    if (ctx->guest_sent_io_arrays) {
       sinfo->output_arrays.num_arrays = 0;
-      for (unsigned i = 0; i < ctx->num_outputs; ++i) {
+      for (uint32_t i = 0; i < ctx->num_outputs; ++i) {
          const struct vrend_shader_io *io = &ctx->outputs[i];
          if (io->array_id  > 0) {
             struct vrend_shader_io_array *array =
@@ -7855,7 +7997,7 @@ struct sso_scan_ctx {
    bool unsupported_io;
 };
 
-static boolean
+static bool
 iter_prop_for_separable(struct tgsi_iterate_context *iter,
           struct tgsi_full_property *prop)
 {
@@ -7866,7 +8008,7 @@ iter_prop_for_separable(struct tgsi_iterate_context *iter,
    return true;
 }
 
-static boolean
+static bool
 iter_decl_for_overlap(struct tgsi_iterate_context *iter,
                       struct tgsi_full_declaration *decl)
 {
@@ -7950,7 +8092,7 @@ bool vrend_convert_shader(const struct vrend_context *rctx,
                           struct vrend_strarray *shader)
 {
    struct dump_ctx ctx;
-   boolean bret;
+   bool bret;
 
    memset(&ctx, 0, sizeof(struct dump_ctx));
    ctx.cfg = cfg;
@@ -8001,9 +8143,18 @@ bool vrend_convert_shader(const struct vrend_context *rctx,
       goto fail;
 
    /* if we are in core profile mode we should use GLSL 1.40 */
-   if (cfg->use_core_profile && cfg->glsl_version >= 140)
+   if (cfg->glsl_version >= 140)
       ctx.glsl_ver_required = require_glsl_ver(&ctx, 140);
 
+   if (ctx.iter.processor.Processor == TGSI_PROCESSOR_GEOMETRY ||
+       key->gs_present)
+      ctx.glsl_ver_required = require_glsl_ver(&ctx, 140);
+
+   if (ctx.iter.processor.Processor == TGSI_PROCESSOR_TESS_EVAL ||
+       ctx.iter.processor.Processor == TGSI_PROCESSOR_TESS_CTRL ||
+       key->tes_present || key->tcs_present)
+         ctx.glsl_ver_required = require_glsl_ver(&ctx, 150);
+
    if (sinfo->so_info.num_outputs) {
       ctx.so = &sinfo->so_info;
       ctx.so_names = calloc(sinfo->so_info.num_outputs, sizeof(char *));
@@ -8030,6 +8181,12 @@ bool vrend_convert_shader(const struct vrend_context *rctx,
    if (bret == false)
       goto fail;
 
+   if (ctx.shader_req_bits & SHADER_REQ_INTS)
+      ctx.glsl_ver_required = require_glsl_ver(&ctx, 150);
+
+   if (ctx.shader_req_bits & SHADER_REQ_FP64)
+      ctx.glsl_ver_required = require_glsl_ver(&ctx, 150);
+
    /* If we need a sysvalue UBO then we require GLSL 1.40 */
    if (ctx.glsl_strbufs.required_sysval_uniform_decls)
       ctx.glsl_ver_required = require_glsl_ver(&ctx, 140);
@@ -8102,7 +8259,7 @@ bool vrend_convert_shader(const struct vrend_context *rctx,
    return false;
 }
 
-static boolean
+static bool
 iter_vs_declaration(struct tgsi_iterate_context *iter,
                     struct tgsi_full_declaration *decl)
 {
@@ -8111,7 +8268,7 @@ iter_vs_declaration(struct tgsi_iterate_context *iter,
    const char *shader_in_prefix = "vso";
    const char *shader_out_prefix = "tco";
    const char *name_prefix = "";
-   unsigned i;
+   uint32_t i;
 
    // Generate a shader that passes through all VS outputs
    if (decl->Declaration.File == TGSI_FILE_OUTPUT) {
@@ -8215,7 +8372,7 @@ bool vrend_shader_create_passthrough_tcs(const struct vrend_context *rctx,
                                          const float tess_factors[6],
                                          struct vrend_shader_info *sinfo,
                                          struct vrend_strarray *shader,
-                                         int vertices_per_patch)
+                                         uint8_t vertices_per_patch)
 {
    struct dump_ctx ctx;
 
@@ -8251,7 +8408,7 @@ bool vrend_shader_create_passthrough_tcs(const struct vrend_context *rctx,
 
    emit_buf(&ctx.glsl_strbufs, "void main() {\n");
 
-   for (unsigned int i = 0; i < ctx.num_inputs; ++i) {
+   for (uint32_t i = 0; i < ctx.num_inputs; ++i) {
       const char *out_prefix = "";
       const char *in_prefix = "";
 
