diff -Naur virglrenderer-upstream/server/render_socket.c virglrenderer-patched/server/render_socket.c
--- virglrenderer-upstream/server/render_socket.c	2026-01-09 14:47:36
+++ virglrenderer-patched/server/render_socket.c	2026-01-09 15:10:16
@@ -4,6 +4,7 @@
  */
 
 #include "render_socket.h"
+#include <fcntl.h>
 
 #include <errno.h>
 #include <sys/socket.h>
@@ -28,11 +29,23 @@
 bool
 render_socket_pair(int out_fds[static 2])
 {
+#if defined(__APPLE__)
+   // macOS doesn't support SOCK_CLOEXEC, manually set FD_CLOEXEC
+   int ret = socketpair(AF_UNIX, SOCK_SEQPACKET, 0, out_fds);
+   if (ret) {
+      render_log("failed to create socket pair");
+      return false;
+   }
+   // Set FD_CLOEXEC on both sockets
+   fcntl(out_fds[0], F_SETFD, FD_CLOEXEC);
+   fcntl(out_fds[1], F_SETFD, FD_CLOEXEC);
+#else
    int ret = socketpair(AF_UNIX, SOCK_SEQPACKET | SOCK_CLOEXEC, 0, out_fds);
    if (ret) {
       render_log("failed to create socket pair");
       return false;
    }
+#endif
 
    return true;
 }
@@ -80,7 +93,12 @@
 render_socket_recvmsg(struct render_socket *socket, struct msghdr *msg, size_t *out_size)
 {
    do {
+#if defined(__APPLE__)
+      // macOS doesn't support MSG_CMSG_CLOEXEC
+      const ssize_t s = recvmsg(socket->fd, msg, 0);
+#else
       const ssize_t s = recvmsg(socket->fd, msg, MSG_CMSG_CLOEXEC);
+#endif
       if (unlikely(s <= 0)) {
          if (!s)
             return false;
diff -Naur virglrenderer-upstream/server/render_worker.c virglrenderer-patched/server/render_worker.c
--- virglrenderer-upstream/server/render_worker.c	2026-01-09 14:47:36
+++ virglrenderer-patched/server/render_worker.c	2026-01-09 15:23:39
@@ -24,10 +24,15 @@
 #include <errno.h>
 #include <fcntl.h>
 #include <signal.h>
+#if defined(__APPLE__)
+// macOS doesn't have signalfd, use alternative implementation
+#include <sys/select.h>
+#else
 #include <sys/signalfd.h>
 #include <sys/types.h>
 #include <sys/wait.h>
 #include <threads.h>
+#endif
 #include <unistd.h>
 
 struct minijail;
@@ -182,11 +187,17 @@
       return -1;
    }
 
+#if defined(__APPLE__)
+   // macOS doesn't support signalfd, use -1 to indicate polling mode
+   sigprocmask(SIG_BLOCK, &set, NULL);
+   int fd = -1;
+#else
    int fd = signalfd(-1, &set, SFD_NONBLOCK | SFD_CLOEXEC);
    if (fd < 0) {
       render_log("failed to create signalfd");
       return -1;
    }
+#endif
 
    if (sigprocmask(SIG_BLOCK, &set, NULL)) {
       render_log("failed to call sigprocmask");
diff -Naur virglrenderer-upstream/src/proxy/proxy_socket.c virglrenderer-patched/src/proxy/proxy_socket.c
--- virglrenderer-upstream/src/proxy/proxy_socket.c	2026-01-09 14:47:36
+++ virglrenderer-patched/src/proxy/proxy_socket.c	2026-01-09 15:21:06
@@ -100,7 +100,12 @@
 proxy_socket_recvmsg(struct proxy_socket *socket, struct msghdr *msg)
 {
    do {
+#if defined(__APPLE__)
+      // macOS doesn't support MSG_CMSG_CLOEXEC
+      const ssize_t s = recvmsg(socket->fd, msg, 0);
+#else
       const ssize_t s = recvmsg(socket->fd, msg, MSG_CMSG_CLOEXEC);
+#endif
       if (unlikely(s < 0)) {
          if (errno == EAGAIN || errno == EINTR)
             continue;
diff -Naur virglrenderer-upstream/src/venus/vkr_ring.c virglrenderer-patched/src/venus/vkr_ring.c
--- virglrenderer-upstream/src/venus/vkr_ring.c	2026-01-09 14:47:36
+++ virglrenderer-patched/src/venus/vkr_ring.c	2026-01-09 15:07:52
@@ -13,6 +13,17 @@
 
 #include "vkr_context.h"
 
+#if defined(__APPLE__)
+// macOS doesn't have clock_nanosleep, use nanosleep instead
+static inline int clock_nanosleep(clockid_t clock_id, int flags,
+                                   const struct timespec *request,
+                                   struct timespec *remain)
+{
+   struct timespec req = *request;
+   return nanosleep(&req, remain);
+}
+#endif
+
 static inline void *
 get_resource_pointer(const struct vkr_resource *res, size_t offset)
 {
diff -Naur virglrenderer-upstream/src/virglrenderer.c virglrenderer-patched/src/virglrenderer.c
--- virglrenderer-upstream/src/virglrenderer.c	2026-01-09 14:47:36
+++ virglrenderer-patched/src/virglrenderer.c	2026-01-09 15:09:13
@@ -178,11 +178,37 @@
 void virgl_renderer_fill_caps(uint32_t set, uint32_t version,
                               void *caps)
 {
+   if (getenv("VIRGL_DEBUG_CAPS")) {
+      fprintf(stderr, "DEBUG virgl_renderer_fill_caps: set=%u version=%u\n", set, version);
+      fflush(stderr);
+   }
+   
    switch (set) {
    case VIRTGPU_DRM_CAPSET_VIRGL:
    case VIRTGPU_DRM_CAPSET_VIRGL2:
-      if (state.vrend_initialized)
+      if (state.vrend_initialized) {
          vrend_renderer_fill_caps(set, version, (union virgl_caps *)caps);
+         if (getenv("VIRGL_DEBUG_CAPS")) {
+            union virgl_caps *vcaps = (union virgl_caps *)caps;
+            fprintf(stderr, "DEBUG virgl_renderer_fill_caps returning to QEMU:\n");
+            fprintf(stderr, "  max_version=%u\n", vcaps->max_version);
+            fprintf(stderr, "  v1.glsl_level=%u at address %p\n", vcaps->v1.glsl_level, (void*)&vcaps->v1.glsl_level);
+            fprintf(stderr, "  v2.v1.glsl_level=%u at address %p\n", vcaps->v2.v1.glsl_level, (void*)&vcaps->v2.v1.glsl_level);
+            fprintf(stderr, "  caps base address=%p\n", (void*)caps);
+            
+            /* Calculate actual offset */
+            size_t glsl_offset = (uint8_t*)&vcaps->v1.glsl_level - (uint8_t*)caps;
+            fprintf(stderr, "  glsl_level offset from caps base: %zu bytes\n", glsl_offset);
+            
+            /* Dump bytes at that offset */
+            uint8_t *bytes = (uint8_t *)caps;
+            fprintf(stderr, "  Bytes at glsl_level offset (%zu-%zu): ", glsl_offset, glsl_offset+7);
+            for (size_t i = glsl_offset; i < glsl_offset + 8 && i < 1024; i++) 
+               fprintf(stderr, "%02x ", bytes[i]);
+            fprintf(stderr, "\n");
+            fflush(stderr);
+         }
+      }
       break;
    case VIRTGPU_DRM_CAPSET_VENUS:
       if (state.proxy_initialized)
@@ -246,10 +272,7 @@
    case VIRTGPU_DRM_CAPSET_DRM:
       if (!state.drm_initialized)
          return EINVAL;
-      if (state.cbs->version >= 2 && state.cbs->get_drm_fd)
-         ctx = drm_renderer_create(nlen, name, state.cbs->get_drm_fd(state.cookie));
-      else
-         ctx = drm_renderer_create(nlen, name, -1);
+      ctx = drm_renderer_create(nlen, name);
       break;
    default:
       return EINVAL;
@@ -557,6 +580,25 @@
    return 0;
 }
 
+int virgl_renderer_borrow_texture_for_scanout(int res_handle,
+                                              struct virgl_renderer_resource_info_ext *info)
+{
+   TRACE_FUNC();
+   struct virgl_resource *res = virgl_resource_lookup(res_handle);
+
+   if (!res)
+      return EINVAL;
+   if (!info)
+      return EINVAL;
+
+   if (!res->pipe_resource)
+      return 0;
+
+   vrend_renderer_borrow_texture_for_scanout(res->pipe_resource);
+
+   return virgl_renderer_resource_get_info_ext(res_handle, info);
+}
+
 void virgl_renderer_get_cap_set(uint32_t cap_set, uint32_t *max_ver,
                                 uint32_t *max_size)
 {
@@ -917,8 +959,6 @@
          renderer_flags |= VREND_USE_COMPAT_CONTEXT;
       if (flags & VIRGL_RENDERER_USE_GLES)
          renderer_flags |= VREND_USE_GLES;
-      if (flags & VIRGL_RENDERER_VENUS)
-         renderer_flags |= VREND_USE_GBM_LAYOUT;
 
       ret = vrend_renderer_init(&vrend_cbs, renderer_flags);
       if (ret) {
@@ -1233,33 +1273,23 @@
    int ret = 0;
    void *map = NULL;
    uint64_t map_size = 0;
-   struct virgl_context *ctx = NULL;
    struct virgl_resource *res = virgl_resource_lookup(res_handle);
    if (!res || res->mapped)
       return -EINVAL;
 
    if (res->pipe_resource) {
       ret = vrend_renderer_resource_map(res->pipe_resource, &map, &map_size);
-      if (!ret) {
+      if (!ret)
          res->map_size = map_size;
-         res->mapped_from_pipe_resource = true;
-      }
    } else {
       enum virgl_resource_fd_type fd_type = res->fd_type;
       enum virgl_resource_fd_type export_fd_type = res->fd_type;
       int fd = res->fd;
 
-      if (fd_type == VIRGL_RESOURCE_OPAQUE_HANDLE) {
-         ctx = virgl_context_lookup(res->opaque_handle_context_id);
-         if (!ctx)
-            return -EINVAL;
+      /* Create a transient dmabuf. */
+      if (fd_type == VIRGL_RESOURCE_OPAQUE_HANDLE)
+         export_fd_type = virgl_resource_export_fd(res, &fd);
 
-         if (!ctx->resource_map) {
-            /* Create a transient dmabuf. */
-            export_fd_type = virgl_resource_export_fd(res, &fd);
-         }
-      }
-
       switch (export_fd_type) {
       case VIRGL_RESOURCE_FD_DMABUF:
       case VIRGL_RESOURCE_FD_SHM:
@@ -1270,9 +1300,6 @@
          ret = vkr_allocator_resource_map(res, &map, &map_size);
          break;
       case VIRGL_RESOURCE_OPAQUE_HANDLE:
-         map = ctx->resource_map(ctx, res, NULL, PROT_WRITE | PROT_READ, MAP_SHARED);
-         map_size = res->map_size;
-         break;
       case VIRGL_RESOURCE_FD_INVALID:
          /* Avoid a default case so that -Wswitch will tell us at compile time
           * if a new virgl resource type is added without being handled here.
@@ -1293,59 +1320,6 @@
    return ret;
 }
 
-int virgl_renderer_resource_map_fixed(uint32_t res_handle, void *addr)
-{
-   void *map = NULL;
-   struct virgl_context *ctx = NULL;
-   struct virgl_resource *res = virgl_resource_lookup(res_handle);
-   enum virgl_resource_fd_type fd_type = res->fd_type;
-   enum virgl_resource_fd_type export_fd_type = res->fd_type;
-   int fd = res->fd;
-
-   if (!res)
-      return -EINVAL;
-
-   if (fd_type == VIRGL_RESOURCE_OPAQUE_HANDLE) {
-      ctx = virgl_context_lookup(res->opaque_handle_context_id);
-      if (!ctx)
-         return -EINVAL;
-
-      if (!ctx->resource_map) {
-         /* Create a transient dmabuf. */
-         export_fd_type = virgl_resource_export_fd(res, &fd);
-      }
-   }
-
-   switch (export_fd_type) {
-      case VIRGL_RESOURCE_FD_DMABUF:
-      case VIRGL_RESOURCE_FD_SHM:
-         map = mmap(addr, res->map_size, PROT_WRITE | PROT_READ,
-                    MAP_FIXED | MAP_SHARED, fd, 0);
-         break;
-      case VIRGL_RESOURCE_OPAQUE_HANDLE:
-         map = ctx->resource_map(ctx, res, addr, PROT_WRITE | PROT_READ,
-                                 MAP_FIXED | MAP_SHARED);
-         break;
-      case VIRGL_RESOURCE_FD_OPAQUE:
-      case VIRGL_RESOURCE_FD_INVALID:
-         /* Avoid a default case so that -Wswitch will tell us at compile time
-          * if a new virgl resource type is added without being handled here.
-          */
-      break;
-   }
-
-   if (export_fd_type != fd_type)
-      close(fd);
-
-   if (!map)
-      return -EOPNOTSUPP;
-
-   if (map == MAP_FAILED)
-      return -EINVAL;
-
-   return 0;
-}
-
 int virgl_renderer_resource_unmap(uint32_t res_handle)
 {
    TRACE_FUNC();
@@ -1354,8 +1328,7 @@
    if (!res || !res->mapped)
       return -EINVAL;
 
-   if (res->mapped_from_pipe_resource) {
-      assert(res->pipe_resource);
+   if (res->pipe_resource) {
       ret = vrend_renderer_resource_unmap(res->pipe_resource);
    } else {
       switch (res->fd_type) {
diff -Naur virglrenderer-upstream/src/virglrenderer.h virglrenderer-patched/src/virglrenderer.h
--- virglrenderer-upstream/src/virglrenderer.h	2026-01-09 14:47:36
+++ virglrenderer-patched/src/virglrenderer.h	2026-01-09 15:09:13
@@ -380,6 +380,9 @@
 VIRGL_EXPORT int virgl_renderer_resource_get_info_ext(int res_handle,
                                                       struct virgl_renderer_resource_info_ext *info);
 
+VIRGL_EXPORT int virgl_renderer_borrow_texture_for_scanout(int res_handle,
+                                                           struct virgl_renderer_resource_info_ext *info);
+
 VIRGL_EXPORT void virgl_renderer_cleanup(void *cookie);
 
 /* reset the rendererer - destroy all contexts and resource */
@@ -459,18 +462,6 @@
 
 VIRGL_EXPORT void virgl_renderer_context_poll(uint32_t ctx_id); /* force fences */
 VIRGL_EXPORT int virgl_renderer_context_get_poll_fd(uint32_t ctx_id);
-
-/* Map a resource to an specific userspace address. If successful, the
- * mapping is owned by the caller and is its responsibility to unmap
- * the resource by its own means (i.e. overriding the map with
- * anonymous memory or calling munmap).
- *
- * Returns -EOPNOTSUPP if mapping the resource using this mechanism is
- * not supported. In that case, you can still try mapping the resource
- * using virgl_renderer_resource_map().
- */
-VIRGL_EXPORT int
-virgl_renderer_resource_map_fixed(uint32_t res_handle, void *addr);
 
 /*
  * These are unstable APIs for development only. Use these for development/testing purposes
diff -Naur virglrenderer-upstream/src/vrend/vrend_blitter.h virglrenderer-patched/src/vrend/vrend_blitter.h
--- virglrenderer-upstream/src/vrend/vrend_blitter.h	2026-01-09 14:47:36
+++ virglrenderer-patched/src/vrend/vrend_blitter.h	2026-01-09 15:09:13
@@ -35,6 +35,12 @@
    "%s"                                         \
 
 #define FS_HEADER_GLES                             \
+   "#version 300 es\n"                          \
+   "// Blitter\n"                               \
+   "%s"                                         \
+   "precision mediump float;\n"                 \
+
+#define FS_HEADER_GLES_MS                       \
    "#version 310 es\n"                          \
    "// Blitter\n"                               \
    "%s"                                         \
@@ -52,6 +58,11 @@
    "// Blitter\n"                               \
 
 #define HEADER_GLES                             \
+   "#version 300 es\n"                          \
+   "// Blitter\n"                               \
+   "precision mediump float;\n"                 \
+
+#define HEADER_GLES_MS                          \
    "#version 310 es\n"                          \
    "// Blitter\n"                               \
    "precision mediump float;\n"                 \
@@ -145,7 +156,7 @@
    "}\n"
 
 #define FS_TEXFETCH_COL_MSAA_GL FS_HEADER_GL FS_TEXFETCH_COL_MSAA_BODY
-#define FS_TEXFETCH_COL_MSAA_GLES FS_HEADER_GLES FS_TEXFETCH_COL_MSAA_BODY
+#define FS_TEXFETCH_COL_MSAA_GLES FS_HEADER_GLES_MS FS_TEXFETCH_COL_MSAA_BODY
 #define FS_TEXFETCH_COL_MSAA_ARRAY_GLES FS_HEADER_GLES_MS_ARRAY FS_TEXFETCH_COL_MSAA_BODY
 
 #define FS_TEXFETCH_DS_BODY                             \
@@ -178,7 +189,7 @@
 struct vrend_resource;
 struct vrend_blit_info;
 #define FS_TEXFETCH_DS_MSAA_GL HEADER_GL FS_TEXFETCH_DS_MSAA_BODY
-#define FS_TEXFETCH_DS_MSAA_GLES HEADER_GLES FS_TEXFETCH_DS_MSAA_BODY_GLES
+#define FS_TEXFETCH_DS_MSAA_GLES HEADER_GLES_MS FS_TEXFETCH_DS_MSAA_BODY_GLES
 #define FS_TEXFETCH_DS_MSAA_ARRAY_GLES HEADER_GLES_MS_ARRAY FS_TEXFETCH_DS_MSAA_BODY_GLES
 
 /* implement blitting using OpenGL. */
diff -Naur virglrenderer-upstream/src/vrend/vrend_decode.c virglrenderer-patched/src/vrend/vrend_decode.c
--- virglrenderer-upstream/src/vrend/vrend_decode.c	2026-01-09 14:47:36
+++ virglrenderer-patched/src/vrend/vrend_decode.c	2026-01-09 15:09:13
@@ -888,6 +888,17 @@
       return EINVAL;
    }
 
+   /* Surface GL errors with object metadata to pinpoint bad creations. */
+   if (!vrend_check_no_error(ctx) && ret == 0) {
+      virgl_error("GL error during CREATE_OBJECT type=%s handle=0x%x len=%u\n",
+                  vrend_get_object_type_name(obj_type), handle, length);
+      /* Dump a small slice of the payload for quick diagnosis. */
+      for (uint32_t i = 0; i < length && i < 12; i++) {
+         virgl_error("  dword[%u]=0x%x\n", i, buf[i]);
+      }
+      ret = EINVAL;
+   }
+
    return ret;
 }
 
@@ -1751,18 +1762,6 @@
    return 0;
 }
 
-static int vrend_decode_get_pipe_resource_layout(struct vrend_context *ctx, const uint32_t *buf, uint32_t length)
-{
-   TRACE_FUNC();
-   if (length < VIRGL_RESOURCE_LAYOUT_SIZE)
-      return EINVAL;
-
-   uint32_t out_handle = get_buf_entry(buf, VIRGL_RESOURCE_LAYOUT_HANDLE_OUT);
-   uint32_t target_handle = get_buf_entry(buf, VIRGL_RESOURCE_LAYOUT_HANDLE_TARGET);
-
-   return vrend_renderer_pipe_resource_get_layout(ctx, out_handle, target_handle);
-}
-
 #ifdef ENABLE_VIDEO
 /* video codec related functions */
 
@@ -2030,7 +2029,6 @@
    [VIRGL_CCMD_ENCODE_BITSTREAM] = vrend_unsupported,
    [VIRGL_CCMD_END_FRAME] = vrend_unsupported,
 #endif
-   [VIRGL_CCMD_GET_PIPE_RESOURCE_LAYOUT] = vrend_decode_get_pipe_resource_layout,
 };
 
 static void dump_command_stream_to_file(const void *buffer, size_t size)
@@ -2095,9 +2093,22 @@
 
       TRACE_SCOPE_SLOW(vrend_get_comand_name(cmd));
 
+      /* If video is disabled at runtime, drop video commands quietly to avoid
+       * noisy errors from guest probes (e.g., gst-plugin-scan).
+       */
+      if (!vrend_renderer_video_available() &&
+          cmd >= VIRGL_CCMD_CREATE_VIDEO_CODEC && cmd <= VIRGL_CCMD_END_FRAME) {
+         continue;
+      }
+
       ret = decode_table[cmd](gdctx->grctx, buf, len);
-      if (!vrend_check_no_error(gdctx->grctx) && !ret)
+      if (!vrend_check_no_error(gdctx->grctx) && !ret) {
+         /* Surface the offending command when a GL error is observed. */
+         virgl_error("GL error after %s (ctx %d cmd=0x%x len=%u offset=%u)\n",
+                     vrend_get_comand_name(cmd), gdctx->base.ctx_id,
+                     cmd, len, cur_offset);
          ret = EINVAL;
+      }
       if (ret) {
          virgl_error("context %d failed to dispatch %s: %d\n",
                gdctx->base.ctx_id, vrend_get_comand_name(cmd), ret);
diff -Naur virglrenderer-upstream/src/vrend/vrend_formats.c virglrenderer-patched/src/vrend/vrend_formats.c
--- virglrenderer-upstream/src/vrend/vrend_formats.c	2026-01-09 14:47:36
+++ virglrenderer-patched/src/vrend/vrend_formats.c	2026-01-09 15:09:13
@@ -716,48 +716,183 @@
    assert(glGetError() == GL_NO_ERROR &&
           "Stale error state detected, please check for failures in initialization");
 
+   /* glTexStorage2DMultisample availability check with graceful downgrade:
+    * 
+    * glTexStorage2DMultisample requires:
+    *   - OpenGL 4.3+ or GL_ARB_texture_storage_multisample (desktop GL)
+    *   - OpenGL ES 3.1+ (mobile/ANGLE)
+    * 
+    * Fallback alternatives available on older versions:
+    *   - glTexImage2DMultisample: GL 3.2+ / ES 3.1+ (works on GL 4.1 Core)
+    *   - glRenderbufferStorageMultisample: GL 3.0+ / ES 3.0+ (works on ANGLE)
+    * 
+    * We'll use glTexStorage2DMultisample if available, otherwise fall back to
+    * glTexImage2DMultisample for proper MSAA capability testing. */
+   
+   const char *renderer = (const char *)glGetString(GL_RENDERER);
+   const char *version_str = (const char *)glGetString(GL_VERSION);
+   
+   /* Check multisample function availability with multiple fallback options */
+   bool has_tex_storage_ms = false;
+   bool has_tex_image_ms = false;
+   bool has_rbo_storage_ms = false;
+   
+   if (epoxy_is_desktop_gl()) {
+      /* Desktop OpenGL path */
+      if (epoxy_gl_version() >= 43) {
+         has_tex_storage_ms = true;
+      } else if (epoxy_has_gl_extension("GL_ARB_texture_storage_multisample")) {
+         has_tex_storage_ms = true;
+      }
+      /* glTexImage2DMultisample available since GL 3.2 */
+      if (epoxy_gl_version() >= 32) {
+         has_tex_image_ms = true;
+      }
+      /* glRenderbufferStorageMultisample available since GL 3.0 */
+      if (epoxy_gl_version() >= 30) {
+         has_rbo_storage_ms = true;
+      }
+   } else {
+      /* OpenGL ES path */
+      if (epoxy_gl_version() >= 31) {
+         has_tex_storage_ms = true;
+         has_tex_image_ms = true;
+      }
+      /* glRenderbufferStorageMultisample available since ES 3.0 (ANGLE/Metal) */
+      if (epoxy_gl_version() >= 30) {
+         has_rbo_storage_ms = true;
+      }
+   }
+   
+   /* If no multisample functions available at all, disable MSAA */
+   if (!has_tex_storage_ms && !has_tex_image_ms && !has_rbo_storage_ms) {
+      fprintf(stderr, "[VREND FORMATS] No multisample functions available "
+                      "(GL version: %s, renderer: %s, is_desktop: %d). "
+                      "Disabling MSAA support.\n", 
+                      version_str ? version_str : "unknown",
+                      renderer ? renderer : "unknown",
+                      epoxy_is_desktop_gl());
+      memset(caps->sample_locations, 0, 8 * sizeof(uint32_t));
+      return 0;  /* Return 0 to indicate MSAA not supported */
+   }
+   
+   /* Log which multisample method we're using */
+   if (has_tex_storage_ms) {
+      fprintf(stderr, "[VREND FORMATS] Testing MSAA with glTexStorage2DMultisample\n");
+   } else if (has_tex_image_ms) {
+      fprintf(stderr, "[VREND FORMATS] Testing MSAA with glTexImage2DMultisample fallback\n");
+   } else if (has_rbo_storage_ms) {
+      fprintf(stderr, "[VREND FORMATS] Testing MSAA with glRenderbufferStorageMultisample fallback "
+                      "(GL version: %s, renderer: %s)\n",
+                      version_str ? version_str : "unknown",
+                      renderer ? renderer : "unknown");
+   }
+
+   fprintf(stderr, "[VREND FORMATS] Starting MSAA capability test with max_samples=%u\n", max_samples);
+   
    glGenFramebuffers( 1, &fbo );
    memset(caps->sample_locations, 0, 8 * sizeof(uint32_t));
 
    for (int i = 3; i >= 0; i--) {
-      if (test_num_samples[i] > max_samples)
+      if (test_num_samples[i] > max_samples) {
+         fprintf(stderr, "[VREND FORMATS] Skipping %u samples (exceeds max %u)\n", 
+                 test_num_samples[i], max_samples);
          continue;
-      glGenTextures(1, &tex);
-      glBindTexture(GL_TEXTURE_2D_MULTISAMPLE, tex);
-      glTexStorage2DMultisample(GL_TEXTURE_2D_MULTISAMPLE, test_num_samples[i], GL_RGBA32F, 64, 64, GL_TRUE);
+      }
+      
+      fprintf(stderr, "[VREND FORMATS] Testing %u samples...\n", test_num_samples[i]);
+      
+      /* Clear any stale errors before testing */
+      while (glGetError() != GL_NO_ERROR);
+      
+      if (has_tex_storage_ms || has_tex_image_ms) {
+         /* Texture-based MSAA testing - use GL_RGBA8 for better compatibility */
+         glGenTextures(1, &tex);
+         GLenum err1 = glGetError();
+         
+         glBindTexture(GL_TEXTURE_2D_MULTISAMPLE, tex);
+         GLenum err2 = glGetError();
+         
+         if (has_tex_storage_ms) {
+            glTexStorage2DMultisample(GL_TEXTURE_2D_MULTISAMPLE, test_num_samples[i], GL_RGBA8, 64, 64, GL_TRUE);
+         } else {
+            glTexImage2DMultisample(GL_TEXTURE_2D_MULTISAMPLE, test_num_samples[i], GL_RGBA8, 64, 64, GL_TRUE);
+         }
+         GLenum err3 = glGetError();
+         
+         if (err1 != GL_NO_ERROR || err2 != GL_NO_ERROR || err3 != GL_NO_ERROR) {
+            fprintf(stderr, "[VREND FORMATS]   glGenTextures err=0x%x, glBindTexture err=0x%x, glTex*Multisample err=0x%x\n",
+                    err1, err2, err3);
+         }
+      } else {
+         /* Renderbuffer-based MSAA testing (fallback for ES 3.0 / ANGLE) */
+         GLuint rbo;
+         glGenRenderbuffers(1, &rbo);
+         glBindRenderbuffer(GL_RENDERBUFFER, rbo);
+         glRenderbufferStorageMultisample(GL_RENDERBUFFER, test_num_samples[i], GL_RGBA8, 64, 64);
+         tex = rbo;  /* Store RBO handle in tex variable for cleanup */
+      }
+      
       status = glGetError();
       if (status == GL_NO_ERROR) {
          glBindFramebuffer(GL_FRAMEBUFFER, fbo);
-         glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D_MULTISAMPLE, tex, 0);
+         
+         if (has_tex_storage_ms || has_tex_image_ms) {
+            glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D_MULTISAMPLE, tex, 0);
+         } else {
+            /* For renderbuffer fallback */
+            glFramebufferRenderbuffer(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_RENDERBUFFER, tex);
+         }
+         
          status = glCheckFramebufferStatus(GL_FRAMEBUFFER);
          if (status == GL_FRAMEBUFFER_COMPLETE) {
+            fprintf(stderr, "[VREND FORMATS] ✓ %u samples COMPLETE\n", test_num_samples[i]);
             if (max_samples_confirmed < test_num_samples[i])
                max_samples_confirmed = test_num_samples[i];
 
-            for (unsigned k = 0; k < test_num_samples[i]; ++k) {
-               float msp[2];
-               uint32_t compressed;
-               glGetMultisamplefv(GL_SAMPLE_POSITION, k, msp);
-               compressed = ((unsigned)(floor(msp[0] * 16.0f)) & 0xf) << 4;
-               compressed |= ((unsigned)(floor(msp[1] * 16.0f)) & 0xf);
-               caps->sample_locations[out_buf_offsets[i] + (k >> 2)] |= compressed  << (8 * (k & 3));
+            /* glGetMultisamplefv only available in desktop GL (since 3.2), not in GL ES */
+            if (epoxy_is_desktop_gl()) {
+               for (unsigned k = 0; k < test_num_samples[i]; ++k) {
+                  float msp[2];
+                  uint32_t compressed;
+                  glGetMultisamplefv(GL_SAMPLE_POSITION, k, msp);
+                  compressed = ((unsigned)(floor(msp[0] * 16.0f)) & 0xf) << 4;
+                  compressed |= ((unsigned)(floor(msp[1] * 16.0f)) & 0xf);
+                  caps->sample_locations[out_buf_offsets[i] + (k >> 2)] |= compressed  << (8 * (k & 3));
+               }
+            } else {
+               /* OpenGL ES: sample locations not available, leave them zero-initialized */
+               fprintf(stderr, "[VREND FORMATS]   (OpenGL ES: sample locations not available)\n");
             }
             lowest_working_ms_count_idx = i;
          } else {
+            fprintf(stderr, "[VREND FORMATS] ✗ %u samples INCOMPLETE (status=0x%x)\n", 
+                    test_num_samples[i], status);
             /* If a framebuffer doesn't support low sample counts,
              * use the sample position from the last working larger count. */
             if (lowest_working_ms_count_idx > 0) {
                for (unsigned k = 0; k < test_num_samples[i]; ++k) {
                   caps->sample_locations[out_buf_offsets[i] + (k >> 2)] =
-                        caps->sample_locations[out_buf_offsets[lowest_working_ms_count_idx]  + (k >> 2)];
+                     caps->sample_locations[out_buf_offsets[lowest_working_ms_count_idx]  + (k >> 2)];
                }
             }
          }
          glBindFramebuffer(GL_FRAMEBUFFER, 0);
+      } else {
+         fprintf(stderr, "[VREND FORMATS] ✗ %u samples GL_ERROR=0x%x\n", test_num_samples[i], status);
       }
-      glDeleteTextures(1, &tex);
+      
+      /* Cleanup - delete texture or renderbuffer */
+      if (has_tex_storage_ms || has_tex_image_ms) {
+         glDeleteTextures(1, &tex);
+      } else {
+         glDeleteRenderbuffers(1, &tex);
+      }
    }
    glDeleteFramebuffers(1, &fbo);
+   
+   fprintf(stderr, "[VREND FORMATS] MSAA test complete: returning max_samples_confirmed=%u\n", 
+           max_samples_confirmed);
    return max_samples_confirmed;
 }
 
diff -Naur virglrenderer-upstream/src/vrend/vrend_renderer.c virglrenderer-patched/src/vrend/vrend_renderer.c
--- virglrenderer-upstream/src/vrend/vrend_renderer.c	2026-01-09 14:47:36
+++ virglrenderer-patched/src/vrend/vrend_renderer.c	2026-01-09 15:09:13
@@ -28,6 +28,7 @@
 #include <unistd.h>
 #include <stdatomic.h>
 #include <stdio.h>
+#include <string.h>
 #include <errno.h>
 #include "pipe/p_shader_tokens.h"
 
@@ -80,6 +81,10 @@
 /*
  * VIRTGPU_DRM_CAPSET_VIRGL has version 0 and 1, but they are both
  * virgl_caps_v1 and are exactly the same.
+bool vrend_renderer_video_available(void)
+{
+   return vrend_state.video_available;
+}
  *
  * VIRTGPU_DRM_CAPSET_VIRGL2 has version 0, 1, and 2, but they are
  * all virgl_caps_v2 and are exactly the same.
@@ -323,7 +328,7 @@
    FEAT(texture_mirror_clamp_to_edge, UNAVAIL, UNAVAIL, "GL_ATI_texture_mirror_once", "GL_EXT_texture_mirror_clamp", "GL_ARB_texture_mirror_clamp_to_edge", "GL_EXT_texture_mirror_clamp_to_edge"),
    FEAT(texture_mirror_clamp, UNAVAIL, UNAVAIL, "GL_ATI_texture_mirror_once", "GL_EXT_texture_mirror_clamp"),
    FEAT(texture_mirror_clamp_to_border, UNAVAIL, UNAVAIL, "GL_EXT_texture_mirror_clamp"),
-   FEAT(texture_multisample, 32, 31,  "GL_ARB_texture_multisample" ),
+   FEAT(texture_multisample, 32, 30,  "GL_ARB_texture_multisample" ),
    FEAT(texture_query_lod, 40, UNAVAIL, "GL_ARB_texture_query_lod", "GL_EXT_texture_query_lod"),
    FEAT(texture_shadow_lod, UNAVAIL, UNAVAIL, "GL_EXT_texture_shadow_lod"),
    FEAT(texture_srgb_decode, UNAVAIL, UNAVAIL,  "GL_EXT_texture_sRGB_decode" ),
@@ -402,7 +407,9 @@
    bool use_egl_fence : 1;
 #endif
    bool d3d_share_texture : 1;
-   bool gbm_layout_feat : 1;
+
+   /* host-side video acceleration availability */
+   bool video_available;
 };
 
 struct sysval_uniform_block {
@@ -902,7 +909,7 @@
    if (tex_conv_table[format].bindings & VIRGL_BIND_SAMPLER_VIEW)
       return true;
 
-#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_GBM_ALLOCATION)
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
    uint32_t gbm_format = 0;
    if (virgl_gbm_convert_format(&format, &gbm_format))
       return false;
@@ -939,7 +946,7 @@
 
 static inline bool vrend_format_can_scanout(enum virgl_formats format)
 {
-#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_GBM_ALLOCATION)
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
    uint32_t gbm_format = 0;
    if (virgl_gbm_convert_format(&format, &gbm_format))
       return false;
@@ -954,7 +961,7 @@
 #endif
 }
 
-#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_GBM_ALLOCATION)
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
 static inline bool vrend_format_can_texture_view(enum virgl_formats format)
 {
    return has_feature(feat_texture_view) &&
@@ -1389,12 +1396,60 @@
 {
    GLint param;
    const char *shader_parts[SHADER_MAX_STRINGS];
+   char *modified_shaders[SHADER_MAX_STRINGS] = {NULL};
 
-   for (int i = 0; i < shader->glsl_strings.num_strings; i++)
-      shader_parts[i] = shader->glsl_strings.strings[i].buf;
+   /* Firefox uses GL_EXT_shader_texture_lod (GLES), but we have GL_ARB_shader_texture_lod (desktop GL).
+    * Rewrite extension directives to use the ARB version. */
+   for (int i = 0; i < shader->glsl_strings.num_strings; i++) {
+      const char *src = shader->glsl_strings.strings[i].buf;
+      const char *ext_check = strstr(src, "GL_EXT_shader_texture_lod");
+      
+      if (ext_check) {
+         /* Found GL_EXT_shader_texture_lod - replace with GL_ARB_shader_texture_lod */
+         if (getenv("VIRGL_DEBUG_SHADERS")) {
+            fprintf(stderr, "DEBUG: Rewriting shader - replacing GL_EXT_shader_texture_lod with GL_ARB_shader_texture_lod\n");
+            fflush(stderr);
+         }
+         size_t src_len = strlen(src);
+         modified_shaders[i] = malloc(src_len + 16); /* Extra space for ARB vs EXT */
+         if (modified_shaders[i]) {
+            char *dst = modified_shaders[i];
+            const char *read_pos = src;
+            
+            while ((ext_check = strstr(read_pos, "GL_EXT_shader_texture_lod")) != NULL) {
+               /* Copy up to the extension name */
+               size_t prefix_len = ext_check - read_pos;
+               memcpy(dst, read_pos, prefix_len);
+               dst += prefix_len;
+               
+               /* Write ARB version instead */
+               memcpy(dst, "GL_ARB_shader_texture_lod", 25);
+               dst += 25;
+               
+               /* Skip past the EXT version */
+               read_pos = ext_check + 25;
+            }
+            
+            /* Copy remaining string */
+            strcpy(dst, read_pos);
+            shader_parts[i] = modified_shaders[i];
+         } else {
+            shader_parts[i] = src;
+         }
+      } else {
+         shader_parts[i] = src;
+      }
+   }
 
    shader->id = glCreateShader(conv_shader_type(shader->sel->type));
    glShaderSource(shader->id, shader->glsl_strings.num_strings, shader_parts, NULL);
+   
+   /* Free temporary modified shader strings */
+   for (int i = 0; i < shader->glsl_strings.num_strings; i++) {
+      if (modified_shaders[i])
+         free(modified_shaders[i]);
+   }
+   
    glCompileShader(shader->id);
    glGetShaderiv(shader->id, GL_COMPILE_STATUS, &param);
    if (param == GL_FALSE) {
@@ -2519,30 +2574,32 @@
    case PIPE_TEX_WRAP_CLAMP: if (vrend_state.use_core_profile == false) return GL_CLAMP; else return GL_CLAMP_TO_EDGE;
 
    case PIPE_TEX_WRAP_CLAMP_TO_EDGE: return GL_CLAMP_TO_EDGE;
-   case PIPE_TEX_WRAP_CLAMP_TO_BORDER: return GL_CLAMP_TO_BORDER;
+   case PIPE_TEX_WRAP_CLAMP_TO_BORDER:
+      /* GLES (ANGLE/Metal) does not support clamp-to-border; fall back to edge
+       * to avoid GL_INVALID_ENUM on sampler parameter calls.
+       */
+      if (vrend_state.use_gles || !has_feature(feat_sampler_border_colors)) {
+         return GL_CLAMP_TO_EDGE;
+      }
+      return GL_CLAMP_TO_BORDER;
 
    case PIPE_TEX_WRAP_MIRROR_REPEAT: return GL_MIRRORED_REPEAT;
    case PIPE_TEX_WRAP_MIRROR_CLAMP:
+      /* Not available on GLES; fall back to mirrored repeat without error. */
       if (has_feature(feat_texture_mirror_clamp))
          return GL_MIRROR_CLAMP_EXT;
-      else {
-          vrend_report_context_error(ctx, VIRGL_ERROR_CTX_UNSUPPORTED_TEX_WRAP, wrap);
-          return GL_MIRRORED_REPEAT;
-      }
+      return GL_MIRRORED_REPEAT;
    case PIPE_TEX_WRAP_MIRROR_CLAMP_TO_EDGE:
+      /* ANGLE/Metal lacks this; fall back silently to clamp-to-edge. */
       if (has_feature(feat_texture_mirror_clamp_to_edge))
          return GL_MIRROR_CLAMP_TO_EDGE_EXT;
-      else {
-         vrend_report_context_error(ctx, VIRGL_ERROR_CTX_UNSUPPORTED_TEX_WRAP, wrap);
-         return GL_MIRRORED_REPEAT;
-      }
+      return GL_CLAMP_TO_EDGE;
    case PIPE_TEX_WRAP_MIRROR_CLAMP_TO_BORDER:
       if (has_feature(feat_texture_mirror_clamp_to_border)) {
          return GL_MIRROR_CLAMP_TO_BORDER_EXT;
-      } else {
-         vrend_report_context_error(ctx, VIRGL_ERROR_CTX_UNSUPPORTED_TEX_WRAP, wrap);
-         return GL_MIRRORED_REPEAT;
       }
+      /* No host support: clamp to edge to avoid GL errors. */
+      return GL_CLAMP_TO_EDGE;
    default:
       assert(0);
       return -1;
@@ -3094,6 +3151,11 @@
 
    if (sub_ctx->nr_cbufs == 0) {
       glReadBuffer(GL_NONE);
+      /* In core profile, must explicitly disable draw buffers when no color attachments */
+      if (vrend_state.use_core_profile) {
+         GLenum none_buf = GL_NONE;
+         glDrawBuffers(1, &none_buf);
+      }
       if (has_feature(feat_srgb_write_control)) {
          glDisable(GL_FRAMEBUFFER_SRGB_EXT);
          sub_ctx->framebuffer_srgb_enabled = false;
@@ -3455,7 +3517,8 @@
       return;
    }
 
-   if (has_feature(feat_gles31_vertex_attrib_binding) && v->id == 0) {
+   if (has_feature(feat_gles31_vertex_attrib_binding)) {
+      if (v->id == 0) {
       glGenVertexArrays(1, &v->id);
       glBindVertexArray(v->id);
       for (uint32_t i = 0; i < v->count; i++) {
@@ -3473,7 +3536,18 @@
          glVertexAttribBinding(i, ve->base.vertex_buffer_index);
          glVertexBindingDivisor(i, ve->base.instance_divisor);
          glEnableVertexAttribArray(i);
+         }
       }
+   } else {
+      for (uint32_t i = 0; i < v->count; i++) {
+         struct vrend_vertex_element *ve = &v->elements[i];
+
+         if (util_format_is_pure_integer(ve->base.src_format)) {
+            UPDATE_INT_SIGN_MASK(ve->base.src_format, i,
+                                 v->signed_int_bitmask,
+                                 v->unsigned_int_bitmask);
+         }
+      }
    }
 }
 
@@ -4871,6 +4945,10 @@
    format = tex_conv_table[fmt].glformat;
    type = tex_conv_table[fmt].gltype;
 
+   if (!has_feature(feat_clear_texture)) {
+      return EINVAL;
+   }
+
    /* 32-bit BGRA resources are always reordered to RGBA ordering before
     * submission to the host driver. Reorder red/blue color bytes in
     * the clear color to match. */
@@ -7500,7 +7578,7 @@
                                                                  UNUSED int *fd,
                                                                  UNUSED void *data)
 {
-#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_GBM_ALLOCATION)
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
    struct vrend_resource *res = (struct vrend_resource *)pres;
 
    if (res->storage_bits & VREND_STORAGE_GBM_BUFFER) {
@@ -7556,39 +7634,9 @@
       return true;
 
    const char * a = (const char *) glGetString(GL_VENDOR);
-   if (!a)
-       return false;
-   if (strcmp(a, "ARM") == 0)
-      return true;
-   return false;
+   return a && !(strcmp(a, "ARM") && strcmp(a, "Google Inc. (Apple)"));
 }
 
-static bool vrend_use_gbm_layout_feature(UNUSED uint32_t flags)
-{
-#if defined(ENABLE_GBM_ALLOCATION) && !defined(MINIGBM)
-   if (!gbm || !gbm->device)
-      return false;
-
-   if (debug_get_bool_option("VIRGL_GBM_LAYOUT_FORCE_ENABLE", false))
-      return true;
-
-   if (!debug_get_bool_option("VIRGL_GBM_LAYOUT_ENABLE", false))
-      return false;
-
-   if (!(flags & VREND_USE_GBM_LAYOUT))
-      return false;
-
-   /* gbm-layout needs a linear modifier quirk today, enable it by default only for
-    * drivers known to work properly */
-   if (!strstr((const char *) glGetString(GL_VERSION), "Mesa"))
-      return false;
-
-   return true;
-#else
-   return false;
-#endif
-}
-
 int vrend_renderer_init(const struct vrend_if_cbs *cbs, uint32_t flags)
 {
    bool gles;
@@ -7630,6 +7678,35 @@
    vrend_clicbs->make_current(gl_context);
    gl_ver = epoxy_gl_version();
 
+   /* Surface the full GL strings early for debugging/profile confirmation. */
+   const GLubyte *gl_ver_str = glGetString(GL_VERSION);
+   const GLubyte *gl_renderer_str = glGetString(GL_RENDERER);
+   const GLubyte *glsl_ver_str = glGetString(GL_SHADING_LANGUAGE_VERSION);
+
+   /* On macOS+Metal the GL_VERSION string can start with just the numeric
+    * version and "Metal"; reshape it to a clearer OpenGL 4.x label for logs.
+    */
+   char gl_ver_buf[128];
+   const char *gl_ver_display = gl_ver_str ? (const char *)gl_ver_str : "(null)";
+#ifdef __APPLE__
+   int gl_major_num = gl_ver / 10;
+   int gl_minor_num = gl_ver % 10;
+   if (gl_ver_str && strstr((const char *)gl_ver_str, "Metal")) {
+      snprintf(gl_ver_buf, sizeof(gl_ver_buf), "OpenGL %d.%d (Metal)", gl_major_num, gl_minor_num);
+      gl_ver_display = gl_ver_buf;
+   }
+#endif
+
+      virgl_info("GL strings: version='%s' renderer='%s' glsl='%s'\n",
+            gl_ver_display,
+            gl_renderer_str ? (const char *)gl_renderer_str : "(null)",
+            glsl_ver_str ? (const char *)glsl_ver_str : "(null)");
+      /* Duplicate to stderr so it survives restrictive log levels or early aborts. */
+      fprintf(stderr, "GL strings: version='%s' renderer='%s' glsl='%s'\n",
+         gl_ver_display,
+         gl_renderer_str ? (const char *)gl_renderer_str : "(null)",
+         glsl_ver_str ? (const char *)glsl_ver_str : "(null)");
+
    /* enable error output as early as possible */
    if (vrend_debug(NULL, dbg_khr) && epoxy_has_gl_extension("GL_KHR_debug")) {
       glDebugMessageCallback(vrend_debug_cb, NULL);
@@ -7661,6 +7738,18 @@
    init_features(gles ? 0 : gl_ver,
                  gles ? gl_ver : 0);
 
+   /* Disable host video decode/encode paths entirely in this build to avoid
+    * guest CREATE_VIDEO_BUFFER commands on configurations that cannot service
+    * them (e.g., macOS ANGLE/Metal). This also keeps caps consistent with
+    * the advertised zero video caps below.
+    */
+   vrend_state.video_available = false;
+
+#ifdef __APPLE__
+   /* macOS core GL 4.1 lacks GL_ARB_copy_image; force fallback paths. */
+   clear_feature(feat_copy_image);
+#endif
+
    if (!vrend_winsys_has_gl_colorspace())
       clear_feature(feat_srgb_write_control) ;
 
@@ -7766,8 +7855,6 @@
 
    vrend_state.d3d_share_texture = flags & VREND_D3D11_SHARE_TEXTURE;
 
-   vrend_state.gbm_layout_feat = vrend_use_gbm_layout_feature(flags);
-
    return 0;
 cleanup_and_fail:
    vrend_renderer_fini();
@@ -8195,7 +8282,7 @@
          return -1;
       }
 
-#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_GBM_ALLOCATION)
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
       if (!virgl_gbm_gpu_import_required(args->bind)) {
          return 0;
       }
@@ -8309,7 +8396,7 @@
          glBufferStorage(gr->target, width, NULL, buffer_storage_flags);
          gr->map_info = vrend_state.inferred_gl_caching_type;
       }
-#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_GBM_ALLOCATION)
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
       else if (has_feature(feat_memory_object_fd) && has_feature(feat_memory_object)) {
          GLuint memobj = 0;
          int fd = -1;
@@ -8587,7 +8674,7 @@
  */
 static void vrend_resource_gbm_init(struct vrend_resource *gr, uint32_t format)
 {
-#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_GBM_ALLOCATION)
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
    uint32_t gbm_flags = virgl_gbm_convert_flags(gr->base.bind);
    uint32_t gbm_format = 0;
    if (virgl_gbm_convert_format(&format, &gbm_format))
@@ -8604,24 +8691,6 @@
    if (!virgl_gbm_external_allocation_preferred(gr->base.bind))
       return;
 
-#if !defined(MINIGBM)
-   /*
-    * GBM allocation may be less optimal compared to a regular GL allocation.
-    * Skip GBM allocation when we don't actually need it.
-    */
-   if (!vrend_state.gbm_layout_feat)
-      return;
-
-   /*
-    * Kernel virtio-gpu driver doesn't support modifiers other than linear and
-    * venus needs a real modifier that backs GBM buffer when it imports vrend
-    * resource. Linear allocation is needed when venus works in KMS mode on guest
-    * and it wants to present vrend resources.
-    */
-   if (gr->base.bind & VIRGL_BIND_SHARED)
-      gbm_flags |= GBM_BO_USE_LINEAR;
-#endif
-
    if (!gbm_device_is_format_supported(gbm->device, gbm_format, gbm_flags))
       return;
 
@@ -8737,7 +8806,19 @@
       }
 
       if (pr->nr_samples > 1) {
-         if (format_can_texture_storage) {
+         /* Metal backend (macOS): Silently downgrade MSAA to non-MSAA when not supported.
+          * gl=es mode (ANGLE) handles MSAA correctly, so only apply this workaround
+          * for desktop GL where Metal backend doesn't support multisampled textures. */
+         const char *renderer_str = (const char *)glGetString(GL_RENDERER);
+         bool is_metal_backend = (renderer_str && strstr(renderer_str, "Metal"));
+         
+         if (is_metal_backend && !vrend_state.use_gles && !has_feature(feat_storage_multisample)) {
+            /* Metal backend: No MSAA support, downgrade to regular texture */
+            fprintf(stderr, "[VREND] Metal backend: MSAA texture requested (samples=%d, target=0x%x), downgrading to non-MSAA\n",
+                    pr->nr_samples, gr->target);
+            gr->target = (gr->target == GL_TEXTURE_2D_MULTISAMPLE) ? GL_TEXTURE_2D : GL_TEXTURE_2D_ARRAY;
+            pr->nr_samples = 0;
+         } else if (format_can_texture_storage) {
             if (gr->target == GL_TEXTURE_2D_MULTISAMPLE) {
                glTexStorage2DMultisample(gr->target, pr->nr_samples,
                                          internalformat, pr->width0, pr->height0,
@@ -8758,7 +8839,9 @@
                                        GL_TRUE);
             }
          }
-      } else if (gr->target == GL_TEXTURE_CUBE_MAP) {
+      }
+      
+      if (pr->nr_samples <= 1 && gr->target == GL_TEXTURE_CUBE_MAP) {
             int i;
             if (format_can_texture_storage)
                glTexStorage2D(GL_TEXTURE_CUBE_MAP, pr->last_level + 1, internalformat, pr->width0, pr->height0);
@@ -8827,7 +8910,7 @@
    glBindTexture(gr->target, 0);
 
    if (image_oes && gr->gbm_bo) {
-#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_GBM_ALLOCATION)
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
       if (!has_bit(gr->storage_bits, VREND_STORAGE_GL_BUFFER) &&
             !vrend_format_can_texture_view(gr->base.format)) {
          for (int i = 0; i < gbm_bo_get_plane_count(gr->gbm_bo); i++) {
@@ -8929,7 +9012,7 @@
       }
    }
 #endif
-#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_GBM_ALLOCATION)
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
    if (res->gbm_bo)
       gbm_bo_destroy(res->gbm_bo);
 #endif
@@ -9677,6 +9760,24 @@
                           GLenum format, GLenum type,
                           GLsizei bufSize, void *data)
 {
+   /* Instrumentation to observe scanout readback health. When
+    * VREND_DUMP_PRESENT is set, log a hash of the first readback. Also
+    * emit a one-time entry log to confirm the path is actually called.
+    */
+   static bool dump_present = false;
+   static bool dump_done = false;
+   static bool logged_entry = false;
+   if (!dump_done) {
+      const char *env = getenv("VREND_DUMP_PRESENT");
+      dump_present = env && *env && strcmp(env, "0") != 0;
+      dump_done = true;
+   }
+   if (!logged_entry) {
+      virgl_info("present-readback entry: w=%d h=%d fmt=0x%x type=0x%x bufSize=%d\n",
+                 width, height, format, type, bufSize);
+      logged_entry = true;
+   }
+
    GLuint fb_id;
 
    glGenFramebuffers(1, &fb_id);
@@ -9726,6 +9827,21 @@
    else
       glReadPixels(x, y, width, height, format, type, data);
 
+   if (dump_present) {
+      /* Simple rolling hash (not cryptographic) to detect non-zero content. */
+      uint64_t hash = 1469598103934665603ull; /* FNV-1a offset */
+      size_t n = bufSize < 65536 ? bufSize : 65536; /* cap for speed */
+      const uint8_t *p = (const uint8_t *)data;
+      for (size_t i = 0; i < n; i++) {
+         hash ^= p[i];
+         hash *= 1099511628211ull; /* FNV-1a prime */
+      }
+      virgl_info("present-readback hash: w=%d h=%d fmt=0x%x type=0x%x hash=0x%016" PRIx64 "\n",
+                 width, height, format, type, hash);
+      /* Only log first hit to avoid spam. */
+      dump_present = false;
+   }
+
    glDeleteFramebuffers(1, &fb_id);
 }
 
@@ -9975,6 +10091,12 @@
    return 0;
 }
 
+/* Forward declaration for MSAA staging path below. */
+static void vrend_renderer_blit_int(struct vrend_context *ctx,
+                                    struct vrend_resource *src_res,
+                                    struct vrend_resource *dst_res,
+                                    const struct pipe_blit_info *info);
+
 static int vrend_renderer_transfer_internal(struct vrend_context *ctx,
                                             struct vrend_resource *res,
                                             const struct vrend_transfer_info *info,
@@ -9998,7 +10120,7 @@
       num_iovs = res->num_iovs;
    }
 
-#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_GBM_ALLOCATION)
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
    if (res->gbm_bo && (transfer_mode == VIRGL_TRANSFER_TO_HOST ||
                        !has_bit(res->storage_bits, VREND_STORAGE_EGL_IMAGE))) {
       const bool success = virgl_gbm_transfer(res->gbm_bo, transfer_mode, iov, num_iovs, info) == 0;
@@ -10091,7 +10213,7 @@
       return EINVAL;
    }
 
-#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_GBM_ALLOCATION)
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
    if (res->gbm_bo) {
       assert(!info->synchronized);
       return virgl_gbm_transfer(res->gbm_bo,
@@ -10105,7 +10227,123 @@
    return vrend_renderer_transfer_write_iov(ctx, res, info->iovec, info->iovec_cnt, info);
 
 }
+static int vrend_renderer_copy_transfer3d_msaa(struct vrend_context *ctx,
+                                               struct vrend_resource *dst_res,
+                                               struct vrend_resource *src_res,
+                                               const struct vrend_transfer_info *info)
+{
+   /* Multisample textures reject TexSubImage uploads; stage into single-sample
+    * texture and resolve via blit to avoid GL_INVALID_OPERATION on macOS core.
+    */
+   if (dst_res->target != GL_TEXTURE_2D_MULTISAMPLE &&
+       dst_res->target != GL_TEXTURE_2D_MULTISAMPLE_ARRAY)
+      return EINVAL;
 
+   const GLenum staging_target = (dst_res->target == GL_TEXTURE_2D_MULTISAMPLE_ARRAY) ?
+                                 GL_TEXTURE_2D_ARRAY : GL_TEXTURE_2D;
+
+   virgl_warn("copy_transfer3d msaa staging: dst target=%u samples=%u level=%u box=[%d,%d,%d %dx%dx%d]\n",
+              dst_res->target, dst_res->base.nr_samples, info->level,
+              info->box->x, info->box->y, info->box->z,
+              info->box->width, info->box->height, info->box->depth);
+
+   struct vrend_resource staging = *dst_res;
+   staging.target = staging_target;
+   staging.base.nr_samples = 1;
+#ifdef PIPE_TEXTURE_2D_MULTISAMPLE_ARRAY
+   if (dst_res->base.target == PIPE_TEXTURE_2D_MULTISAMPLE_ARRAY)
+      staging.base.target = PIPE_TEXTURE_2D_ARRAY;
+   else
+#endif
+      staging.base.target = PIPE_TEXTURE_2D;
+   staging.storage_bits = VREND_STORAGE_GL_TEXTURE;
+   staging.gbm_bo = NULL;
+   staging.egl_image = 0;
+   staging.iov = NULL;
+   staging.num_iovs = 0;
+
+   glGenTextures(1, &staging.gl_id);
+   glBindTexture(staging_target, staging.gl_id);
+   glTexParameteri(staging_target, GL_TEXTURE_MIN_FILTER, GL_NEAREST);
+   glTexParameteri(staging_target, GL_TEXTURE_MAG_FILTER, GL_NEAREST);
+
+   const GLint internalformat = tex_conv_table[staging.base.format].internalformat;
+   const GLsizei level_w = u_minify(staging.base.width0, info->level);
+   const GLsizei level_h = u_minify(staging.base.height0, info->level);
+   const GLsizei level_d = (staging_target == GL_TEXTURE_2D_ARRAY) ? staging.base.array_size : 1;
+
+   if (util_format_is_compressed(staging.base.format)) {
+      const GLsizei comp_size = util_format_get_2d_size(staging.base.format,
+                                                        util_format_get_stride(staging.base.format, level_w),
+                                                        level_h);
+      if (staging_target == GL_TEXTURE_2D_ARRAY) {
+         glCompressedTexImage3D(staging_target, info->level, internalformat,
+                                level_w, level_h, level_d, 0,
+                                comp_size * level_d, NULL);
+      } else {
+         glCompressedTexImage2D(staging_target, info->level, internalformat,
+                                level_w, level_h, 0, comp_size, NULL);
+      }
+   } else {
+      if (staging_target == GL_TEXTURE_2D_ARRAY) {
+         glTexImage3D(staging_target, info->level, internalformat,
+                      level_w, level_h, level_d, 0,
+                      tex_conv_table[staging.base.format].glformat,
+                      tex_conv_table[staging.base.format].gltype,
+                      NULL);
+      } else {
+         glTexImage2D(staging_target, info->level, internalformat,
+                      level_w, level_h, 0,
+                      tex_conv_table[staging.base.format].glformat,
+                      tex_conv_table[staging.base.format].gltype,
+                      NULL);
+      }
+   }
+
+   int ret = vrend_renderer_transfer_write_iov(ctx, &staging, src_res->iov,
+                                               src_res->num_iovs, info);
+   if (ret) {
+      glDeleteTextures(1, &staging.gl_id);
+      return ret;
+   }
+
+   struct pipe_blit_info blit = { 0 };
+   blit.src.resource = &staging.base;
+   blit.src.level = info->level;
+   blit.src.box.x = info->box->x;
+   blit.src.box.y = info->box->y;
+   blit.src.box.z = info->box->z;
+   blit.src.box.width = info->box->width;
+   blit.src.box.height = info->box->height;
+   blit.src.box.depth = info->box->depth;
+   blit.src.format = staging.base.format;
+
+   blit.dst.resource = &dst_res->base;
+   blit.dst.level = info->level;
+   blit.dst.box = blit.src.box;
+   blit.dst.format = dst_res->base.format;
+
+   blit.mask = PIPE_MASK_RGBA;
+   if (vrend_format_is_ds(dst_res->base.format)) {
+      blit.mask = PIPE_MASK_Z;
+      if (util_format_has_stencil(util_format_description(dst_res->base.format)))
+         blit.mask |= PIPE_MASK_S;
+   }
+   blit.filter = PIPE_TEX_FILTER_NEAREST;
+   blit.scissor_enable = false;
+   blit.render_condition_enable = false;
+   blit.alpha_blend = false;
+
+   /* Force shader-based blit to MSAA target; GL forbids single->multi FBO blits. */
+   vrend_renderer_blit_int(ctx, &staging, dst_res, &blit);
+
+   virgl_warn("copy_transfer3d msaa staging: shader blit completed\n");
+
+   glDeleteTextures(1, &staging.gl_id);
+   return 0;
+}
+
+
 int vrend_renderer_copy_transfer3d(struct vrend_context *ctx,
                                    uint32_t dst_handle,
 
@@ -10113,6 +10351,16 @@
                                    struct vrend_resource *src_res,
                                    const struct vrend_transfer_info *info)
 {
+   static int copy_log_budget = 8;
+   if (copy_log_budget > 0) {
+      virgl_warn("copy_transfer3d: dst target=%u base.target=%u samples=%u format=%s level=%u box=[%d,%d,%d %dx%dx%d]\n",
+                 dst_res->target, dst_res->base.target, dst_res->base.nr_samples,
+                 util_format_name(dst_res->base.format), info->level,
+                 info->box->x, info->box->y, info->box->z,
+                 info->box->width, info->box->height, info->box->depth);
+      copy_log_budget--;
+   }
+
    if (!resource_contains_box(dst_res, info->box, info->level)) {
       vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_CMD_BUFFER, dst_handle);
       return EINVAL;
@@ -10123,7 +10371,7 @@
       return EINVAL;
    }
 
-#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_GBM_ALLOCATION)
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
    if (dst_res->gbm_bo && !TRANSFER_NO_GBM_MAPPING(info)) {
       bool use_gbm = true;
 
@@ -10159,6 +10407,14 @@
    }
 #endif
 
+  if (dst_res->target == GL_TEXTURE_2D_MULTISAMPLE ||
+      dst_res->target == GL_TEXTURE_2D_MULTISAMPLE_ARRAY) {
+     int ret = vrend_renderer_copy_transfer3d_msaa(ctx, dst_res, src_res, info);
+     if (!ret)
+        return 0;
+     virgl_warn("copy_transfer3d MSAA staging fallback failed (%d), trying direct upload\n", ret);
+  }
+
   return vrend_renderer_transfer_write_iov(ctx, dst_res, src_res->iov,
                                            src_res->num_iovs, info);
 }
@@ -10180,7 +10436,7 @@
       return EINVAL;
    }
 
-#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_GBM_ALLOCATION)
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
    if (src_res->gbm_bo && !TRANSFER_NO_GBM_MAPPING(info)) {
       bool use_gbm = true;
 
@@ -10550,7 +10806,10 @@
    slice_offset = src_box->z * slice_size;
    cube_slice = (src_res->target == GL_TEXTURE_CUBE_MAP) ? src_box->z + src_box->depth : cube_slice;
    i = (src_res->target == GL_TEXTURE_CUBE_MAP) ? src_box->z : 0;
-   if (slice_offset + src_box->width * src_box->height + cube_slice * slice_size > total_size) {
+   /* Allow depth==0 (treated as 1 slice) and avoid width/height product overflow. */
+   uint32_t slices_to_copy = src_box->depth ? src_box->depth : 1;
+   uint64_t required_size = (uint64_t)slice_offset + (uint64_t)slices_to_copy * slice_size;
+   if (required_size > total_size) {
       virgl_error("Offset out of bound: %d\n", src_box->z);
       goto cleanup;
    }
@@ -10714,9 +10973,32 @@
    if (dst_res->egl_image)
       comp_flags ^= VREND_COPY_COMPAT_FLAG_ONE_IS_EGL_IMAGE;
 
-   if (has_feature(feat_copy_image) &&
-       format_is_copy_compatible(src_res->base.format,dst_res->base.format, comp_flags) &&
-       src_res->base.nr_samples == dst_res->base.nr_samples) {
+   bool allow_copy_image = has_feature(feat_copy_image) &&
+                           format_is_copy_compatible(src_res->base.format,
+                                                     dst_res->base.format,
+                                                     comp_flags) &&
+                           src_res->base.nr_samples == dst_res->base.nr_samples;
+
+   /* ANGLE/Metal on macOS returns GL_INVALID_ENUM for multisample copy_image
+    * targets when running in GLES mode. Prefer the blit fallback instead.
+    */
+   if (allow_copy_image &&
+       (src_res->target == GL_TEXTURE_2D_MULTISAMPLE ||
+        src_res->target == GL_TEXTURE_2D_MULTISAMPLE_ARRAY ||
+        dst_res->target == GL_TEXTURE_2D_MULTISAMPLE ||
+        dst_res->target == GL_TEXTURE_2D_MULTISAMPLE_ARRAY)) {
+      allow_copy_image = false;
+   }
+#ifdef __APPLE__
+   /* ANGLE GLES on macOS can still advertise copy_image but fail at runtime;
+    * force shader blit when running GLES on Apple to avoid GL_INVALID_ENUM.
+    */
+   if (allow_copy_image && vrend_state.use_gles) {
+      allow_copy_image = false;
+   }
+#endif
+
+   if (allow_copy_image) {
       VREND_DEBUG(dbg_copy_resource, ctx, "COPY_REGION: use glCopyImageSubData\n");
       vrend_copy_sub_image(src_res, dst_res, src_level, src_box,
                            dst_level, dstx, dsty, dstz);
@@ -11296,7 +11578,7 @@
     * to resource_copy_region, in this case and if no render states etx need
     * to be applied, forward the call to glCopyImageSubData, otherwise do a
     * normal blit. */
-   if (has_feature(feat_copy_image) &&
+   bool allow_copy_image = has_feature(feat_copy_image) &&
        (!info->render_condition_enable || !ctx->sub->cond_render_gl_mode) &&
        format_is_copy_compatible(info->src.format,info->dst.format, comp_flags) &&
        eglimage_copy_compatible &&
@@ -11309,7 +11591,26 @@
        info->dst.box.y + info->dst.box.height <= dst_height &&
        info->src.box.width == info->dst.box.width &&
        info->src.box.height == info->dst.box.height &&
-       info->src.box.depth == info->dst.box.depth) {
+       info->src.box.depth == info->dst.box.depth;
+
+   /* ANGLE/Metal GLES path returns GL_INVALID_ENUM for copy_image on MSAA
+    * targets; avoid copy_image there. Also prefer shader blit when running
+    * GLES on macOS even for non-MSAA to steer clear of driver quirks.
+    */
+   if (allow_copy_image &&
+       (src_res->target == GL_TEXTURE_2D_MULTISAMPLE ||
+        src_res->target == GL_TEXTURE_2D_MULTISAMPLE_ARRAY ||
+        dst_res->target == GL_TEXTURE_2D_MULTISAMPLE ||
+        dst_res->target == GL_TEXTURE_2D_MULTISAMPLE_ARRAY)) {
+      allow_copy_image = false;
+   }
+#ifdef __APPLE__
+   if (allow_copy_image && vrend_state.use_gles) {
+      allow_copy_image = false;
+   }
+#endif
+
+   if (allow_copy_image) {
       VREND_DEBUG(dbg_blit, ctx,  "  Use glCopyImageSubData\n");
       vrend_copy_sub_image(src_res, dst_res, info->src.level, &info->src.box,
                            info->dst.level, info->dst.box.x, info->dst.box.y,
@@ -12100,6 +12401,30 @@
 static void vrend_fill_caps_glsl_version(int gl_ver, int gles_ver,
                                          union virgl_caps *caps)
 {
+   static int debug_caps = -1;
+   if (debug_caps == -1)
+      debug_caps = getenv("VIRGL_DEBUG_CAPS") != NULL;
+   
+   if (debug_caps) {
+      fprintf(stderr, "DEBUG vrend_fill_caps_glsl_version: gl_ver=%d gles_ver=%d caps=%p\n", gl_ver, gles_ver, (void*)caps);
+      fflush(stderr);
+   }
+   
+#ifdef __APPLE__
+   /* macOS Metal reports GL 4.1 core but Mesa needs explicit GLSL 4.10.
+    * Force this for any desktop GL context on macOS to avoid fallback to GL 2.1.
+    */
+   if (gl_ver >= 30 && gles_ver == 0) {
+      caps->v1.glsl_level = 410;
+      if (debug_caps) {
+         fprintf(stderr, "DEBUG macOS override: set glsl_level=%d (caps->v1.glsl_level now=%d)\n", 
+                 410, caps->v1.glsl_level);
+         fflush(stderr);
+      }
+      return;
+   }
+#endif
+
    if (gles_ver > 0) {
       caps->v1.glsl_level = 120;
 
@@ -12145,7 +12470,7 @@
          }
       }
    }
-   virgl_info("GLSL feature level %d\n", caps->v1.glsl_level);
+      virgl_info("GLSL feature level %d\n", caps->v1.glsl_level);
 }
 
 static void set_format_bit(struct virgl_supported_format_mask *mask, enum virgl_formats fmt)
@@ -12166,6 +12491,10 @@
 {
    int i;
    GLint max;
+   const char *gl_version_str = (const char *)glGetString(GL_VERSION);
+   const char *gl_renderer_str = (const char *)glGetString(GL_RENDERER);
+   const bool is_angle = ((gl_version_str && strstr(gl_version_str, "ANGLE")) ||
+                          (gl_renderer_str && strstr(gl_renderer_str, "ANGLE")));
 
    /*
     * We can't fully support this feature on GLES,
@@ -12210,9 +12539,27 @@
 
    if (has_feature(feat_ubo)) {
       glGetIntegerv(GL_MAX_VERTEX_UNIFORM_BLOCKS, &max);
+      const char *version_str = (const char *)glGetString(GL_VERSION);
+      bool is_angle_local = (version_str && strstr(version_str, "ANGLE"));
       /* GL_MAX_VERTEX_UNIFORM_BLOCKS is omitting the ordinary uniform block, add it
-       * also reduce by 1 as we might generate a VirglBlock helper uniform block */
-      caps->v1.max_uniform_blocks = max + 1 - 1;
+       * also reduce by 1 as we might generate a VirglBlock helper uniform block.
+       * Mesa needs at least 12 per shader after its own adjustments, so report max+1.
+       * 
+       * Special handling for ANGLE/Metal: ANGLE clamps reported values to ES spec minimums
+       * (12 for UBOs) even though Metal backend supports 14. Detect ANGLE and report a
+       * higher value to ensure Mesa gets enough after its adjustments. */
+      if (is_angle_local && max <= 12) {
+         caps->v1.max_uniform_blocks = 14;  // ANGLE Metal internal limit
+         fprintf(stderr, "[VREND CAPS] ANGLE backend with UBO limit %d (ES spec minimum), "
+                         "overriding to 14 for Mesa compatibility\n", max);
+      } else {
+         caps->v1.max_uniform_blocks = max + 1;
+         fprintf(stderr, "[VREND CAPS] feat_ubo=YES, GL_MAX_VERTEX_UNIFORM_BLOCKS=%d, reporting max_uniform_blocks=%d\n", 
+                 max, caps->v1.max_uniform_blocks);
+      }
+   } else {
+      fprintf(stderr, "[VREND CAPS] feat_ubo=NO (gl_ver=%d, gles_ver=%d, epoxy_gl_version=%d, epoxy_is_desktop_gl=%d)\n",
+              gl_ver, gles_ver, epoxy_gl_version(), epoxy_is_desktop_gl());
    }
 
    if (has_feature(feat_depth_clamp))
@@ -12231,8 +12578,13 @@
    if (has_feature(feat_seamless_cubemap_per_texture))
       caps->v1.bset.seamless_cube_map_per_texture = 1;
 
-   if (has_feature(feat_texture_multisample))
+   if (has_feature(feat_texture_multisample)) {
       caps->v1.bset.texture_multisample = 1;
+      fprintf(stderr, "[VREND CAPS] feat_texture_multisample enabled, setting caps->v1.bset.texture_multisample=1\n");
+   } else {
+      fprintf(stderr, "[VREND CAPS] feat_texture_multisample NOT enabled (gl_ver=%d, gles_ver=%d)\n",
+              vrend_state.use_gles ? 0 : gl_ver, vrend_state.use_gles ? gl_ver : 0);
+   }
 
    if (has_feature(feat_tessellation))
       caps->v1.bset.has_tessellation_shaders = 1;
@@ -12349,6 +12701,7 @@
 
    glGetIntegerv(GL_MAX_SAMPLES, &max);
    caps->v1.max_samples = max;
+   fprintf(stderr, "[VREND] GL_MAX_SAMPLES from glGetIntegerv: %d\n", max);
 
    /* All of the formats are common. */
    for (i = 0; i < VIRGL_FORMAT_MAX; i++) {
@@ -12376,6 +12729,7 @@
    GLfloat range[2];
    uint32_t video_memory;
    const char *renderer = (const char *)glGetString(GL_RENDERER);
+   const bool angle_in_renderer = (renderer && strstr(renderer, "ANGLE"));
 
    /* Count this up when you add a feature flag that is used to set a CAP in
     * the guest that was set unconditionally before. Then check that flag and
@@ -12383,10 +12737,134 @@
     * run on an old virgl host. Use it also to indicate non-cap fixes on the
     * host that help enable features in the guest. */
    caps->v2.host_feature_check_version = 23;
+   if (gles_ver > 0 && angle_in_renderer)
+      caps->v2.host_feature_check_version = 4;
 
-   /* Forward host GL_RENDERER to the guest. */
-   strncpy(caps->v2.renderer, renderer, sizeof(caps->v2.renderer) - 1);
+   /* Forward host GL_RENDERER to the guest.
+    *
+    * Firefox has an ANGLE-specific GL_RENDERER parser that triggers on the
+    * substring "ANGLE" and may fail hard if the string doesn't match its
+    * expected formats.
+    *
+    * Additionally, Firefox's WebGL renderer sanitizer recognizes a limited set
+    * of device names (e.g. strings starting with "Apple"). The plain "virgl"
+    * renderer name does not match those heuristics.
+    *
+    * For ANGLE-on-Metal, extract the Apple SoC / device name and forward that
+    * (without the "ANGLE" token) so the guest GL_RENDERER can be both parse- and
+    * sanitize-friendly.
+    */
+   if (renderer && strstr(renderer, "ANGLE Metal Renderer:")) {
+      const char *metal_start = strstr(renderer, "ANGLE Metal Renderer:");
+      const char *device_start = metal_start + strlen("ANGLE Metal Renderer:");
+      while (*device_start == ' ')
+         device_start++;
 
+      const char *device_end = device_start;
+      while (*device_end && *device_end != ',' && *device_end != ')')
+         device_end++;
+
+      while (device_end > device_start && device_end[-1] == ' ')
+         device_end--;
+
+      /* Forward the real Metal device name.
+       * If it isn't vendor-prefixed, try to prefix it with the ANGLE vendor
+       * field from the leading "ANGLE (vendor, ...)" without including the
+       * "ANGLE" token itself.
+       */
+      if (device_end > device_start) {
+         const size_t device_len = (size_t)(device_end - device_start);
+
+         /* If already vendor-prefixed (common on Apple), keep as-is. */
+         if (device_len >= 5 && !strncmp(device_start, "Apple", 5)) {
+            snprintf(caps->v2.renderer, sizeof(caps->v2.renderer), "%.*s",
+                     (int)device_len, device_start);
+         } else {
+            const char *angle_prefix = "ANGLE (";
+            const char *vendor_start = strstr(renderer, angle_prefix);
+            if (vendor_start == renderer) {
+               vendor_start += strlen(angle_prefix);
+               const char *vendor_end = strchr(vendor_start, ',');
+               if (vendor_end && vendor_end > vendor_start) {
+                  while (*vendor_start == ' ')
+                     vendor_start++;
+                  while (vendor_end > vendor_start && vendor_end[-1] == ' ')
+                     vendor_end--;
+               }
+
+               if (vendor_end && vendor_end > vendor_start) {
+                  snprintf(caps->v2.renderer, sizeof(caps->v2.renderer), "%.*s %.*s",
+                           (int)(vendor_end - vendor_start), vendor_start,
+                           (int)device_len, device_start);
+               } else {
+                  snprintf(caps->v2.renderer, sizeof(caps->v2.renderer), "%.*s",
+                           (int)device_len, device_start);
+               }
+            } else {
+               snprintf(caps->v2.renderer, sizeof(caps->v2.renderer), "%.*s",
+                        (int)device_len, device_start);
+            }
+         }
+      } else {
+         strncpy(caps->v2.renderer, "Generic Renderer", sizeof(caps->v2.renderer) - 1);
+         caps->v2.renderer[sizeof(caps->v2.renderer) - 1] = '\0';
+      }
+   } else if (renderer && !strncmp(renderer, "ANGLE (", 7)) {
+      /* Common ANGLE format:
+       *   "ANGLE (Apple, Apple M4 Pro, OpenGL ES 3.2 ... )"
+       * Extract the renderer field (second CSV field) and forward it without
+       * the "ANGLE" token.
+       */
+      const char *p = renderer + 7; /* after "ANGLE (" */
+      while (*p == ' ')
+         p++;
+
+      const char *vendor_start = p;
+      const char *vendor_end = strchr(vendor_start, ',');
+      if (!vendor_end)
+         goto angle_generic;
+
+      const char *device_start = vendor_end + 1;
+      while (*device_start == ' ')
+         device_start++;
+
+      const char *device_end = strchr(device_start, ',');
+      if (!device_end)
+         goto angle_generic;
+
+      while (device_end > device_start && device_end[-1] == ' ')
+         device_end--;
+
+      const size_t device_len = (size_t)(device_end - device_start);
+      if (!device_len)
+         goto angle_generic;
+
+      if (device_len >= 5 && !strncmp(device_start, "Apple", 5)) {
+         snprintf(caps->v2.renderer, sizeof(caps->v2.renderer), "%.*s",
+                  (int)device_len, device_start);
+      } else {
+         /* If not Apple-prefixed, keep the renderer field as-is (still
+          * avoiding the "ANGLE" token).
+          */
+         snprintf(caps->v2.renderer, sizeof(caps->v2.renderer), "%.*s",
+                  (int)device_len, device_start);
+      }
+   } else {
+      if (renderer)
+         strncpy(caps->v2.renderer, renderer, sizeof(caps->v2.renderer) - 1);
+      else
+         strncpy(caps->v2.renderer, "(null)", sizeof(caps->v2.renderer) - 1);
+      caps->v2.renderer[sizeof(caps->v2.renderer) - 1] = '\0';
+   }
+
+   goto angle_done;
+
+angle_generic:
+   strncpy(caps->v2.renderer, "Generic Renderer", sizeof(caps->v2.renderer) - 1);
+   caps->v2.renderer[sizeof(caps->v2.renderer) - 1] = '\0';
+
+angle_done:
+
    /* glamor reject llvmpipe, and since the renderer string is
     * composed of "virgl" and this renderer string we have to
     * hide the "llvmpipe" part */
@@ -12507,8 +12985,25 @@
          glGetIntegerv(GL_MAX_IMAGE_SAMPLES, (GLint*)&caps->v2.max_image_samples);
    }
 
-   if (has_feature(feat_storage_multisample))
-      caps->v1.max_samples = vrend_renderer_query_multisample_caps(caps->v1.max_samples, &caps->v2);
+   /* Always call query_multisample_caps - it will handle the case where
+    * GL_ARB_texture_storage_multisample isn't available by skipping the test
+    * and returning the faked max_samples value with graceful downgrade. */
+   caps->v1.max_samples = vrend_renderer_query_multisample_caps(caps->v1.max_samples, &caps->v2);
+   fprintf(stderr, "[VREND CAPS] After query_multisample_caps: max_samples=%u\n", caps->v1.max_samples);
+   
+   /* For macOS Metal backend: Override to max_samples=1 to trigger Mesa's fake_sw_msaa.
+    * Even though MSAA tests confirm 4 samples work, Mesa's format queries can't verify
+    * multisample support, causing MaxSamples=0. Reporting 1 triggers fake_sw_msaa workaround.
+    * Apply to both desktop GL and GLES (ANGLE) modes. */
+   const char *version = (const char *)glGetString(GL_VERSION);
+   bool is_metal = (version && strstr(version, "Metal"));
+   bool is_angle = (version && strstr(version, "ANGLE"));
+   if ((is_metal || is_angle) && caps->v1.max_samples > 1) {
+      fprintf(stderr, "[VREND CAPS] %s backend: Overriding max_samples %u -> 1 for fake_sw_msaa\n", 
+              is_angle ? "ANGLE" : "Metal",
+              caps->v1.max_samples);
+      caps->v1.max_samples = 1;
+   }
 
    caps->v2.capability_bits |= VIRGL_CAP_TGSI_INVARIANT | VIRGL_CAP_SET_MIN_SAMPLES |
                                VIRGL_CAP_TGSI_PRECISE | VIRGL_CAP_APP_TWEAK_SUPPORT;
@@ -12670,6 +13165,10 @@
             readback_str = "readback";
             set_format_bit(&caps->v2.supported_readback_formats, fmt);
          }
+         /* Only report MSAA support for formats that actually support it.
+          * Even though we fake max_samples=4 for GL 3.0 requirements, we don't
+          * advertise MSAA format support to prevent Mesa from trying to use MSAA
+          * (which would fail on Metal backend). */
          if (vrend_format_can_multisample(fmt)) {
             log_texture_feature = true;
             multisample_str = "multisample";
@@ -12732,7 +13231,7 @@
          caps->v2.capability_bits |= VIRGL_CAP_ARB_BUFFER_STORAGE;
    }
 
-#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_GBM_ALLOCATION)
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
    if (gbm) {
       if (has_feature(feat_memory_object) && has_feature(feat_memory_object_fd)) {
          if ((!strcmp(gbm_device_get_backend_name(gbm->device), "i915") ||
@@ -12801,6 +13300,7 @@
    if (has_feature(feat_ubo)) {
       glGetIntegerv(GL_MAX_UNIFORM_BLOCK_SIZE, &max);
       caps->v2.max_uniform_block_size = max;
+      fprintf(stderr, "[VREND CAPS] GL_MAX_UNIFORM_BLOCK_SIZE=%d (Mesa needs >=16384 for UBO)\n", max);
    }
 
    /* Propagate the max of Uniform Components */
@@ -12852,12 +13352,13 @@
       caps->v2.capability_bits_v2 |= VIRGL_CAP_V2_MIRROR_CLAMP;
 
 #ifdef ENABLE_VIDEO
-   vrend_video_fill_caps(caps);
+   /* Disable video caps entirely to prevent guest CREATE_VIDEO_BUFFER commands
+    * that the host cannot service in this configuration.
+    */
+   caps->v2.num_video_caps = 0;
 #else
    caps->v2.num_video_caps = 0;
 #endif
-   if (vrend_state.gbm_layout_feat)
-      caps->v2.capability_bits_v2 |= VIRGL_CAP_V2_RESOURCE_LAYOUT;
 }
 
 void vrend_renderer_fill_caps(uint32_t set, uint32_t version,
@@ -12867,6 +13368,11 @@
    GLenum err;
    bool fill_capset2 = false;
 
+   if (getenv("VIRGL_DEBUG_CAPS")) {
+      fprintf(stderr, "DEBUG vrend_renderer_fill_caps called: set=%u version=%u caps=%p\n", set, version, (void*)caps);
+      fflush(stderr);
+   }
+
    if (!caps)
       return;
 
@@ -12874,12 +13380,18 @@
    case VIRTGPU_DRM_CAPSET_VIRGL:
       if (version > VREND_CAPSET_VIRGL_MAX_VERSION)
          return;
+      fprintf(stderr, "DEBUG: VIRGL capset (v1), zeroing caps\n");
+      fflush(stderr);
       memset(caps, 0, sizeof(struct virgl_caps_v1));
       caps->max_version = VREND_CAPSET_VIRGL_MAX_VERSION;
       break;
    case VIRTGPU_DRM_CAPSET_VIRGL2:
       if (version > VREND_CAPSET_VIRGL2_MAX_VERSION)
          return;
+      if (getenv("VIRGL_DEBUG_CAPS")) {
+         fprintf(stderr, "DEBUG: VIRGL2 capset (v2), zeroing caps\n");
+         fflush(stderr);
+      }
       memset(caps, 0, sizeof(*caps));
       caps->max_version = VREND_CAPSET_VIRGL2_MAX_VERSION;
       fill_capset2 = true;
@@ -12904,13 +13416,33 @@
 
    vrend_fill_caps_glsl_version(gl_ver, gles_ver, caps);
    VREND_DEBUG(dbg_features, NULL, "GLSL support level: %d", caps->v1.glsl_level);
+   
+   if (getenv("VIRGL_DEBUG_CAPS")) {
+      fprintf(stderr, "DEBUG after vrend_fill_caps_glsl_version: glsl_level=%d\n", caps->v1.glsl_level);
+      fflush(stderr);
+   }
 
    vrend_renderer_fill_caps_v1(gl_ver, gles_ver, caps);
+   
+   if (getenv("VIRGL_DEBUG_CAPS")) {
+      fprintf(stderr, "DEBUG after vrend_renderer_fill_caps_v1: glsl_level=%d\n", caps->v1.glsl_level);
+      fflush(stderr);
+   }
 
    if (!fill_capset2)
       return;
 
    vrend_renderer_fill_caps_v2(gl_ver, gles_ver, caps);
+   
+   if (getenv("VIRGL_DEBUG_CAPS")) {
+      fprintf(stderr, "DEBUG after vrend_renderer_fill_caps_v2: glsl_level=%d\n", caps->v1.glsl_level);
+      fflush(stderr);
+   }
+
+   /* Final caps report */
+   fprintf(stderr, "[VREND CAPS] FINAL VALUES: glsl_level=%u, max_samples=%u\n", 
+           caps->v1.glsl_level, caps->v1.max_samples);
+
 }
 
 GLint64 vrend_renderer_get_timestamp(void)
@@ -13109,6 +13641,35 @@
    info->stride = util_format_get_nblocksx(res->base.format, u_minify(res->base.width0, 0)) * elsize;
 }
 
+void vrend_renderer_borrow_texture_for_scanout(struct pipe_resource *pres)
+{
+   struct vrend_texture *tex = (struct vrend_texture *)pres;
+   struct vrend_format_table *tex_conv = &tex_conv_table[tex->base.base.format];
+
+   assert(tex->base.target == GL_TEXTURE_2D);
+   assert(!util_format_is_depth_or_stencil(tex->base.base.format));
+
+   glBindTexture(GL_TEXTURE_2D, tex->base.gl_id);
+
+   if (tex_conv->flags & VIRGL_TEXTURE_NEED_SWIZZLE) {
+      for (unsigned i = 0; i < ARRAY_SIZE(tex->cur_swizzle); ++i) {
+         GLint next_swizzle = to_gl_swizzle(tex_conv->swizzle[i]);
+         if (tex->cur_swizzle[i] != next_swizzle) {
+            glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_SWIZZLE_R + i, next_swizzle);
+            tex->cur_swizzle[i] = next_swizzle;
+         }
+      }
+   }
+
+   if (tex->cur_srgb_decode != GL_DECODE_EXT && util_format_is_srgb(tex->base.base.format)) {
+      if (has_feature(feat_texture_srgb_decode)) {
+         glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_SRGB_DECODE_EXT,
+                         GL_DECODE_EXT);
+         tex->cur_srgb_decode = GL_DECODE_EXT;
+      }
+   }
+}
+
 int
 vrend_renderer_resource_d3d11_texture2d(struct pipe_resource *pres, void **d3d_tex2d)
 {
@@ -13147,6 +13708,7 @@
       *max_size = 0;
       break;
    }
+
 }
 
 void vrend_renderer_create_sub_ctx(struct vrend_context *ctx, int sub_ctx_id)
@@ -13291,7 +13853,7 @@
 {
    struct vrend_resource *res = (struct vrend_resource *)pres;
 
-#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_GBM_ALLOCATION)
+#if defined(HAVE_EPOXY_EGL_H) && defined(ENABLE_MINIGBM_ALLOCATION)
    if (res->gbm_bo)
       return virgl_gbm_export_query(res->gbm_bo, export_query);
 #else
@@ -13699,93 +14261,7 @@
 }
 #endif
 
-#ifdef ENABLE_GBM
-static size_t get_gbm_bo_plane_size(struct gbm_bo *bo, int plane)
+bool vrend_renderer_video_available(void)
 {
-   off_t size;
-   int fd;
-
-   fd = gbm_bo_get_fd_for_plane(bo, plane);
-   if (fd < 0)
-      return 0;
-
-   size = lseek(fd, 0, SEEK_END);
-   close(fd);
-
-   if (size < 0)
-      return 0;
-
-   return size;
+   return vrend_state.video_available;
 }
-
-int
-vrend_renderer_pipe_resource_get_layout(struct vrend_context *ctx,
-                                        uint32_t out_res_id, uint32_t res_id)
-{
-   struct virgl_resource_layout layout = { 0 };
-   struct vrend_resource *out_res, *res;
-
-   out_res = vrend_renderer_ctx_res_lookup(ctx, out_res_id);
-   if (!out_res) {
-      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, out_res_id);
-      return EINVAL;
-   }
-
-   res = vrend_renderer_ctx_res_lookup(ctx, res_id);
-   if (!res) {
-      vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, res_id);
-      return EINVAL;
-   }
-
-   if (res->gbm_bo) {
-      int num_planes = gbm_bo_get_plane_count(res->gbm_bo);
-
-      if (layout.num_planes > VIRGL_RESOURCE_MAX_PLANES) {
-         virgl_error("resource GBM has too many planes\n");
-         vrend_report_context_error(ctx, VIRGL_ERROR_CTX_UNKNOWN, res_id);
-         return EINVAL;
-      }
-
-      layout.num_planes = num_planes;
-      layout.modifier = gbm_bo_get_modifier(res->gbm_bo);
-
-      for (uint32_t i = 0; i < layout.num_planes; i++) {
-         layout.planes[i].offset = gbm_bo_get_offset(res->gbm_bo, i);
-         layout.planes[i].stride = gbm_bo_get_stride_for_plane(res->gbm_bo, i);
-         layout.planes[i].size = get_gbm_bo_plane_size(res->gbm_bo, i);
-
-         if (!layout.planes[i].size) {
-            virgl_error("failed to get GBM plane size\n");
-            vrend_report_context_error(ctx, VIRGL_ERROR_CTX_UNKNOWN, res_id);
-            return EINVAL;
-         }
-      }
-   }
-
-   if (out_res->iov) {
-      if (vrend_write_to_iovec(out_res->iov, out_res->num_iovs, 0,
-                               (const void *) &layout, sizeof(layout)) != sizeof(layout)) {
-         virgl_error("resource layout does not fit IOV size\n");
-         vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, out_res_id);
-         return EINVAL;
-      }
-   } else {
-      if (out_res->base.width0 >= sizeof(layout)) {
-         *((struct virgl_resource_layout *) out_res->ptr) = layout;
-      } else {
-         virgl_error("resource layout does not fit buffer size\n");
-         vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_RESOURCE, out_res_id);
-         return EINVAL;
-      }
-   }
-
-   return 0;
-}
-#else
-int
-vrend_renderer_pipe_resource_get_layout(UNUSED struct vrend_context *ctx,
-                                        UNUSED uint32_t out_res_id, UNUSED uint32_t res_id)
-{
-   return EINVAL;
-}
-#endif
diff -Naur virglrenderer-upstream/src/vrend/vrend_renderer.h virglrenderer-patched/src/vrend/vrend_renderer.h
--- virglrenderer-upstream/src/vrend/vrend_renderer.h	2026-01-09 14:47:36
+++ virglrenderer-patched/src/vrend/vrend_renderer.h	2026-01-09 15:09:13
@@ -197,7 +197,6 @@
 #define VREND_D3D11_SHARE_TEXTURE (1 << 4)
 #define VREND_USE_COMPAT_CONTEXT (1 << 5)
 #define VREND_USE_GLES (1 << 6)
-#define VREND_USE_GBM_LAYOUT (1 << 7)
 
 bool vrend_check_no_error(struct vrend_context *ctx);
 
@@ -564,6 +563,8 @@
 void vrend_renderer_resource_get_info(struct pipe_resource *pres,
                                       struct vrend_renderer_resource_info *info);
 
+void vrend_renderer_borrow_texture_for_scanout(struct pipe_resource *pres);
+
 void vrend_renderer_get_cap_set(uint32_t cap_set, uint32_t *max_ver,
                                 uint32_t *max_size);
 
@@ -612,6 +613,8 @@
 
 extern const struct vrend_if_cbs *vrend_clicbs;
 
+bool vrend_renderer_video_available(void);
+
 int vrend_renderer_export_query(struct pipe_resource *pres,
                                 struct virgl_renderer_export_query *export_query);
 
@@ -643,8 +646,5 @@
 int
 vrend_renderer_resource_d3d11_texture2d(struct pipe_resource *res, void **handle);
 
-int
-vrend_renderer_pipe_resource_get_layout(struct vrend_context *ctx,
-                                        uint32_t out_res_id, uint32_t res_id);
 
 #endif
diff -Naur virglrenderer-upstream/src/vrend/vrend_shader.c virglrenderer-patched/src/vrend/vrend_shader.c
--- virglrenderer-upstream/src/vrend/vrend_shader.c	2026-01-09 14:47:36
+++ virglrenderer-patched/src/vrend/vrend_shader.c	2026-01-09 15:08:31
@@ -6301,10 +6301,26 @@
 
       if (ctx->prog_type == TGSI_PROCESSOR_VERTEX && ctx->cfg->use_explicit_locations)
          emit_ext(glsl_strbufs, "ARB_explicit_attrib_location", "require");
-      if (ctx->prog_type == TGSI_PROCESSOR_FRAGMENT && fs_emit_layout(ctx))
-         emit_ext(glsl_strbufs, "ARB_fragment_coord_conventions", "require");
 
-      if (ctx->ubo_used_mask)
+      /* Fragment coord conventions:
+       * - Core in GLSL 1.50+ (layout qualifier support)
+       * - Extension ARB_fragment_coord_conventions for older GLSL
+       * - macOS core GL 4.1 reports the capability but NOT the extension string
+       * - Check capability bit instead of extension string for GLSL < 150
+       */
+      if (ctx->prog_type == TGSI_PROCESSOR_FRAGMENT && fs_emit_layout(ctx)) {
+         if (ctx->glsl_ver_required < 150) {
+            /* For GLSL < 150, only require extension if not in core profile
+             * (i.e., capability was set via extension string, not GL 3.2+ core) */
+            if (!ctx->cfg->use_core_profile) {
+               emit_ext(glsl_strbufs, "ARB_fragment_coord_conventions", "require");
+            }
+         }
+         /* For GLSL >= 150, the extension is core - no need to require it */
+      }
+
+      /* Uniform buffers are core in GLSL 1.40+; only request the ARB extension when targeting older versions. */
+      if (ctx->ubo_used_mask && ctx->glsl_ver_required < 140)
          emit_ext(glsl_strbufs, "ARB_uniform_buffer_object", "require");
 
       if (ctx->num_cull_dist_prop || ctx->key->num_in_cull || ctx->key->num_out_cull)
