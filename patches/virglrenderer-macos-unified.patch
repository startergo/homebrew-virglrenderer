diff --git a/config.h.meson b/config.h.meson
index 55497ff..b6f1f4c 100644
--- a/config.h.meson
+++ b/config.h.meson
@@ -38,6 +38,8 @@
 #mesondefine ENABLE_VULKAN_DLOAD
 #mesondefine ENABLE_VULKAN_PRELOAD
 #mesondefine ENABLE_GBM
+#mesondefine ENABLE_METAL
+#define UTIL_ARCH_LITTLE_ENDIAN 1
 #mesondefine ENABLE_DRM
 #mesondefine ENABLE_DRM_MSM
 #mesondefine ENABLE_DRM_AMDGPU
--- meson.build.orig	2026-02-12 09:39:27
+++ meson.build	2026-02-12 09:40:46
@@ -93,7 +93,7 @@
     dl_dep = cc.find_library('dl', required : true)
   endif
 endif
-
+   all_languages = ['c']
 conf_data = configuration_data()
 conf_data.set('VERSION', meson.project_version())
 conf_data.set('_GNU_SOURCE', 1)
@@ -217,6 +217,14 @@
 
 if cc.has_header('linux/udmabuf.h')
    conf_data.set('HAVE_LINUX_UDMABUF_H', 1)
+endif
+
+metal_dep = dependency('appleframeworks', modules : ['Metal'], required: false)
+if metal_dep.found()
+   conf_data.set('ENABLE_METAL', 1)
+   add_languages('objc', required: true, native: false)
+   objc = meson.get_compiler('objc')
+   all_languages += 'objc'
 endif
 
 foreach b : ['bswap32', 'bswap64', 'clz', 'clzll', 'expect', 'ffs', 'ffsll',

diff --git a/server/meson.build b/server/meson.build
index e73f8de..4d1216d 100644
--- a/server/meson.build
+++ b/server/meson.build
@@ -27,6 +27,7 @@ endif
 virgl_render_server = executable(
    'virgl_render_server',
    virgl_render_server_sources,
+   c_args : [ '-DSTANDALONE_SERVER=1' ],
    dependencies : virgl_render_server_depends,
    install : true,
    install_dir : render_server_install_dir,
diff --git a/server/render_client.c b/server/render_client.c
index 87bab87..fd88f53 100644
--- a/server/render_client.c
+++ b/server/render_client.c
@@ -76,13 +76,15 @@ static void
 init_context_args(struct render_context_args *ctx_args,
                   uint32_t init_flags,
                   const struct render_client_op_create_context_request *req,
-                  int ctx_fd)
+                  int ctx_fd,
+                  bool in_process)
 {
    *ctx_args = (struct render_context_args){
       .valid = true,
       .init_flags = init_flags,
       .ctx_id = req->ctx_id,
       .ctx_fd = ctx_fd,
+      .in_process = in_process,
    };
 
    static_assert(sizeof(ctx_args->ctx_name) == sizeof(req->ctx_name), "");
@@ -123,7 +125,8 @@ render_client_create_context(struct render_client *client,
    int remote_fd = socket_fds[1];
 
    struct render_context_args ctx_args;
-   init_context_args(&ctx_args, client->init_flags, req, ctx_fd);
+   bool in_process = srv->context_args->in_process;
+   init_context_args(&ctx_args, client->init_flags, req, ctx_fd, in_process);
 
 #ifdef ENABLE_RENDER_SERVER_WORKER_THREAD
    rec->worker = render_worker_create(srv->worker_jail, render_client_worker_thread,
diff --git a/server/render_common.c b/server/render_common.c
index e51bb88..144364e 100644
--- a/server/render_common.c
+++ b/server/render_common.c
@@ -4,6 +4,9 @@
  */
 
 #include "render_common.h"
+#ifndef STANDALONE_SERVER
+#include "virgl_util.h"
+#endif
 
 #include <stdarg.h>
 #include <stdio.h>
@@ -12,7 +15,9 @@
 void
 render_log_init(void)
 {
+#ifdef STANDALONE_SERVER
    openlog(NULL, LOG_NDELAY | LOG_PERROR | LOG_PID, LOG_USER);
+#endif
 }
 
 void
@@ -21,6 +26,10 @@ render_log(const char *fmt, ...)
    va_list va;
 
    va_start(va, fmt);
+#ifdef STANDALONE_SERVER
    vsyslog(LOG_DEBUG, fmt, va);
+#else
+   virgl_prefixed_logv("server", VIRGL_LOG_LEVEL_INFO, fmt, va);
+#endif
    va_end(va);
 }
diff --git a/server/render_context.c b/server/render_context.c
index b44b5b5..39890c2 100644
--- a/server/render_context.c
+++ b/server/render_context.c
@@ -108,16 +108,28 @@ render_context_dispatch_create_resource(struct render_context *ctx,
    struct render_context_op_create_resource_reply reply = {
       .fd_type = VIRGL_RESOURCE_FD_INVALID,
    };
-   int res_fd;
+   int res_fd = -1;
    bool ok = render_state_create_resource(ctx->ctx_id, req->res_id, req->blob_id,
                                           req->blob_size, req->blob_flags, &reply.fd_type,
-                                          &res_fd, &reply.map_info, &reply.vulkan_info);
+                                          &res_fd, &reply.res_ptr, &reply.map_info,
+                                          &reply.vulkan_info);
    if (!ok)
       return render_socket_send_reply(&ctx->socket, &reply, sizeof(reply));
 
-   ok =
-      render_socket_send_reply_with_fds(&ctx->socket, &reply, sizeof(reply), &res_fd, 1);
-   close(res_fd);
+   if (res_fd >= 0) {
+      ok =
+         render_socket_send_reply_with_fds(&ctx->socket, &reply, sizeof(reply), &res_fd, 1);
+      close(res_fd);
+   } else {
+      if (!ctx->in_process) {
+         /* not in_process, cannot send pointers */
+         render_log("cannot send pointer for resource %d", req->res_id);
+         render_state_destroy_resource(ctx->ctx_id, req->res_id);
+         reply.fd_type = VIRGL_RESOURCE_FD_INVALID;
+         reply.res_ptr = NULL;
+      }
+      ok = render_socket_send_reply(&ctx->socket, &reply, sizeof(reply));
+   }
 
    return ok;
 }
@@ -343,6 +355,7 @@ render_context_init(struct render_context *ctx, const struct render_context_args
    render_socket_init(&ctx->socket, args->ctx_fd);
    ctx->shmem_fd = -1;
    ctx->fence_eventfd = -1;
+   ctx->in_process = args->in_process;
 
    if (!render_context_init_name(ctx, args->ctx_id, args->ctx_name))
       return false;
diff --git a/server/render_context.h b/server/render_context.h
index 24435ab..4345176 100644
--- a/server/render_context.h
+++ b/server/render_context.h
@@ -25,12 +25,15 @@ struct render_context {
 
    int timeline_count;
 
+   bool in_process;
+
    /* optional */
    int fence_eventfd;
 };
 
 struct render_context_args {
    bool valid;
+   bool in_process;
 
    uint32_t init_flags;
 
diff --git a/server/render_protocol.h b/server/render_protocol.h
index 7e38ac0..1705fdf 100644
--- a/server/render_protocol.h
+++ b/server/render_protocol.h
@@ -148,9 +148,11 @@ struct render_context_op_create_resource_request {
 struct render_context_op_create_resource_reply {
    enum virgl_resource_fd_type fd_type;
    uint32_t map_info; /* VIRGL_RENDERER_MAP_* */
-   /* vulkan_info is set if the fd_type is VIRGL_RESOURCE_FD_OPAQUE */
+   /* vulkan_info is set if the fd_type is opaque or Metal */
    struct virgl_resource_vulkan_info vulkan_info;
-   /* followed by 1 fd if not VIRGL_RESOURCE_FD_INVALID */
+   /* When fd_type == VIRGL_RESOURCE_METAL_HEAP */
+   void *res_ptr;
+   /* otherwise followed by 1 fd if not VIRGL_RESOURCE_FD_INVALID */
 };
 
 /* Import a blob resource to the context
@@ -219,4 +221,15 @@ union render_context_op_request {
    struct render_context_op_submit_fence_request submit_fence;
 };
 
+/**
+ * When we do not have SOCK_SEQPACKET support, we need to manage framing.
+ * This will be sent as the header of each packet when necessary.
+ */
+struct render_context_socket_header {
+   union {
+      uint32_t length;
+      uint8_t b[sizeof(uint32_t)];
+   };
+};
+
 #endif /* RENDER_PROTOCOL_H */
diff --git a/server/render_server.c b/server/render_server.c
index 6b129e4..f3a5444 100644
--- a/server/render_server.c
+++ b/server/render_server.c
@@ -148,7 +148,7 @@ render_server_parse_options(struct render_server *srv, int argc, char **argv)
       return false;
    }
 
-   if (srv->client_fd < 0 || !render_socket_is_seqpacket(srv->client_fd)) {
+   if (srv->client_fd < 0) {
       render_log("no valid client fd specified");
       return false;
    }
diff --git a/server/render_socket.c b/server/render_socket.c
index 58edcf4..3201da6 100644
--- a/server/render_socket.c
+++ b/server/render_socket.c
@@ -12,6 +12,30 @@
 
 #define RENDER_SOCKET_MAX_FD_COUNT 8
 
+#if !defined(MSG_CMSG_CLOEXEC) || !defined(SOCK_CLOEXEC)
+#include <fcntl.h>
+static int
+render_socket_set_cloexec(int fd)
+{
+   long flags;
+
+   if (fd == -1)
+      return -1;
+
+   flags = fcntl(fd, F_GETFD);
+   if (flags == -1)
+      goto err;
+
+   if (fcntl(fd, F_SETFD, flags | FD_CLOEXEC) == -1)
+      goto err;
+
+   return 0;
+
+err:
+   return -1;
+}
+#endif
+
 /* The socket pair between the server process and the client process is set up
  * by the client process (or yet another process).  Because render_server_run
  * does not poll yet, the fd is expected to be blocking.
@@ -28,7 +52,24 @@
 bool
 render_socket_pair(int out_fds[static 2])
 {
-   int ret = socketpair(AF_UNIX, SOCK_SEQPACKET | SOCK_CLOEXEC, 0, out_fds);
+#ifdef __APPLE__
+   int type = SOCK_SEQPACKET;
+#else
+   int type = SOCK_SEQPACKET;
+#endif
+#ifdef SOCK_CLOEXEC
+   type |= SOCK_CLOEXEC;
+#endif
+
+   int ret = socketpair(AF_UNIX, type, 0, out_fds);
+#ifndef SOCK_CLOEXEC
+   if (!ret) {
+      ret = render_socket_set_cloexec(out_fds[0]);
+   }
+   if (!ret) {
+      ret = render_socket_set_cloexec(out_fds[1]);
+   }
+#endif
    if (ret) {
       render_log("failed to create socket pair");
       return false;
@@ -50,9 +91,11 @@ render_socket_is_seqpacket(int fd)
 void
 render_socket_init(struct render_socket *socket, int fd)
 {
+   bool is_seqpacket = render_socket_is_seqpacket(fd);
    assert(fd >= 0);
    *socket = (struct render_socket){
       .fd = fd,
+      .is_seqpacket = is_seqpacket,
    };
 }
 
@@ -76,15 +119,47 @@ get_received_fds(const struct msghdr *msg, int *out_count)
    return (const int *)CMSG_DATA(cmsg);
 }
 
+enum socket_state {
+   SOCKET_STATE_FIRST_MSG,
+   SOCKET_STATE_HEADER,
+   SOCKET_STATE_DATA,
+};
+
 static bool
 render_socket_recvmsg(struct render_socket *socket, struct msghdr *msg, size_t *out_size)
 {
-   do {
-      const ssize_t s = recvmsg(socket->fd, msg, MSG_CMSG_CLOEXEC);
-      if (unlikely(s <= 0)) {
-         if (!s)
-            return false;
+   int flags = 0;
+#ifdef MSG_CMSG_CLOEXEC
+   flags = MSG_CMSG_CLOEXEC;
+#endif
+
+   enum socket_state state = SOCKET_STATE_FIRST_MSG;
+   struct render_context_socket_header hdr = {0};
+   ssize_t want = sizeof(hdr);
+   struct msghdr _msg = {
+      .msg_iov =
+         &(struct iovec){
+            .iov_base = &hdr,
+            .iov_len = want,
+         },
+      .msg_iovlen = 1,
+      .msg_control = msg->msg_control,
+      .msg_controllen = msg->msg_controllen,
+   };
+	socklen_t _msg_controllen;
+
+   assert(msg->msg_iovlen == 1);
+
+   if (socket->is_seqpacket) {
+      _msg.msg_iov[0].iov_base = msg->msg_iov[0].iov_base;
+      _msg.msg_iov[0].iov_len = msg->msg_iov[0].iov_len;
+      want = 0;
+   }
 
+   *out_size = 0;
+   do {
+      const ssize_t s = recvmsg(socket->fd, &_msg, flags);
+      if (unlikely(s < 0)) {
          if (errno == EAGAIN || errno == EINTR)
             continue;
 
@@ -92,20 +167,65 @@ render_socket_recvmsg(struct render_socket *socket, struct msghdr *msg, size_t *
          return false;
       }
 
-      if (unlikely(msg->msg_flags & (MSG_TRUNC | MSG_CTRUNC))) {
-         render_log("failed to receive message: truncated");
+      if (state == SOCKET_STATE_FIRST_MSG) {
+         _msg_controllen = _msg.msg_controllen;
+         state = socket->is_seqpacket ? SOCKET_STATE_DATA : SOCKET_STATE_HEADER;
+      } else {
+         /* retain the cmsg from first message */
+         assert(_msg.msg_controllen == 0);
+      }
+
+      if (unlikely(_msg.msg_flags & MSG_CTRUNC ||
+                   (socket->is_seqpacket &&
+                     (_msg.msg_flags & MSG_TRUNC) ||
+                      _msg.msg_iov[0].iov_len != (size_t)s))) {
+         render_log("failed to receive message: truncated or incomplete");
 
          int fd_count;
-         const int *fds = get_received_fds(msg, &fd_count);
+         const int *fds = get_received_fds(&_msg, &fd_count);
          for (int i = 0; i < fd_count; i++)
             close(fds[i]);
 
          return false;
       }
 
-      *out_size = s;
-      return true;
+      if (s <= want) {
+         _msg.msg_iov[0].iov_base = (char *)_msg.msg_iov[0].iov_base + s;
+         _msg.msg_iov[0].iov_len -= s;
+         want -= s;
+      }
+
+      if (state == SOCKET_STATE_DATA) {
+         *out_size += s;
+      }
+
+      if (!want && state == SOCKET_STATE_HEADER) {
+         want = ntohl(hdr.length);
+         _msg.msg_iov[0].iov_base = msg->msg_iov[0].iov_base;
+         _msg.msg_iov[0].iov_len = want;
+         state = SOCKET_STATE_DATA;
+      } else if (!want && state == SOCKET_STATE_DATA) {
+         msg->msg_controllen = _msg_controllen;
+         break;
+      }
    } while (true);
+
+#ifndef MSG_CMSG_CLOEXEC
+   int fd_count;
+   int ret = 0;
+   const int *fds = get_received_fds(msg, &fd_count);
+   for (int i = 0; !ret && i < fd_count; i++) {
+      ret = render_socket_set_cloexec(fds[i]);
+   }
+   if (ret) {
+      for (int i = 0; i < fd_count; i++) {
+         close(fds[i]);
+      }
+      return false;
+   }
+#endif
+
+   return true;
 }
 
 static bool
@@ -196,8 +316,32 @@ render_socket_receive_data(struct render_socket *socket, void *data, size_t size
 static bool
 render_socket_sendmsg(struct render_socket *socket, const struct msghdr *msg)
 {
+   enum socket_state state = SOCKET_STATE_FIRST_MSG;
+   struct render_context_socket_header hdr = {
+      .length = htonl(msg->msg_iov[0].iov_len),
+   };
+   ssize_t want = sizeof(hdr);
+   struct msghdr _msg = {
+      .msg_iov =
+         &(struct iovec){
+            .iov_base = &hdr,
+            .iov_len = want,
+         },
+      .msg_iovlen = 1,
+      .msg_control = msg->msg_control,
+      .msg_controllen = msg->msg_controllen,
+   };
+
+   assert(msg->msg_iovlen == 1);
+
+   if (socket->is_seqpacket) {
+      _msg.msg_iov[0].iov_base = msg->msg_iov[0].iov_base;
+      _msg.msg_iov[0].iov_len = msg->msg_iov[0].iov_len;
+      want = 0;
+   }
+
    do {
-      const ssize_t s = sendmsg(socket->fd, msg, MSG_NOSIGNAL);
+      const ssize_t s = sendmsg(socket->fd, &_msg, MSG_NOSIGNAL);
       if (unlikely(s < 0)) {
          if (errno == EAGAIN || errno == EINTR)
             continue;
@@ -206,9 +350,30 @@ render_socket_sendmsg(struct render_socket *socket, const struct msghdr *msg)
          return false;
       }
 
-      /* no partial send since the socket type is SOCK_SEQPACKET */
-      assert(msg->msg_iovlen == 1 && msg->msg_iov[0].iov_len == (size_t)s);
-      return true;
+      if (socket->is_seqpacket) {
+         /* no partial send since the socket type is SOCK_SEQPACKET */
+         assert(_msg.msg_iovlen == 1 && _msg.msg_iov[0].iov_len == (size_t)s);
+         state = SOCKET_STATE_DATA;
+      } else if (state == SOCKET_STATE_FIRST_MSG) {
+         _msg.msg_controllen = 0;
+         _msg.msg_control = NULL;
+         state = SOCKET_STATE_HEADER;
+      }
+
+      if (s <= want) {
+         _msg.msg_iov[0].iov_base = (char *)_msg.msg_iov[0].iov_base + s;
+         _msg.msg_iov[0].iov_len -= s;
+         want -= s;
+      }
+
+      if (!want && state == SOCKET_STATE_HEADER) {
+         want = ntohl(hdr.length);
+         _msg.msg_iov[0].iov_base = msg->msg_iov[0].iov_base;
+         _msg.msg_iov[0].iov_len = want;
+         state = SOCKET_STATE_DATA;
+      } else if (!want && state == SOCKET_STATE_DATA) {
+         return true;
+      }
    } while (true);
 }
 
diff --git a/server/render_socket.h b/server/render_socket.h
index ead1f8c..18a427e 100644
--- a/server/render_socket.h
+++ b/server/render_socket.h
@@ -10,6 +10,7 @@
 
 struct render_socket {
    int fd;
+   bool is_seqpacket;
 };
 
 bool
diff --git a/server/render_state.c b/server/render_state.c
index 715f85e..b30fe7e 100644
--- a/server/render_state.c
+++ b/server/render_state.c
@@ -93,6 +93,7 @@ render_state_lookup_context(uint32_t ctx_id)
    return ctx;
 }
 
+#ifdef STANDALONE_SERVER
 static void
 render_state_cb_debug_logger(UNUSED enum virgl_log_level_flags log_level,
                              const char *message,
@@ -100,6 +101,7 @@ render_state_cb_debug_logger(UNUSED enum virgl_log_level_flags log_level,
 {
    render_log(message);
 }
+#endif
 
 static void
 render_state_cb_retire_fence(uint32_t ctx_id, uint32_t ring_idx, uint64_t fence_id)
@@ -112,7 +114,9 @@ render_state_cb_retire_fence(uint32_t ctx_id, uint32_t ring_idx, uint64_t fence_
 }
 
 static const struct vkr_renderer_callbacks render_state_cbs = {
+#ifdef STANDALONE_SERVER
    .debug_logger = render_state_cb_debug_logger,
+#endif
    .retire_fence = render_state_cb_retire_fence,
 };
 
@@ -221,13 +225,14 @@ render_state_create_resource(uint32_t ctx_id,
                              uint32_t blob_flags,
                              enum virgl_resource_fd_type *out_fd_type,
                              int *out_res_fd,
+                             void **out_res_ptr,
                              uint32_t *out_map_info,
                              struct virgl_resource_vulkan_info *out_vulkan_info)
 {
    SCOPE_LOCK_RENDERER();
    return vkr_renderer_create_resource(ctx_id, res_id, blob_id, blob_size, blob_flags,
-                                       out_fd_type, out_res_fd, out_map_info,
-                                       out_vulkan_info);
+                                       out_fd_type, out_res_fd, out_res_ptr,
+                                       out_map_info, out_vulkan_info);
 }
 
 bool
diff --git a/server/render_state.h b/server/render_state.h
index 98de272..3d6264c 100644
--- a/server/render_state.h
+++ b/server/render_state.h
@@ -40,6 +40,7 @@ render_state_create_resource(uint32_t ctx_id,
                              uint32_t blob_flags,
                              enum virgl_resource_fd_type *out_fd_type,
                              int *out_res_fd,
+                             void **out_res_ptr,
                              uint32_t *out_map_info,
                              struct virgl_resource_vulkan_info *out_vulkan_info);
 
diff --git a/server/render_worker.c b/server/render_worker.c
index 24d2c36..4a6c279 100644
--- a/server/render_worker.c
+++ b/server/render_worker.c
@@ -3,6 +3,12 @@
  * SPDX-License-Identifier: MIT
  */
 
+#ifndef __APPLE__
+#include <sys/signalfd.h>
+#else
+#include <sys/event.h>
+#undef LIST_ENTRY
+#endif
 #include "render_worker.h"
 
 /* One and only one of ENABLE_RENDER_SERVER_WORKER_* must be set.
@@ -24,10 +30,11 @@
 #include <errno.h>
 #include <fcntl.h>
 #include <signal.h>
-#include <sys/signalfd.h>
 #include <sys/types.h>
 #include <sys/wait.h>
-#include <threads.h>
+#ifdef ENABLE_RENDER_SERVER_WORKER_THREAD
+#include "c11/threads.h"
+#endif
 #include <unistd.h>
 
 struct minijail;
@@ -161,6 +168,48 @@ fork_minijail(const struct minijail *template)
 
 #ifndef ENABLE_RENDER_SERVER_WORKER_THREAD
 
+#ifdef __APPLE__
+static int
+create_sigchld_fd(void)
+{
+   /* On macOS, use kqueue to monitor SIGCHLD */
+   int kq = kqueue();
+   if (kq == -1) {
+      render_log("failed to create kqueue");
+      return -1;
+   }
+
+   /* Set up kqueue to monitor SIGCHLD signal */
+   struct kevent kev;
+   EV_SET(&kev, SIGCHLD, EVFILT_SIGNAL, EV_ADD, 0, 0, NULL);
+   
+   if (kevent(kq, &kev, 1, NULL, 0, NULL) == -1) {
+      render_log("failed to add SIGCHLD to kqueue");
+      close(kq);
+      return -1;
+   }
+
+   /* Block SIGCHLD so it's only handled through kqueue */
+   sigset_t set;
+   sigemptyset(&set);
+   sigaddset(&set, SIGCHLD);
+   if (sigprocmask(SIG_BLOCK, &set, NULL) == -1) {
+      render_log("failed to block SIGCHLD");
+      close(kq);
+      return -1;
+   }
+
+   /* Set kqueue to non-blocking mode */
+   int flags = fcntl(kq, F_GETFL);
+   if (flags == -1 || fcntl(kq, F_SETFL, flags | O_NONBLOCK) == -1) {
+      render_log("failed to set kqueue non-blocking");
+      close(kq);
+      return -1;
+   }
+
+   return kq;
+}
+#else
 static int
 create_sigchld_fd(void)
 {
@@ -196,6 +245,7 @@ create_sigchld_fd(void)
 
    return fd;
 }
+#endif /* __APPLE__ */
 
 #endif /* !ENABLE_RENDER_SERVER_WORKER_THREAD */
 
@@ -321,6 +371,31 @@ render_worker_jail_drain_sigchld_fd(struct render_worker_jail *jail)
    if (jail->sigchld_fd < 0)
       return true;
 
+#ifdef __APPLE__
+   /* On macOS, drain kqueue events */
+   do {
+      struct kevent events[8];
+      struct timespec timeout = {0, 0}; /* non-blocking */
+      int nevents = kevent(jail->sigchld_fd, NULL, 0, events, 8, &timeout);
+      
+      if (nevents == -1) {
+         if (errno == EINTR)
+            continue;
+         render_log("failed to read kqueue events");
+         return false;
+      }
+      
+      if (nevents == 0)
+         break; /* no more events */
+         
+      /* Process SIGCHLD events - we don't need to do anything special
+       * as the signal count is in events[i].data, but we just need to drain */
+      if (nevents < 8)
+         break; /* got fewer events than requested, so we're done */
+         
+   } while (true);
+#else
+   /* On Linux, drain signalfd */
    do {
       struct signalfd_siginfo siginfos[8];
       const ssize_t r = read(jail->sigchld_fd, siginfos, sizeof(siginfos));
@@ -332,6 +407,7 @@ render_worker_jail_drain_sigchld_fd(struct render_worker_jail *jail)
       render_log("failed to read signalfd");
       return false;
    } while (true);
+#endif
 
    return true;
 }
diff --git a/src/meson.build b/src/meson.build
index 35ba7ec..efbc633 100644
--- a/src/meson.build
+++ b/src/meson.build
@@ -60,6 +60,10 @@ vrend_winsys_glx_sources = [
    'vrend/vrend_winsys_glx.c',
 ]
 
+vrend_metal_sources = [
+   'vrend/vrend_metal.m',
+]
+
 venus_sources = [
    'venus/vkr_acceleration_structure.c',
    'venus/vkr_allocator.c',
@@ -153,6 +157,16 @@ proxy_sources = [
    'proxy/proxy_socket.c',
 ]
 
+server_sources = [
+   '../server/render_client.c',
+   '../server/render_common.c',
+   '../server/render_context.c',
+   '../server/render_server.c',
+   '../server/render_socket.c',
+   '../server/render_state.c',
+   '../server/render_worker.c',
+]
+
 video_sources = [
    'vrend/virgl_video.c',
    'vrend/vrend_video.c',
@@ -194,10 +208,15 @@ if have_glx
    virgl_depends += [glx_dep]
 endif
 
+if metal_dep.found()
+   virgl_sources += vrend_metal_sources
+   virgl_depends += [metal_dep]
+endif
+
 if with_venus
    virgl_sources += venus_sources
    virgl_sources += venus_codegen
-   virgl_depends += [venus_dep]
+   virgl_depends += [venus_dep]
 endif
 
 if with_drm_renderers
@@ -229,6 +248,10 @@ if with_render_server
    virgl_sources += proxy_sources
 endif
 
+if with_render_server_worker == 'thread'
+   virgl_sources += server_sources
+endif
+
 if with_video
   virgl_sources += video_sources
   virgl_depends += [libva_dep, libvadrm_dep]
diff --git a/src/proxy/proxy_client.c b/src/proxy/proxy_client.c
index 97e025f..fcdd5d8 100644
--- a/src/proxy/proxy_client.c
+++ b/src/proxy/proxy_client.c
@@ -55,11 +55,6 @@ proxy_client_create_context(struct proxy_client *client,
       return false;
    }
 
-   if (!proxy_socket_is_seqpacket(ctx_fd)) {
-      close(ctx_fd);
-      return false;
-   }
-
    *out_ctx_fd = ctx_fd;
    return true;
 }
diff --git a/src/proxy/proxy_context.c b/src/proxy/proxy_context.c
index dc23ef5..b145cff 100644
--- a/src/proxy/proxy_context.c
+++ b/src/proxy/proxy_context.c
@@ -367,7 +367,16 @@ proxy_context_get_blob(struct virgl_context *base,
       return -1;
    }
 
-   if (!reply_fd_count) {
+   if (reply.fd_type == VIRGL_RESOURCE_METAL_HEAP) {
+      blob->type = reply.fd_type;
+      blob->u.metal_heap = reply.res_ptr;
+      blob->map_info = reply.map_info;
+      blob->vulkan_info = reply.vulkan_info;
+
+      proxy_context_resource_add(ctx, res_id);
+
+      return 0;
+   } else if (!reply_fd_count) {
       proxy_log("invalid reply for blob %" PRIu64, blob_id);
       return -1;
    }
diff --git a/src/proxy/proxy_renderer.c b/src/proxy/proxy_renderer.c
index 59c2d20..2c2d04b 100644
--- a/src/proxy/proxy_renderer.c
+++ b/src/proxy/proxy_renderer.c
@@ -15,12 +15,14 @@
 int
 proxy_renderer_init(const struct proxy_renderer_cbs *cbs, uint32_t flags)
 {
+   bool in_process = !(flags & VIRGL_RENDERER_RENDER_SERVER);
+
    assert(flags & VIRGL_RENDERER_NO_VIRGL);
 
    proxy_renderer.cbs = cbs;
    proxy_renderer.flags = flags;
 
-   proxy_renderer.server = proxy_server_create();
+   proxy_renderer.server = proxy_server_create(in_process);
    if (!proxy_renderer.server)
       goto fail;
 
diff --git a/src/proxy/proxy_server.c b/src/proxy/proxy_server.c
index c9ea27e..e66a449 100644
--- a/src/proxy/proxy_server.c
+++ b/src/proxy/proxy_server.c
@@ -9,7 +9,9 @@
 #include <sys/wait.h>
 #include <unistd.h>
 
+#include "server/render_context.h"
 #include "server/render_protocol.h"
+#include "server/render_server.h"
 
 int
 proxy_server_connect(struct proxy_server *srv)
@@ -33,6 +35,13 @@ proxy_server_destroy(struct proxy_server *srv)
    if (srv->client_fd >= 0)
       close(srv->client_fd);
 
+#ifdef ENABLE_RENDER_SERVER_WORKER_THREAD
+   if (srv->in_process) {
+      thrd_join(srv->thread, NULL);
+      srv->in_process = false;
+   }
+#endif
+
    free(srv);
 }
 
@@ -95,8 +104,54 @@ proxy_server_init_fd(struct proxy_server *srv)
    return true;
 }
 
+#ifdef ENABLE_RENDER_SERVER_WORKER_THREAD
+static int
+proxy_server_start_thread(void *args)
+{
+   int remote_fd = (int)(uintptr_t)args;
+   char fd_str[16];
+   snprintf(fd_str, sizeof(fd_str), "%d", remote_fd);
+   char *argv[] = {
+      RENDER_SERVER_EXEC_PATH,
+      "--socket-fd",
+      fd_str,
+      NULL,
+   };
+   struct render_context_args ctx_args;
+
+   ctx_args.in_process = true;
+   bool ok = render_server_main(3, argv, &ctx_args);
+
+   return ok ? 0 : -1;
+}
+
+static bool
+proxy_server_init_thread(struct proxy_server *srv)
+{
+   int socket_fds[2];
+
+   if (!proxy_socket_pair(socket_fds))
+      return false;
+
+   const int client_fd = socket_fds[0];
+   const uintptr_t remote_fd = socket_fds[1];
+
+   bool ok = thrd_create(&srv->thread, proxy_server_start_thread, (void *)remote_fd) == thrd_success;
+
+   if (ok) {
+      srv->client_fd = client_fd;
+      srv->in_process = true;
+   } else {
+      close(client_fd);
+      close(remote_fd);
+   }
+
+   return ok;
+}
+#endif
+
 struct proxy_server *
-proxy_server_create(void)
+proxy_server_create(bool in_process)
 {
    struct proxy_server *srv = calloc(1, sizeof(*srv));
    if (!srv)
@@ -104,7 +159,18 @@ proxy_server_create(void)
 
    srv->pid = -1;
 
-   if (!proxy_server_init_fd(srv)) {
+   if (in_process) {
+#ifdef ENABLE_RENDER_SERVER_WORKER_THREAD
+      if (!proxy_server_init_thread(srv)) {
+         free(srv);
+         return NULL;
+      }
+#else
+      proxy_log("in process server not supported");
+      free(srv);
+      return NULL;
+#endif
+   } else if (!proxy_server_init_fd(srv)) {
       /* start the render server on demand when the client does not provide a
        * server fd
        */
@@ -114,13 +180,6 @@ proxy_server_create(void)
       }
    }
 
-   if (!proxy_socket_is_seqpacket(srv->client_fd)) {
-      proxy_log("invalid client fd type");
-      close(srv->client_fd);
-      free(srv);
-      return NULL;
-   }
-
    proxy_log("proxy server with pid %d", srv->pid);
 
    return srv;
diff --git a/src/proxy/proxy_server.h b/src/proxy/proxy_server.h
index 1b5ca02..df7dbcc 100644
--- a/src/proxy/proxy_server.h
+++ b/src/proxy/proxy_server.h
@@ -10,13 +10,21 @@
 
 #include <sys/types.h>
 
+#ifdef ENABLE_RENDER_SERVER_WORKER_THREAD
+#include "c11/threads.h"
+#endif
+
 struct proxy_server {
    pid_t pid;
    int client_fd;
+#ifdef ENABLE_RENDER_SERVER_WORKER_THREAD
+   bool in_process;
+   thrd_t thread;
+#endif
 };
 
 struct proxy_server *
-proxy_server_create(void);
+proxy_server_create(bool in_process);
 
 void
 proxy_server_destroy(struct proxy_server *srv);
diff --git a/src/proxy/proxy_socket.c b/src/proxy/proxy_socket.c
index 51223c6..eb81cbf 100644
--- a/src/proxy/proxy_socket.c
+++ b/src/proxy/proxy_socket.c
@@ -4,6 +4,7 @@
  */
 
 #include "proxy_socket.h"
+#include "server/render_protocol.h"
 
 #include <poll.h>
 #include <sys/socket.h>
@@ -13,11 +14,40 @@
 
 #define PROXY_SOCKET_MAX_FD_COUNT 8
 
+#ifndef MSG_CMSG_CLOEXEC
+#include <fcntl.h>
+static int
+proxy_socket_set_cloexec(int fd)
+{
+   long flags;
+
+   if (fd == -1)
+      return -1;
+
+   flags = fcntl(fd, F_GETFD);
+   if (flags == -1)
+      goto err;
+
+   if (fcntl(fd, F_SETFD, flags | FD_CLOEXEC) == -1)
+      goto err;
+
+   return 0;
+
+err:
+   return -1;
+}
+#endif
+
 /* this is only used when the render server is started on demand */
 bool
 proxy_socket_pair(int out_fds[static 2])
 {
-   int ret = socketpair(AF_UNIX, SOCK_SEQPACKET, 0, out_fds);
+#ifdef __APPLE__
+   int type = SOCK_SEQPACKET;
+#else
+   int type = SOCK_SEQPACKET;
+#endif
+   int ret = socketpair(AF_UNIX, type, 0, out_fds);
    if (ret) {
       proxy_log("failed to create socket pair");
       return false;
@@ -41,10 +71,12 @@ proxy_socket_is_seqpacket(int fd)
 void
 proxy_socket_init(struct proxy_socket *socket, int fd)
 {
+   bool is_seqpacket = proxy_socket_is_seqpacket(fd);
    /* TODO make fd non-blocking and perform io with timeout */
    assert(fd >= 0);
    *socket = (struct proxy_socket){
       .fd = fd,
+      .is_seqpacket = is_seqpacket,
    };
 }
 
@@ -96,11 +128,45 @@ get_received_fds(const struct msghdr *msg, int *out_count)
    return (const int *)CMSG_DATA(cmsg);
 }
 
+enum socket_state {
+   SOCKET_STATE_FIRST_MSG,
+   SOCKET_STATE_HEADER,
+   SOCKET_STATE_DATA,
+};
+
 static bool
 proxy_socket_recvmsg(struct proxy_socket *socket, struct msghdr *msg)
 {
+   int flags = 0;
+#ifdef MSG_CMSG_CLOEXEC
+   flags = MSG_CMSG_CLOEXEC;
+#endif
+
+   enum socket_state state = SOCKET_STATE_FIRST_MSG;
+   struct render_context_socket_header hdr = {0};
+   ssize_t want = sizeof(hdr);
+   struct msghdr _msg = {
+      .msg_iov =
+         &(struct iovec){
+            .iov_base = &hdr,
+            .iov_len = want,
+         },
+      .msg_iovlen = 1,
+      .msg_control = msg->msg_control,
+      .msg_controllen = msg->msg_controllen,
+   };
+	socklen_t _msg_controllen;
+
+   assert(msg->msg_iovlen == 1);
+
+   if (socket->is_seqpacket) {
+      _msg.msg_iov[0].iov_base = msg->msg_iov[0].iov_base;
+      _msg.msg_iov[0].iov_len = msg->msg_iov[0].iov_len;
+      want = 0;
+   }
+
    do {
-      const ssize_t s = recvmsg(socket->fd, msg, MSG_CMSG_CLOEXEC);
+      const ssize_t s = recvmsg(socket->fd, &_msg, flags);
       if (unlikely(s < 0)) {
          if (errno == EAGAIN || errno == EINTR)
             continue;
@@ -109,21 +175,61 @@ proxy_socket_recvmsg(struct proxy_socket *socket, struct msghdr *msg)
          return false;
       }
 
-      assert(msg->msg_iovlen == 1);
-      if (unlikely((msg->msg_flags & (MSG_TRUNC | MSG_CTRUNC)) ||
-                   msg->msg_iov[0].iov_len != (size_t)s)) {
+      if (state == SOCKET_STATE_FIRST_MSG) {
+         _msg_controllen = _msg.msg_controllen;
+         state = socket->is_seqpacket ? SOCKET_STATE_DATA : SOCKET_STATE_HEADER;
+      } else {
+         /* retain the cmsg from first message */
+         assert(_msg.msg_controllen == 0);
+      }
+
+      if (unlikely(_msg.msg_flags & MSG_CTRUNC ||
+                   (socket->is_seqpacket &&
+                     (_msg.msg_flags & MSG_TRUNC) ||
+                      _msg.msg_iov[0].iov_len != (size_t)s))) {
          proxy_log("failed to receive message: truncated or incomplete");
 
          int fd_count;
-         const int *fds = get_received_fds(msg, &fd_count);
+         const int *fds = get_received_fds(&_msg, &fd_count);
          for (int i = 0; i < fd_count; i++)
             close(fds[i]);
 
          return false;
       }
 
-      return true;
+      if (s <= want) {
+         _msg.msg_iov[0].iov_base = (char *)_msg.msg_iov[0].iov_base + s;
+         _msg.msg_iov[0].iov_len -= s;
+         want -= s;
+      }
+
+      if (!want && state == SOCKET_STATE_HEADER) {
+         want = ntohl(hdr.length);
+         _msg.msg_iov[0].iov_base = msg->msg_iov[0].iov_base;
+         _msg.msg_iov[0].iov_len = want;
+         state = SOCKET_STATE_DATA;
+      } else if (!want && state == SOCKET_STATE_DATA) {
+         msg->msg_controllen = _msg_controllen;
+         break;
+      }
    } while (true);
+
+#ifndef MSG_CMSG_CLOEXEC
+   int fd_count;
+   int ret = 0;
+   const int *fds = get_received_fds(msg, &fd_count);
+   for (int i = 0; !ret && i < fd_count; i++) {
+      ret = proxy_socket_set_cloexec(fds[i]);
+   }
+   if (ret) {
+      for (int i = 0; i < fd_count; i++) {
+         close(fds[i]);
+      }
+      return false;
+   }
+#endif
+
+   return true;
 }
 
 static bool
@@ -192,8 +298,32 @@ proxy_socket_receive_reply_with_fds(struct proxy_socket *socket,
 static bool
 proxy_socket_sendmsg(struct proxy_socket *socket, const struct msghdr *msg)
 {
+   enum socket_state state = SOCKET_STATE_FIRST_MSG;
+   struct render_context_socket_header hdr = {
+      .length = htonl(msg->msg_iov[0].iov_len),
+   };
+   ssize_t want = sizeof(hdr);
+   struct msghdr _msg = {
+      .msg_iov =
+         &(struct iovec){
+            .iov_base = &hdr,
+            .iov_len = want,
+         },
+      .msg_iovlen = 1,
+      .msg_control = msg->msg_control,
+      .msg_controllen = msg->msg_controllen,
+   };
+
+   assert(msg->msg_iovlen == 1);
+
+   if (socket->is_seqpacket) {
+      _msg.msg_iov[0].iov_base = msg->msg_iov[0].iov_base;
+      _msg.msg_iov[0].iov_len = msg->msg_iov[0].iov_len;
+      want = 0;
+   }
+
    do {
-      const ssize_t s = sendmsg(socket->fd, msg, MSG_NOSIGNAL);
+      const ssize_t s = sendmsg(socket->fd, &_msg, MSG_NOSIGNAL);
       if (unlikely(s < 0)) {
          if (errno == EAGAIN || errno == EINTR)
             continue;
@@ -202,9 +332,30 @@ proxy_socket_sendmsg(struct proxy_socket *socket, const struct msghdr *msg)
          return false;
       }
 
-      /* no partial send since the socket type is SOCK_SEQPACKET */
-      assert(msg->msg_iovlen == 1 && msg->msg_iov[0].iov_len == (size_t)s);
-      return true;
+      if (socket->is_seqpacket) {
+         /* no partial send since the socket type is SOCK_SEQPACKET */
+         assert(_msg.msg_iovlen == 1 && _msg.msg_iov[0].iov_len == (size_t)s);
+         state = SOCKET_STATE_DATA;
+      } else if (state == SOCKET_STATE_FIRST_MSG) {
+         _msg.msg_controllen = 0;
+         _msg.msg_control = NULL;
+         state = SOCKET_STATE_HEADER;
+      }
+
+      if (s <= want) {
+         _msg.msg_iov[0].iov_base = (char *)_msg.msg_iov[0].iov_base + s;
+         _msg.msg_iov[0].iov_len -= s;
+         want -= s;
+      }
+
+      if (!want && state == SOCKET_STATE_HEADER) {
+         want = ntohl(hdr.length);
+         _msg.msg_iov[0].iov_base = msg->msg_iov[0].iov_base;
+         _msg.msg_iov[0].iov_len = want;
+         state = SOCKET_STATE_DATA;
+      } else if (!want && state == SOCKET_STATE_DATA) {
+         return true;
+      }
    } while (true);
 }
 
diff --git a/src/proxy/proxy_socket.h b/src/proxy/proxy_socket.h
index 04e2f54..7cf02e9 100644
--- a/src/proxy/proxy_socket.h
+++ b/src/proxy/proxy_socket.h
@@ -10,6 +10,7 @@
 
 struct proxy_socket {
    int fd;
+   bool is_seqpacket;
 };
 
 bool
diff --git a/src/venus/venus-protocol/vn_protocol_renderer.h b/src/venus/venus-protocol/vn_protocol_renderer.h
index 46718c3..4c7342f 100644
--- a/src/venus/venus-protocol/vn_protocol_renderer.h
+++ b/src/venus/venus-protocol/vn_protocol_renderer.h
@@ -1,4 +1,4 @@
-/* This file is generated by venus-protocol git-9fa07f3c. */
+/* This file is generated by venus-protocol git-ce1b3c7c. */
 
 /*
  * Copyright 2020 Google LLC
diff --git a/src/venus/venus-protocol/vn_protocol_renderer_defines.h b/src/venus/venus-protocol/vn_protocol_renderer_defines.h
index 866bba2..d06c4be 100644
--- a/src/venus/venus-protocol/vn_protocol_renderer_defines.h
+++ b/src/venus/venus-protocol/vn_protocol_renderer_defines.h
@@ -465,7 +465,9 @@ typedef enum VkCommandTypeEXT {
     VK_COMMAND_TYPE_vkCmdSetAttachmentFeedbackLoopEnableEXT_EXT = 329,
     VK_COMMAND_TYPE_vkCmdSetDepthClampRangeEXT_EXT = 330,
     VK_COMMAND_TYPE_vkGetPhysicalDeviceCooperativeMatrixPropertiesKHR_EXT = 331,
+    VK_COMMAND_TYPE_vkGetMemoryMetalHandleEXT_EXT = 331,
     VK_COMMAND_TYPE_vkCmdDrawMeshTasksEXT_EXT = 332,
+    VK_COMMAND_TYPE_vkGetMemoryMetalHandlePropertiesEXT_EXT = 332,
     VK_COMMAND_TYPE_vkCmdDrawMeshTasksIndirectEXT_EXT = 333,
     VK_COMMAND_TYPE_vkCmdDrawMeshTasksIndirectCountEXT_EXT = 334,
 } VkCommandTypeEXT;
@@ -2869,6 +2871,23 @@ struct vn_command_vkCmdSetDepthClampRangeEXT {
     const VkDepthClampRangeEXT* pDepthClampRange;
 };
 
+struct vn_command_vkGetMemoryMetalHandleEXT {
+    VkDevice device;
+    const VkMemoryGetMetalHandleInfoEXT* pGetMetalHandleInfo;
+    void** pHandle;
+
+    VkResult ret;
+};
+
+struct vn_command_vkGetMemoryMetalHandlePropertiesEXT {
+    VkDevice device;
+    VkExternalMemoryHandleTypeFlagBits handleType;
+    const void* pHandle;
+    VkMemoryMetalHandlePropertiesEXT* pMemoryMetalHandleProperties;
+
+    VkResult ret;
+};
+
 struct vn_command_vkSetReplyCommandStreamMESA {
     const VkCommandStreamDescriptionMESA* pStream;
 };
@@ -3285,6 +3304,8 @@ struct vn_dispatch_context {
     void (*dispatch_vkCmdSetRenderingAttachmentLocations)(struct vn_dispatch_context *ctx, struct vn_command_vkCmdSetRenderingAttachmentLocations *args);
     void (*dispatch_vkCmdSetRenderingInputAttachmentIndices)(struct vn_dispatch_context *ctx, struct vn_command_vkCmdSetRenderingInputAttachmentIndices *args);
     void (*dispatch_vkCmdSetDepthClampRangeEXT)(struct vn_dispatch_context *ctx, struct vn_command_vkCmdSetDepthClampRangeEXT *args);
+    void (*dispatch_vkGetMemoryMetalHandleEXT)(struct vn_dispatch_context *ctx, struct vn_command_vkGetMemoryMetalHandleEXT *args);
+    void (*dispatch_vkGetMemoryMetalHandlePropertiesEXT)(struct vn_dispatch_context *ctx, struct vn_command_vkGetMemoryMetalHandlePropertiesEXT *args);
     void (*dispatch_vkSetReplyCommandStreamMESA)(struct vn_dispatch_context *ctx, struct vn_command_vkSetReplyCommandStreamMESA *args);
     void (*dispatch_vkSeekReplyCommandStreamMESA)(struct vn_dispatch_context *ctx, struct vn_command_vkSeekReplyCommandStreamMESA *args);
     void (*dispatch_vkExecuteCommandStreamsMESA)(struct vn_dispatch_context *ctx, struct vn_command_vkExecuteCommandStreamsMESA *args);
diff --git a/src/venus/venus-protocol/vn_protocol_renderer_device_memory.h b/src/venus/venus-protocol/vn_protocol_renderer_device_memory.h
index 5f3f19d..c3f6235 100644
--- a/src/venus/venus-protocol/vn_protocol_renderer_device_memory.h
+++ b/src/venus/venus-protocol/vn_protocol_renderer_device_memory.h
@@ -25,6 +25,8 @@
  *   vkGetMemoryFdKHR
  *   vkGetMemoryFdPropertiesKHR
  *   vkMapMemory2
+ *   vkGetMemoryMetalHandleEXT
+ *   vkGetMemoryMetalHandlePropertiesEXT
  */
 
 /* struct VkExportMemoryAllocateInfo chain */
@@ -84,6 +86,69 @@ vn_replace_VkExportMemoryAllocateInfo_handle(VkExportMemoryAllocateInfo *val)
     } while (pnext);
 }
 
+/* struct VkImportMemoryMetalHandleInfoEXT chain */
+
+static inline void *
+vn_decode_VkImportMemoryMetalHandleInfoEXT_pnext_temp(struct vn_cs_decoder *dec)
+{
+    /* no known/supported struct */
+    if (vn_decode_simple_pointer(dec))
+        vn_cs_decoder_set_fatal(dec);
+    return NULL;
+}
+
+static inline void
+vn_decode_VkImportMemoryMetalHandleInfoEXT_self_temp(struct vn_cs_decoder *dec, VkImportMemoryMetalHandleInfoEXT *val)
+{
+    /* skip val->{sType,pNext} */
+    vn_decode_VkExternalMemoryHandleTypeFlagBits(dec, &val->handleType);
+    if (vn_decode_simple_pointer(dec)) {
+        vn_cs_decoder_set_fatal(dec);
+    } else {
+        val->handle = NULL;
+    }
+}
+
+static inline void
+vn_decode_VkImportMemoryMetalHandleInfoEXT_temp(struct vn_cs_decoder *dec, VkImportMemoryMetalHandleInfoEXT *val)
+{
+    VkStructureType stype;
+    vn_decode_VkStructureType(dec, &stype);
+    if (stype != VK_STRUCTURE_TYPE_IMPORT_MEMORY_METAL_HANDLE_INFO_EXT)
+        vn_cs_decoder_set_fatal(dec);
+
+    val->sType = stype;
+    val->pNext = vn_decode_VkImportMemoryMetalHandleInfoEXT_pnext_temp(dec);
+    vn_decode_VkImportMemoryMetalHandleInfoEXT_self_temp(dec, val);
+}
+
+static inline void
+vn_replace_VkImportMemoryMetalHandleInfoEXT_handle_self(VkImportMemoryMetalHandleInfoEXT *val)
+{
+    /* skip val->sType */
+    /* skip val->pNext */
+    /* skip val->handleType */
+    /* skip val->handle */
+}
+
+static inline void
+vn_replace_VkImportMemoryMetalHandleInfoEXT_handle(VkImportMemoryMetalHandleInfoEXT *val)
+{
+    struct VkBaseOutStructure *pnext = (struct VkBaseOutStructure *)val;
+
+    do {
+        switch ((int32_t)pnext->sType) {
+        case VK_STRUCTURE_TYPE_IMPORT_MEMORY_METAL_HANDLE_INFO_EXT:
+            vn_replace_VkImportMemoryMetalHandleInfoEXT_handle_self((VkImportMemoryMetalHandleInfoEXT *)pnext);
+            break;
+        default:
+            /* ignore unknown/unsupported struct */
+            break;
+        }
+        pnext = pnext->pNext;
+    } while (pnext);
+}
+
 /* struct VkMemoryAllocateFlagsInfo chain */
 
 static inline void *
@@ -337,6 +402,14 @@ vn_decode_VkMemoryAllocateInfo_pnext_temp(struct vn_cs_decoder *dec)
             vn_decode_VkExportMemoryAllocateInfo_self_temp(dec, (VkExportMemoryAllocateInfo *)pnext);
         }
         break;
+    case VK_STRUCTURE_TYPE_IMPORT_MEMORY_METAL_HANDLE_INFO_EXT:
+        pnext = vn_cs_decoder_alloc_temp(dec, sizeof(VkImportMemoryMetalHandleInfoEXT));
+        if (pnext) {
+            pnext->sType = stype;
+            pnext->pNext = vn_decode_VkMemoryAllocateInfo_pnext_temp(dec);
+            vn_decode_VkImportMemoryMetalHandleInfoEXT_self_temp(dec, (VkImportMemoryMetalHandleInfoEXT *)pnext);
+        }
+        break;
     case VK_STRUCTURE_TYPE_MEMORY_ALLOCATE_FLAGS_INFO:
         pnext = vn_cs_decoder_alloc_temp(dec, sizeof(VkMemoryAllocateFlagsInfo));
         if (pnext) {
@@ -423,6 +496,9 @@ vn_replace_VkMemoryAllocateInfo_handle(VkMemoryAllocateInfo *val)
         case VK_STRUCTURE_TYPE_EXPORT_MEMORY_ALLOCATE_INFO:
             vn_replace_VkExportMemoryAllocateInfo_handle_self((VkExportMemoryAllocateInfo *)pnext);
             break;
+        case VK_STRUCTURE_TYPE_IMPORT_MEMORY_METAL_HANDLE_INFO_EXT:
+            vn_replace_VkImportMemoryMetalHandleInfoEXT_handle_self((VkImportMemoryMetalHandleInfoEXT *)pnext);
+            break;
         case VK_STRUCTURE_TYPE_MEMORY_ALLOCATE_FLAGS_INFO:
             vn_replace_VkMemoryAllocateFlagsInfo_handle_self((VkMemoryAllocateFlagsInfo *)pnext);
             break;
diff --git a/src/venus/venus-protocol/vn_protocol_renderer_info.h b/src/venus/venus-protocol/vn_protocol_renderer_info.h
index f55dc34..6882b8f 100644
--- a/src/venus/venus-protocol/vn_protocol_renderer_info.h
+++ b/src/venus/venus-protocol/vn_protocol_renderer_info.h
@@ -12,7 +12,7 @@
 
 struct vn_info_extension_table {
    union {
-      bool enabled[185];
+      bool enabled[186];
       struct {
          bool ARM_rasterization_order_attachment_access;
          bool EXT_4444_formats;
@@ -40,6 +40,7 @@ struct vn_info_extension_table {
          bool EXT_extended_dynamic_state3;
          bool EXT_external_memory_acquire_unmodified;
          bool EXT_external_memory_dma_buf;
+         bool EXT_external_memory_metal;
          bool EXT_filter_cubic;
          bool EXT_fragment_shader_interlock;
          bool EXT_global_priority;
@@ -212,8 +213,8 @@ struct vn_info_extension {
 };
 
 /* sorted by extension names for bsearch */
-static const uint32_t _vn_info_extension_count = 185;
-static const struct vn_info_extension _vn_info_extensions[185] = {
+static const uint32_t _vn_info_extension_count = 186;
+static const struct vn_info_extension _vn_info_extensions[186] = {
    { "VK_ARM_rasterization_order_attachment_access", 343, 1 },
    { "VK_EXT_4444_formats", 341, 1 },
    { "VK_EXT_attachment_feedback_loop_dynamic_state", 525, 1 },
@@ -240,6 +241,7 @@ static const struct vn_info_extension _vn_info_extensions[185] = {
    { "VK_EXT_extended_dynamic_state3", 456, 2 },
    { "VK_EXT_external_memory_acquire_unmodified", 454, 1 },
    { "VK_EXT_external_memory_dma_buf", 126, 1 },
+   { "VK_EXT_external_memory_metal", 603, 1 },
    { "VK_EXT_filter_cubic", 171, 3 },
    { "VK_EXT_fragment_shader_interlock", 252, 1 },
    { "VK_EXT_global_priority", 175, 2 },
diff --git a/src/venus/venus-protocol/vn_protocol_renderer_util.h b/src/venus/venus-protocol/vn_protocol_renderer_util.h
index 21e4a06..6728ec4 100644
--- a/src/venus/venus-protocol/vn_protocol_renderer_util.h
+++ b/src/venus/venus-protocol/vn_protocol_renderer_util.h
@@ -300,6 +300,8 @@ struct vn_device_proc_table {
    PFN_vkGetImageSubresourceLayout2 GetImageSubresourceLayout2;
    PFN_vkGetMemoryFdKHR GetMemoryFdKHR;
    PFN_vkGetMemoryFdPropertiesKHR GetMemoryFdPropertiesKHR;
+   PFN_vkGetMemoryMetalHandleEXT GetMemoryMetalHandleEXT;
+   PFN_vkGetMemoryMetalHandlePropertiesEXT GetMemoryMetalHandlePropertiesEXT;
    PFN_vkGetPipelineCacheData GetPipelineCacheData;
    PFN_vkGetPrivateData GetPrivateData;
    PFN_vkGetQueryPoolResults GetQueryPoolResults;
@@ -1043,6 +1045,12 @@ vn_util_init_device_proc_table(VkDevice dev,
    proc_table->GetMemoryFdPropertiesKHR =
       ext_table->KHR_external_memory_fd ? VN_GDPA(dev, vkGetMemoryFdPropertiesKHR) :
       NULL;
+   proc_table->GetMemoryMetalHandleEXT =
+      ext_table->EXT_external_memory_metal ? VN_GDPA(dev, vkGetMemoryMetalHandleEXT) :
+      NULL;
+   proc_table->GetMemoryMetalHandlePropertiesEXT =
+      ext_table->EXT_external_memory_metal ? VN_GDPA(dev, vkGetMemoryMetalHandlePropertiesEXT) :
+      NULL;
    proc_table->GetPipelineCacheData = VN_GDPA(dev, vkGetPipelineCacheData);
    proc_table->GetPrivateData =
       api_version >= VK_API_VERSION_1_3 ? VN_GDPA(dev, vkGetPrivateData) :
diff --git a/src/venus/venus-protocol/vulkan.h b/src/venus/venus-protocol/vulkan.h
index 9457a52..359f026 100644
--- a/src/venus/venus-protocol/vulkan.h
+++ b/src/venus/venus-protocol/vulkan.h
@@ -28,9 +28,9 @@
 #include "vulkan_macos.h"
 #endif
 
-#ifdef VK_USE_PLATFORM_METAL_EXT
+//#ifdef VK_USE_PLATFORM_METAL_EXT
 #include "vulkan_metal.h"
-#endif
+//#endif
 
 #ifdef VK_USE_PLATFORM_VI_NN
 #include "vulkan_vi.h"
@@ -92,9 +92,9 @@
 #endif
 
 
-#ifdef VK_ENABLE_BETA_EXTENSIONS
+//#ifdef VK_ENABLE_BETA_EXTENSIONS
 #include "vulkan_beta.h"
-#endif
+//#endif
 
 #ifdef VK_USE_PLATFORM_OHOS
 #include "vulkan_ohos.h"
diff --git a/src/venus/venus-protocol/vulkan_beta.h b/src/venus/venus-protocol/vulkan_beta.h
new file mode 100644
index 0000000..867483d
--- /dev/null
+++ b/src/venus/venus-protocol/vulkan_beta.h
@@ -0,0 +1,226 @@
+#ifndef VULKAN_BETA_H_
+#define VULKAN_BETA_H_ 1
+
+/*
+** Copyright 2015-2025 The Khronos Group Inc.
+**
+** SPDX-License-Identifier: Apache-2.0
+*/
+
+/*
+** This header is generated from the Khronos Vulkan XML API Registry.
+**
+*/
+
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+
+
+// VK_KHR_portability_subset is a preprocessor guard. Do not pass it to API calls.
+#define VK_KHR_portability_subset 1
+#define VK_KHR_PORTABILITY_SUBSET_SPEC_VERSION 1
+#define VK_KHR_PORTABILITY_SUBSET_EXTENSION_NAME "VK_KHR_portability_subset"
+typedef struct VkPhysicalDevicePortabilitySubsetFeaturesKHR {
+    VkStructureType    sType;
+    void*              pNext;
+    VkBool32           constantAlphaColorBlendFactors;
+    VkBool32           events;
+    VkBool32           imageViewFormatReinterpretation;
+    VkBool32           imageViewFormatSwizzle;
+    VkBool32           imageView2DOn3DImage;
+    VkBool32           multisampleArrayImage;
+    VkBool32           mutableComparisonSamplers;
+    VkBool32           pointPolygons;
+    VkBool32           samplerMipLodBias;
+    VkBool32           separateStencilMaskRef;
+    VkBool32           shaderSampleRateInterpolationFunctions;
+    VkBool32           tessellationIsolines;
+    VkBool32           tessellationPointMode;
+    VkBool32           triangleFans;
+    VkBool32           vertexAttributeAccessBeyondStride;
+} VkPhysicalDevicePortabilitySubsetFeaturesKHR;
+
+typedef struct VkPhysicalDevicePortabilitySubsetPropertiesKHR {
+    VkStructureType    sType;
+    void*              pNext;
+    uint32_t           minVertexInputBindingStrideAlignment;
+} VkPhysicalDevicePortabilitySubsetPropertiesKHR;
+
+
+
+// VK_AMDX_shader_enqueue is a preprocessor guard. Do not pass it to API calls.
+#define VK_AMDX_shader_enqueue 1
+#define VK_AMDX_SHADER_ENQUEUE_SPEC_VERSION 2
+#define VK_AMDX_SHADER_ENQUEUE_EXTENSION_NAME "VK_AMDX_shader_enqueue"
+#define VK_SHADER_INDEX_UNUSED_AMDX       (~0U)
+typedef struct VkPhysicalDeviceShaderEnqueueFeaturesAMDX {
+    VkStructureType    sType;
+    void*              pNext;
+    VkBool32           shaderEnqueue;
+    VkBool32           shaderMeshEnqueue;
+} VkPhysicalDeviceShaderEnqueueFeaturesAMDX;
+
+typedef struct VkPhysicalDeviceShaderEnqueuePropertiesAMDX {
+    VkStructureType    sType;
+    void*              pNext;
+    uint32_t           maxExecutionGraphDepth;
+    uint32_t           maxExecutionGraphShaderOutputNodes;
+    uint32_t           maxExecutionGraphShaderPayloadSize;
+    uint32_t           maxExecutionGraphShaderPayloadCount;
+    uint32_t           executionGraphDispatchAddressAlignment;
+    uint32_t           maxExecutionGraphWorkgroupCount[3];
+    uint32_t           maxExecutionGraphWorkgroups;
+} VkPhysicalDeviceShaderEnqueuePropertiesAMDX;
+
+typedef struct VkExecutionGraphPipelineScratchSizeAMDX {
+    VkStructureType    sType;
+    void*              pNext;
+    VkDeviceSize       minSize;
+    VkDeviceSize       maxSize;
+    VkDeviceSize       sizeGranularity;
+} VkExecutionGraphPipelineScratchSizeAMDX;
+
+typedef struct VkExecutionGraphPipelineCreateInfoAMDX {
+    VkStructureType                           sType;
+    const void*                               pNext;
+    VkPipelineCreateFlags                     flags;
+    uint32_t                                  stageCount;
+    const VkPipelineShaderStageCreateInfo*    pStages;
+    const VkPipelineLibraryCreateInfoKHR*     pLibraryInfo;
+    VkPipelineLayout                          layout;
+    VkPipeline                                basePipelineHandle;
+    int32_t                                   basePipelineIndex;
+} VkExecutionGraphPipelineCreateInfoAMDX;
+
+typedef union VkDeviceOrHostAddressConstAMDX {
+    VkDeviceAddress    deviceAddress;
+    const void*        hostAddress;
+} VkDeviceOrHostAddressConstAMDX;
+
+typedef struct VkDispatchGraphInfoAMDX {
+    uint32_t                          nodeIndex;
+    uint32_t                          payloadCount;
+    VkDeviceOrHostAddressConstAMDX    payloads;
+    uint64_t                          payloadStride;
+} VkDispatchGraphInfoAMDX;
+
+typedef struct VkDispatchGraphCountInfoAMDX {
+    uint32_t                          count;
+    VkDeviceOrHostAddressConstAMDX    infos;
+    uint64_t                          stride;
+} VkDispatchGraphCountInfoAMDX;
+
+typedef struct VkPipelineShaderStageNodeCreateInfoAMDX {
+      VkStructureType    sType;
+    const void*          pNext;
+    const char*          pName;
+    uint32_t             index;
+} VkPipelineShaderStageNodeCreateInfoAMDX;
+
+typedef VkResult (VKAPI_PTR *PFN_vkCreateExecutionGraphPipelinesAMDX)(VkDevice                                        device, VkPipelineCache                 pipelineCache, uint32_t                                        createInfoCount, const VkExecutionGraphPipelineCreateInfoAMDX* pCreateInfos, const VkAllocationCallbacks*    pAllocator, VkPipeline*               pPipelines);
+typedef VkResult (VKAPI_PTR *PFN_vkGetExecutionGraphPipelineScratchSizeAMDX)(VkDevice                                        device, VkPipeline                                      executionGraph, VkExecutionGraphPipelineScratchSizeAMDX*        pSizeInfo);
+typedef VkResult (VKAPI_PTR *PFN_vkGetExecutionGraphPipelineNodeIndexAMDX)(VkDevice                                        device, VkPipeline                                      executionGraph, const VkPipelineShaderStageNodeCreateInfoAMDX*  pNodeInfo, uint32_t*                                       pNodeIndex);
+typedef void (VKAPI_PTR *PFN_vkCmdInitializeGraphScratchMemoryAMDX)(VkCommandBuffer                                 commandBuffer, VkPipeline                                      executionGraph, VkDeviceAddress                                 scratch, VkDeviceSize                                    scratchSize);
+typedef void (VKAPI_PTR *PFN_vkCmdDispatchGraphAMDX)(VkCommandBuffer                                 commandBuffer, VkDeviceAddress                                 scratch, VkDeviceSize                                    scratchSize, const VkDispatchGraphCountInfoAMDX*             pCountInfo);
+typedef void (VKAPI_PTR *PFN_vkCmdDispatchGraphIndirectAMDX)(VkCommandBuffer                                 commandBuffer, VkDeviceAddress                                 scratch, VkDeviceSize                                    scratchSize, const VkDispatchGraphCountInfoAMDX*             pCountInfo);
+typedef void (VKAPI_PTR *PFN_vkCmdDispatchGraphIndirectCountAMDX)(VkCommandBuffer                                 commandBuffer, VkDeviceAddress                                 scratch, VkDeviceSize                                    scratchSize, VkDeviceAddress                                 countInfo);
+
+#ifndef VK_NO_PROTOTYPES
+VKAPI_ATTR VkResult VKAPI_CALL vkCreateExecutionGraphPipelinesAMDX(
+    VkDevice                                    device,
+    VkPipelineCache                             pipelineCache,
+    uint32_t                                    createInfoCount,
+    const VkExecutionGraphPipelineCreateInfoAMDX* pCreateInfos,
+    const VkAllocationCallbacks*                pAllocator,
+    VkPipeline*                                 pPipelines);
+
+VKAPI_ATTR VkResult VKAPI_CALL vkGetExecutionGraphPipelineScratchSizeAMDX(
+    VkDevice                                    device,
+    VkPipeline                                  executionGraph,
+    VkExecutionGraphPipelineScratchSizeAMDX*    pSizeInfo);
+
+VKAPI_ATTR VkResult VKAPI_CALL vkGetExecutionGraphPipelineNodeIndexAMDX(
+    VkDevice                                    device,
+    VkPipeline                                  executionGraph,
+    const VkPipelineShaderStageNodeCreateInfoAMDX* pNodeInfo,
+    uint32_t*                                   pNodeIndex);
+
+VKAPI_ATTR void VKAPI_CALL vkCmdInitializeGraphScratchMemoryAMDX(
+    VkCommandBuffer                             commandBuffer,
+    VkPipeline                                  executionGraph,
+    VkDeviceAddress                             scratch,
+    VkDeviceSize                                scratchSize);
+
+VKAPI_ATTR void VKAPI_CALL vkCmdDispatchGraphAMDX(
+    VkCommandBuffer                             commandBuffer,
+    VkDeviceAddress                             scratch,
+    VkDeviceSize                                scratchSize,
+    const VkDispatchGraphCountInfoAMDX*         pCountInfo);
+
+VKAPI_ATTR void VKAPI_CALL vkCmdDispatchGraphIndirectAMDX(
+    VkCommandBuffer                             commandBuffer,
+    VkDeviceAddress                             scratch,
+    VkDeviceSize                                scratchSize,
+    const VkDispatchGraphCountInfoAMDX*         pCountInfo);
+
+VKAPI_ATTR void VKAPI_CALL vkCmdDispatchGraphIndirectCountAMDX(
+    VkCommandBuffer                             commandBuffer,
+    VkDeviceAddress                             scratch,
+    VkDeviceSize                                scratchSize,
+    VkDeviceAddress                             countInfo);
+#endif
+
+
+// VK_NV_displacement_micromap is a preprocessor guard. Do not pass it to API calls.
+#define VK_NV_displacement_micromap 1
+#define VK_NV_DISPLACEMENT_MICROMAP_SPEC_VERSION 2
+#define VK_NV_DISPLACEMENT_MICROMAP_EXTENSION_NAME "VK_NV_displacement_micromap"
+
+typedef enum VkDisplacementMicromapFormatNV {
+    VK_DISPLACEMENT_MICROMAP_FORMAT_64_TRIANGLES_64_BYTES_NV = 1,
+    VK_DISPLACEMENT_MICROMAP_FORMAT_256_TRIANGLES_128_BYTES_NV = 2,
+    VK_DISPLACEMENT_MICROMAP_FORMAT_1024_TRIANGLES_128_BYTES_NV = 3,
+    VK_DISPLACEMENT_MICROMAP_FORMAT_MAX_ENUM_NV = 0x7FFFFFFF
+} VkDisplacementMicromapFormatNV;
+typedef struct VkPhysicalDeviceDisplacementMicromapFeaturesNV {
+    VkStructureType    sType;
+    void*              pNext;
+    VkBool32           displacementMicromap;
+} VkPhysicalDeviceDisplacementMicromapFeaturesNV;
+
+typedef struct VkPhysicalDeviceDisplacementMicromapPropertiesNV {
+    VkStructureType    sType;
+    void*              pNext;
+    uint32_t           maxDisplacementMicromapSubdivisionLevel;
+} VkPhysicalDeviceDisplacementMicromapPropertiesNV;
+
+typedef struct VkAccelerationStructureTrianglesDisplacementMicromapNV {
+    VkStructureType                     sType;
+    void*                               pNext;
+    VkFormat                            displacementBiasAndScaleFormat;
+    VkFormat                            displacementVectorFormat;
+    VkDeviceOrHostAddressConstKHR       displacementBiasAndScaleBuffer;
+    VkDeviceSize                        displacementBiasAndScaleStride;
+    VkDeviceOrHostAddressConstKHR       displacementVectorBuffer;
+    VkDeviceSize                        displacementVectorStride;
+    VkDeviceOrHostAddressConstKHR       displacedMicromapPrimitiveFlags;
+    VkDeviceSize                        displacedMicromapPrimitiveFlagsStride;
+    VkIndexType                         indexType;
+    VkDeviceOrHostAddressConstKHR       indexBuffer;
+    VkDeviceSize                        indexStride;
+    uint32_t                            baseTriangle;
+    uint32_t                            usageCountsCount;
+    const VkMicromapUsageEXT*           pUsageCounts;
+    const VkMicromapUsageEXT* const*    ppUsageCounts;
+    VkMicromapEXT                       micromap;
+} VkAccelerationStructureTrianglesDisplacementMicromapNV;
+
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif
diff --git a/src/venus/venus-protocol/vulkan_metal.h b/src/venus/venus-protocol/vulkan_metal.h
new file mode 100644
index 0000000..ab8c576
--- /dev/null
+++ b/src/venus/venus-protocol/vulkan_metal.h
@@ -0,0 +1,238 @@
+#ifndef VULKAN_METAL_H_
+#define VULKAN_METAL_H_ 1
+
+/*
+** Copyright 2015-2025 The Khronos Group Inc.
+**
+** SPDX-License-Identifier: Apache-2.0
+*/
+
+/*
+** This header is generated from the Khronos Vulkan XML API Registry.
+**
+*/
+
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+
+
+// VK_EXT_metal_surface is a preprocessor guard. Do not pass it to API calls.
+#define VK_EXT_metal_surface 1
+#ifdef __OBJC__
+@class CAMetalLayer;
+#else
+typedef void CAMetalLayer;
+#endif
+
+#define VK_EXT_METAL_SURFACE_SPEC_VERSION 1
+#define VK_EXT_METAL_SURFACE_EXTENSION_NAME "VK_EXT_metal_surface"
+typedef VkFlags VkMetalSurfaceCreateFlagsEXT;
+typedef struct VkMetalSurfaceCreateInfoEXT {
+    VkStructureType                 sType;
+    const void*                     pNext;
+    VkMetalSurfaceCreateFlagsEXT    flags;
+    const CAMetalLayer*             pLayer;
+} VkMetalSurfaceCreateInfoEXT;
+
+typedef VkResult (VKAPI_PTR *PFN_vkCreateMetalSurfaceEXT)(VkInstance instance, const VkMetalSurfaceCreateInfoEXT* pCreateInfo, const VkAllocationCallbacks* pAllocator, VkSurfaceKHR* pSurface);
+
+#ifndef VK_NO_PROTOTYPES
+VKAPI_ATTR VkResult VKAPI_CALL vkCreateMetalSurfaceEXT(
+    VkInstance                                  instance,
+    const VkMetalSurfaceCreateInfoEXT*          pCreateInfo,
+    const VkAllocationCallbacks*                pAllocator,
+    VkSurfaceKHR*                               pSurface);
+#endif
+
+
+// VK_EXT_metal_objects is a preprocessor guard. Do not pass it to API calls.
+#define VK_EXT_metal_objects 1
+#ifdef __OBJC__
+@protocol MTLDevice;
+typedef __unsafe_unretained id<MTLDevice> MTLDevice_id;
+#else
+typedef void* MTLDevice_id;
+#endif
+
+#ifdef __OBJC__
+@protocol MTLCommandQueue;
+typedef __unsafe_unretained id<MTLCommandQueue> MTLCommandQueue_id;
+#else
+typedef void* MTLCommandQueue_id;
+#endif
+
+#ifdef __OBJC__
+@protocol MTLBuffer;
+typedef __unsafe_unretained id<MTLBuffer> MTLBuffer_id;
+#else
+typedef void* MTLBuffer_id;
+#endif
+
+#ifdef __OBJC__
+@protocol MTLTexture;
+typedef __unsafe_unretained id<MTLTexture> MTLTexture_id;
+#else
+typedef void* MTLTexture_id;
+#endif
+
+typedef struct __IOSurface* IOSurfaceRef;
+#ifdef __OBJC__
+@protocol MTLSharedEvent;
+typedef __unsafe_unretained id<MTLSharedEvent> MTLSharedEvent_id;
+#else
+typedef void* MTLSharedEvent_id;
+#endif
+
+#define VK_EXT_METAL_OBJECTS_SPEC_VERSION 2
+#define VK_EXT_METAL_OBJECTS_EXTENSION_NAME "VK_EXT_metal_objects"
+
+typedef enum VkExportMetalObjectTypeFlagBitsEXT {
+    VK_EXPORT_METAL_OBJECT_TYPE_METAL_DEVICE_BIT_EXT = 0x00000001,
+    VK_EXPORT_METAL_OBJECT_TYPE_METAL_COMMAND_QUEUE_BIT_EXT = 0x00000002,
+    VK_EXPORT_METAL_OBJECT_TYPE_METAL_BUFFER_BIT_EXT = 0x00000004,
+    VK_EXPORT_METAL_OBJECT_TYPE_METAL_TEXTURE_BIT_EXT = 0x00000008,
+    VK_EXPORT_METAL_OBJECT_TYPE_METAL_IOSURFACE_BIT_EXT = 0x00000010,
+    VK_EXPORT_METAL_OBJECT_TYPE_METAL_SHARED_EVENT_BIT_EXT = 0x00000020,
+    VK_EXPORT_METAL_OBJECT_TYPE_FLAG_BITS_MAX_ENUM_EXT = 0x7FFFFFFF
+} VkExportMetalObjectTypeFlagBitsEXT;
+typedef VkFlags VkExportMetalObjectTypeFlagsEXT;
+typedef struct VkExportMetalObjectCreateInfoEXT {
+    VkStructureType                       sType;
+    const void*                           pNext;
+    VkExportMetalObjectTypeFlagBitsEXT    exportObjectType;
+} VkExportMetalObjectCreateInfoEXT;
+
+typedef struct VkExportMetalObjectsInfoEXT {
+    VkStructureType    sType;
+    const void*        pNext;
+} VkExportMetalObjectsInfoEXT;
+
+typedef struct VkExportMetalDeviceInfoEXT {
+    VkStructureType    sType;
+    const void*        pNext;
+    MTLDevice_id       mtlDevice;
+} VkExportMetalDeviceInfoEXT;
+
+typedef struct VkExportMetalCommandQueueInfoEXT {
+    VkStructureType       sType;
+    const void*           pNext;
+    VkQueue               queue;
+    MTLCommandQueue_id    mtlCommandQueue;
+} VkExportMetalCommandQueueInfoEXT;
+
+typedef struct VkExportMetalBufferInfoEXT {
+    VkStructureType    sType;
+    const void*        pNext;
+    VkDeviceMemory     memory;
+    MTLBuffer_id       mtlBuffer;
+} VkExportMetalBufferInfoEXT;
+
+typedef struct VkImportMetalBufferInfoEXT {
+    VkStructureType    sType;
+    const void*        pNext;
+    MTLBuffer_id       mtlBuffer;
+} VkImportMetalBufferInfoEXT;
+
+typedef struct VkExportMetalTextureInfoEXT {
+    VkStructureType          sType;
+    const void*              pNext;
+    VkImage                  image;
+    VkImageView              imageView;
+    VkBufferView             bufferView;
+    VkImageAspectFlagBits    plane;
+    MTLTexture_id            mtlTexture;
+} VkExportMetalTextureInfoEXT;
+
+typedef struct VkImportMetalTextureInfoEXT {
+    VkStructureType          sType;
+    const void*              pNext;
+    VkImageAspectFlagBits    plane;
+    MTLTexture_id            mtlTexture;
+} VkImportMetalTextureInfoEXT;
+
+typedef struct VkExportMetalIOSurfaceInfoEXT {
+    VkStructureType    sType;
+    const void*        pNext;
+    VkImage            image;
+    IOSurfaceRef       ioSurface;
+} VkExportMetalIOSurfaceInfoEXT;
+
+typedef struct VkImportMetalIOSurfaceInfoEXT {
+    VkStructureType    sType;
+    const void*        pNext;
+    IOSurfaceRef       ioSurface;
+} VkImportMetalIOSurfaceInfoEXT;
+
+typedef struct VkExportMetalSharedEventInfoEXT {
+    VkStructureType      sType;
+    const void*          pNext;
+    VkSemaphore          semaphore;
+    VkEvent              event;
+    MTLSharedEvent_id    mtlSharedEvent;
+} VkExportMetalSharedEventInfoEXT;
+
+typedef struct VkImportMetalSharedEventInfoEXT {
+    VkStructureType      sType;
+    const void*          pNext;
+    MTLSharedEvent_id    mtlSharedEvent;
+} VkImportMetalSharedEventInfoEXT;
+
+typedef void (VKAPI_PTR *PFN_vkExportMetalObjectsEXT)(VkDevice device, VkExportMetalObjectsInfoEXT* pMetalObjectsInfo);
+
+#ifndef VK_NO_PROTOTYPES
+VKAPI_ATTR void VKAPI_CALL vkExportMetalObjectsEXT(
+    VkDevice                                    device,
+    VkExportMetalObjectsInfoEXT*                pMetalObjectsInfo);
+#endif
+
+// TODO(b/417176273): this didn't made it into the spec, start using void*?
+typedef void* MTLResource_id;
+
+// VK_EXT_external_memory_metal is a preprocessor guard. Do not pass it to API calls.
+#define VK_EXT_external_memory_metal 1
+#define VK_EXT_EXTERNAL_MEMORY_METAL_SPEC_VERSION 1
+#define VK_EXT_EXTERNAL_MEMORY_METAL_EXTENSION_NAME "VK_EXT_external_memory_metal"
+typedef struct VkImportMemoryMetalHandleInfoEXT {
+    VkStructureType                       sType;
+    const void*                           pNext;
+    VkExternalMemoryHandleTypeFlagBits    handleType;
+    MTLResource_id                        handle;
+} VkImportMemoryMetalHandleInfoEXT;
+
+typedef struct VkMemoryMetalHandlePropertiesEXT {
+    VkStructureType    sType;
+    void*              pNext;
+    uint32_t           memoryTypeBits;
+} VkMemoryMetalHandlePropertiesEXT;
+
+typedef struct VkMemoryGetMetalHandleInfoEXT {
+    VkStructureType                       sType;
+    const void*                           pNext;
+    VkDeviceMemory                        memory;
+    VkExternalMemoryHandleTypeFlagBits    handleType;
+} VkMemoryGetMetalHandleInfoEXT;
+
+typedef VkResult (VKAPI_PTR *PFN_vkGetMemoryMetalHandleEXT)(VkDevice device, const VkMemoryGetMetalHandleInfoEXT* pGetMetalHandleInfo, MTLResource_id* pHandle);
+typedef VkResult (VKAPI_PTR *PFN_vkGetMemoryMetalHandlePropertiesEXT)(VkDevice device, VkExternalMemoryHandleTypeFlagBits handleType, const MTLResource_id pHandle, VkMemoryMetalHandlePropertiesEXT* pMemoryMetalHandleProperties);
+
+#ifndef VK_NO_PROTOTYPES
+VKAPI_ATTR VkResult VKAPI_CALL vkGetMemoryMetalHandleEXT(
+    VkDevice                                    device,
+    const VkMemoryGetMetalHandleInfoEXT*        pGetMetalHandleInfo,
+    void**                                      pHandle);
+
+VKAPI_ATTR VkResult VKAPI_CALL vkGetMemoryMetalHandlePropertiesEXT(
+    VkDevice                                    device,
+    VkExternalMemoryHandleTypeFlagBits          handleType,
+    const void*                                 pHandle,
+    VkMemoryMetalHandlePropertiesEXT*           pMemoryMetalHandleProperties);
+#endif
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif
diff --git a/src/venus/vkr_allocator.c b/src/venus/vkr_allocator.c
index ed25083..f87e033 100644
--- a/src/venus/vkr_allocator.c
+++ b/src/venus/vkr_allocator.c
@@ -52,6 +52,7 @@ struct vkr_inst_proc_table {
    PFN_vkGetPhysicalDeviceProperties2 GetPhysicalDeviceProperties2;
    PFN_vkCreateDevice CreateDevice;
    PFN_vkGetDeviceProcAddr GetDeviceProcAddr;
+   PFN_vkEnumerateDeviceExtensionProperties EnumerateDeviceExtensionProperties;
 };
 
 struct vkr_dev_proc_table {
@@ -109,6 +110,44 @@ vkr_allocator_get_dev_idx(struct virgl_resource *res)
    return VKR_ALLOCATOR_MAX_DEVICE_COUNT;
 }
 
+static const char *
+vkr_allocator_get_external_mem_ext(struct vkr_inst_proc_table *vk,
+                                   VkPhysicalDevice handle)
+{
+   VkExtensionProperties *exts;
+   uint32_t count;
+   VkResult result = vk->EnumerateDeviceExtensionProperties(handle, NULL, &count, NULL);
+   if (result != VK_SUCCESS)
+      return NULL;
+
+   exts = malloc(sizeof(*exts) * count);
+   if (!exts)
+      return NULL;
+
+   result = vk->EnumerateDeviceExtensionProperties(handle, NULL, &count, exts);
+   if (result != VK_SUCCESS) {
+      free(exts);
+      return NULL;
+   }
+
+   const char *name = NULL;
+   for (uint32_t i = 0; i < count; i++) {
+      VkExtensionProperties *props = &exts[i];
+
+      if (!strcmp(props->extensionName, VK_EXT_EXTERNAL_MEMORY_DMA_BUF_EXTENSION_NAME)) {
+         name = VK_EXT_EXTERNAL_MEMORY_DMA_BUF_EXTENSION_NAME;
+         break;
+      } else if (!strcmp(props->extensionName, VK_EXT_EXTERNAL_MEMORY_METAL_EXTENSION_NAME)) {
+         name = VK_EXT_EXTERNAL_MEMORY_METAL_EXTENSION_NAME;
+         break;
+      }
+   }
+
+   free(exts);
+
+   return name;
+}
+
 static struct vkr_opaque_fd_mem_info *
 vkr_allocator_allocate_memory(struct virgl_resource *res)
 {
@@ -123,26 +162,37 @@ vkr_allocator_allocate_memory(struct virgl_resource *res)
    struct vkr_dev_proc_table *vk = &vkr_allocator.proc_tables[idx];
 
    int fd = -1;
-   if (virgl_resource_export_fd(res, &fd) != VIRGL_RESOURCE_FD_OPAQUE) {
-      if (fd >= 0)
-         close(fd);
-      return NULL;
-   }
-
+   VkImportMemoryMetalHandleInfoEXT metal_info = { 0 };
+   VkImportMemoryFdInfoKHR fd_info = { 0 };
    VkMemoryAllocateInfo alloc_info = {
       .sType = VK_STRUCTURE_TYPE_MEMORY_ALLOCATE_INFO,
-      .pNext =
-         &(VkImportMemoryFdInfoKHR){ .sType = VK_STRUCTURE_TYPE_IMPORT_MEMORY_FD_INFO_KHR,
-                                     .handleType =
-                                        VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_FD_BIT,
-                                     .fd = fd },
       .allocationSize = res->vulkan_info.allocation_size,
       .memoryTypeIndex = res->vulkan_info.memory_type_index
    };
 
+   if (res->fd_type == VIRGL_RESOURCE_METAL_HEAP) {
+      metal_info = (VkImportMemoryMetalHandleInfoEXT){ .sType = VK_STRUCTURE_TYPE_IMPORT_MEMORY_METAL_HANDLE_INFO_EXT,
+                                                       .handle = res->metal_heap,
+                                                       .handleType = VK_EXTERNAL_MEMORY_HANDLE_TYPE_MTLHEAP_BIT_EXT };
+      alloc_info.pNext = &metal_info;
+   } else {
+      if (virgl_resource_export_fd(res, &fd) != VIRGL_RESOURCE_FD_OPAQUE) {
+         if (fd >= 0)
+            close(fd);
+         return NULL;
+      }
+
+      fd_info = (VkImportMemoryFdInfoKHR){ .sType = VK_STRUCTURE_TYPE_IMPORT_MEMORY_FD_INFO_KHR,
+                                           .handleType =
+                                             VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_FD_BIT,
+                                           .fd = fd };
+      alloc_info.pNext = &fd_info;
+   }
+
    VkDeviceMemory mem_handle;
    if (vk->AllocateMemory(dev_handle, &alloc_info, NULL, &mem_handle) != VK_SUCCESS) {
-      close(fd);
+      if (fd >= 0)
+         close(fd);
       return NULL;
    }
 
@@ -198,6 +248,7 @@ vkr_allocator_inst_proc_table_init(VkInstance inst_handle,
    vk->GetPhysicalDeviceProperties2 = VN_GIPA(vkGetPhysicalDeviceProperties2);
    vk->CreateDevice = VN_GIPA(vkCreateDevice);
    vk->GetDeviceProcAddr = VN_GIPA(vkGetDeviceProcAddr);
+   vk->EnumerateDeviceExtensionProperties = VN_GIPA(vkEnumerateDeviceExtensionProperties);
 #undef VN_GIPA
 }
 
@@ -218,11 +269,13 @@ vkr_allocator_dev_proc_table_init(VkDevice dev_handle,
 int
 vkr_allocator_init(void)
 {
-   static const char *required_extensions[] = {
-      "VK_KHR_external_memory_fd",
+   static const char *required_portability_exts[] = {
+      VK_KHR_PORTABILITY_ENUMERATION_EXTENSION_NAME,
    };
+   const char *required_extension;
    struct vkr_inst_proc_table *vk = &vkr_allocator.proc_table;
    VkResult res;
+   bool has_portability_enumeration = false;
 
    bool ret = vkr_library_load(&vkr_allocator.vulkan_library);
    if (!ret) {
@@ -232,6 +285,12 @@ vkr_allocator_init(void)
    /* Get vkGetInstanceProcAddr from libvulkan */
    PFN_vkGetInstanceProcAddr get_proc_addr = vkr_allocator.vulkan_library.GetInstanceProcAddr;
 
+   PFN_vkEnumerateInstanceExtensionProperties enum_inst_ext_props =
+      (PFN_vkEnumerateInstanceExtensionProperties)get_proc_addr(VK_NULL_HANDLE,
+                                                                "vkEnumerateInstanceExtensionProperties");
+
+   has_portability_enumeration = vkr_library_has_portability_enumeration(enum_inst_ext_props);
+
    VkApplicationInfo app_info = {
       .sType = VK_STRUCTURE_TYPE_APPLICATION_INFO,
       .apiVersion = VK_API_VERSION_1_1,
@@ -240,13 +299,16 @@ vkr_allocator_init(void)
    VkInstanceCreateInfo inst_info = {
       .sType = VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO,
       .pApplicationInfo = &app_info,
+      .flags = has_portability_enumeration ? VK_INSTANCE_CREATE_ENUMERATE_PORTABILITY_BIT_KHR : 0,
+      .enabledExtensionCount = has_portability_enumeration ? ARRAY_SIZE(required_portability_exts) : 0,
+      .ppEnabledExtensionNames = has_portability_enumeration ? required_portability_exts : NULL,
    };
 
    vk->CreateInstance =
       (PFN_vkCreateInstance)get_proc_addr(VK_NULL_HANDLE, "vkCreateInstance");
    res = vk->CreateInstance(&inst_info, NULL, &vkr_allocator.instance);
    if (res != VK_SUCCESS)
-      goto fail;
+      goto early_fail;
 
    vkr_allocator_inst_proc_table_init(vkr_allocator.instance, get_proc_addr, vk);
 
@@ -270,6 +332,11 @@ vkr_allocator_init(void)
 
       memcpy(vkr_allocator.device_uuids[i], id_props.deviceUUID, VK_UUID_SIZE);
 
+      required_extension = vkr_allocator_get_external_mem_ext(vk, physical_dev_handle);
+      if (!required_extension) {
+         continue;
+      }
+
       float priority = 1.0;
       VkDeviceQueueCreateInfo queue_info = {
          .sType = VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO,
@@ -284,8 +351,8 @@ vkr_allocator_init(void)
          .sType = VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO,
          .queueCreateInfoCount = 1,
          .pQueueCreateInfos = &queue_info,
-         .enabledExtensionCount = ARRAY_SIZE(required_extensions),
-         .ppEnabledExtensionNames = required_extensions,
+         .enabledExtensionCount = 1,
+         .ppEnabledExtensionNames = &required_extension,
       };
 
       res = vk->CreateDevice(physical_dev_handle, &dev_info, NULL,
@@ -312,6 +379,7 @@ fail:
    }
    vk->DestroyInstance(vkr_allocator.instance, NULL);
 
+early_fail:
    memset(&vkr_allocator, 0, sizeof(vkr_allocator));
 
    vkr_library_unload(&vkr_allocator.vulkan_library);
diff --git a/src/venus/vkr_buffer.c b/src/venus/vkr_buffer.c
index 9e466f8..aca760e 100644
--- a/src/venus/vkr_buffer.c
+++ b/src/venus/vkr_buffer.c
@@ -8,10 +8,37 @@
 #include "vkr_buffer_gen.h"
 #include "vkr_physical_device.h"
 
+static void
+vkr_buffer_fix_create_info(struct vkr_device *dev,
+                           VkBufferCreateInfo *pCreateInfo)
+{
+   VkExternalMemoryBufferCreateInfo *ext_create_info;
+
+   ext_create_info = vkr_find_struct(
+            pCreateInfo, VK_STRUCTURE_TYPE_EXTERNAL_MEMORY_BUFFER_CREATE_INFO);
+   if (ext_create_info) {
+      /* strip out dmabuf */
+      if ((ext_create_info->handleTypes & VK_EXTERNAL_MEMORY_HANDLE_TYPE_DMA_BUF_BIT_EXT) != 0) {
+         ext_create_info->handleTypes &= ~VK_EXTERNAL_MEMORY_HANDLE_TYPE_DMA_BUF_BIT_EXT;
+         /* add in supported handles */
+         if (dev->physical_device->is_metal_export_supported) {
+            ext_create_info->handleTypes |= VK_EXTERNAL_MEMORY_HANDLE_TYPE_MTLHEAP_BIT_EXT;
+         }
+      }
+   }
+}
+
 static void
 vkr_dispatch_vkCreateBuffer(struct vn_dispatch_context *dispatch,
                             struct vn_command_vkCreateBuffer *args)
 {
+   struct vkr_device *dev = vkr_device_from_handle(args->device);
+
+   /* if host does not natively support dmabuf we need to patch create info */
+   if (dev->physical_device->is_dma_buf_emulated) {
+      vkr_buffer_fix_create_info(dev, (VkBufferCreateInfo *)args->pCreateInfo);
+   }
+
    /* XXX If VkExternalMemoryBufferCreateInfo is chained by the app, all is
     * good.  If it is not chained, we might still bind an external memory to
     * the buffer, because vkr_dispatch_vkAllocateMemory makes any HOST_VISIBLE
@@ -139,6 +166,11 @@ vkr_dispatch_vkGetDeviceBufferMemoryRequirements(
    struct vkr_device *dev = vkr_device_from_handle(args->device);
    struct vn_device_proc_table *vk = &dev->proc_table;
 
+   /* if host does not natively support dmabuf we need to patch create info */
+   if (dev->physical_device->is_dma_buf_emulated) {
+      vkr_buffer_fix_create_info(dev, (VkBufferCreateInfo *)args->pInfo->pCreateInfo);
+   }
+
    vn_replace_vkGetDeviceBufferMemoryRequirements_args_handle(args);
    vk->GetDeviceBufferMemoryRequirements(args->device, args->pInfo,
                                          args->pMemoryRequirements);
diff --git a/src/venus/vkr_common.h b/src/venus/vkr_common.h
index e24a16f..0953e84 100644
--- a/src/venus/vkr_common.h
+++ b/src/venus/vkr_common.h
@@ -79,6 +79,11 @@
    if (name != _stack_##name)                                                            \
    free(name)
 
+/* Used for DRM format emulation when extension is not available */
+#ifndef DRM_FORMAT_MOD_LINEAR
+#define DRM_FORMAT_MOD_LINEAR (0)
+#endif
+
 struct vn_info_extension_table;
 struct vkr_context;
 struct vkr_ring;
diff --git a/src/venus/vkr_context.c b/src/venus/vkr_context.c
index 4bf526a..5f460b0 100644
--- a/src/venus/vkr_context.c
+++ b/src/venus/vkr_context.c
@@ -34,6 +34,13 @@
 #include "vkr_ring.h"
 #include "vkr_transport.h"
 
+#ifdef __APPLE__
+#include <CoreFoundation/CoreFoundation.h>
+#else
+#define CFRetain(x) (x)
+#define CFRelease(x)
+#endif
+
 void
 vkr_context_add_instance(struct vkr_context *ctx,
                          struct vkr_instance *instance,
@@ -189,6 +196,8 @@ vkr_context_free_resource(struct hash_entry *entry)
    struct vkr_resource *res = entry->data;
    if (res->fd_type == VIRGL_RESOURCE_FD_SHM)
       munmap(res->u.data, res->size);
+   else if (res->fd_type == VIRGL_RESOURCE_METAL_HEAP)
+      CFRelease(res->u.metal_heap);
    else if (res->u.fd >= 0)
       close(res->u.fd);
    free(res);
@@ -306,6 +315,34 @@ vkr_context_create_resource_from_shm(struct vkr_context *ctx,
    return true;
 }
 
+static bool
+vkr_context_import_resource_metal(struct vkr_context *ctx,
+                                  uint32_t res_id,
+                                  uint64_t blob_size,
+                                  enum virgl_resource_fd_type fd_type,
+                                  void *metal_heap)
+{
+   assert(!vkr_context_get_resource(ctx, res_id));
+   assert(fd_type == VIRGL_RESOURCE_METAL_HEAP);
+
+   struct vkr_resource *res = malloc(sizeof(*res));
+   if (!res)
+      return false;
+
+   res->res_id = res_id;
+   res->fd_type = fd_type;
+   res->size = blob_size;
+   res->u.metal_heap = (void *)CFRetain(metal_heap);
+
+   if (!vkr_context_add_resource(ctx, res)) {
+      CFRelease(metal_heap);
+      free(res);
+      return false;
+   }
+
+   return true;
+}
+
 static bool
 vkr_context_create_resource_from_device_memory(struct vkr_context *ctx,
                                                uint32_t res_id,
@@ -324,6 +361,11 @@ vkr_context_create_resource_from_device_memory(struct vkr_context *ctx,
    if (!vkr_device_memory_export_blob(mem, blob_size, blob_flags, &blob))
       return false;
 
+   if (blob.type == VIRGL_RESOURCE_METAL_HEAP) {
+      *out_blob = blob;
+      return vkr_context_import_resource_metal(ctx, res_id, blob_size, blob.type, blob.u.metal_heap);
+   }
+
    /* If memory might get exported, store a dup'ed fd in vkr_resource for:
     * - vkAllocateMemory for dma_buf import
     * - vkGetMemoryFdPropertiesKHR for dma_buf fd properties query
diff --git a/src/venus/vkr_context.h b/src/venus/vkr_context.h
index 12541a7..6f9bd83 100644
--- a/src/venus/vkr_context.h
+++ b/src/venus/vkr_context.h
@@ -29,6 +29,8 @@ struct vkr_resource {
       int fd;
       /* valid when fd_type is shm */
       uint8_t *data;
+      /* valid when fd_type is metal heap */
+      MTLResource_id metal_heap;
    } u;
 
    size_t size;
diff --git a/src/venus/vkr_device.c b/src/venus/vkr_device.c
index 9144180..cd0c786 100644
--- a/src/venus/vkr_device.c
+++ b/src/venus/vkr_device.c
@@ -130,30 +130,50 @@ vkr_dispatch_vkCreateDevice(struct vn_dispatch_context *dispatch,
    /* append extensions for our own use */
    const char **exts = NULL;
    uint32_t ext_count = args->pCreateInfo->enabledExtensionCount;
-   ext_count += physical_dev->KHR_external_memory_fd;
-   ext_count += physical_dev->EXT_external_memory_dma_buf;
-   ext_count += physical_dev->KHR_external_fence_fd;
-   if (ext_count > args->pCreateInfo->enabledExtensionCount) {
-      exts = malloc(sizeof(*exts) * ext_count);
-      if (!exts) {
-         args->ret = VK_ERROR_OUT_OF_HOST_MEMORY;
-         return;
-      }
-      for (uint32_t i = 0; i < args->pCreateInfo->enabledExtensionCount; i++)
-         exts[i] = args->pCreateInfo->ppEnabledExtensionNames[i];
-
-      ext_count = args->pCreateInfo->enabledExtensionCount;
-      if (physical_dev->KHR_external_memory_fd)
-         exts[ext_count++] = "VK_KHR_external_memory_fd";
-      if (physical_dev->EXT_external_memory_dma_buf)
-         exts[ext_count++] = "VK_EXT_external_memory_dma_buf";
-      if (physical_dev->KHR_external_fence_fd)
-         exts[ext_count++] = "VK_KHR_external_fence_fd";
-
-      ((VkDeviceCreateInfo *)args->pCreateInfo)->ppEnabledExtensionNames = exts;
-      ((VkDeviceCreateInfo *)args->pCreateInfo)->enabledExtensionCount = ext_count;
+   uint32_t add_count = 0;
+   add_count += physical_dev->KHR_external_memory_fd;
+   add_count += physical_dev->EXT_external_memory_dma_buf;
+   add_count += physical_dev->KHR_external_fence_fd;
+   add_count += physical_dev->EXT_external_memory_metal;
+   add_count += physical_dev->KHR_portability_subset;
+   exts = malloc(sizeof(*exts) * (ext_count + add_count));
+   if (!exts) {
+      args->ret = VK_ERROR_OUT_OF_HOST_MEMORY;
+      return;
+   }
+
+   /* skip any emulated extensions */
+   ext_count = 0;
+   for (uint32_t i = 0; i < args->pCreateInfo->enabledExtensionCount; i++) {
+      if (physical_dev->is_dma_buf_emulated &&
+            !strcmp(args->pCreateInfo->ppEnabledExtensionNames[i], VK_EXT_EXTERNAL_MEMORY_DMA_BUF_EXTENSION_NAME))
+         continue;
+      if (physical_dev->is_dma_buf_emulated &&
+            !strcmp(args->pCreateInfo->ppEnabledExtensionNames[i], VK_KHR_EXTERNAL_MEMORY_FD_EXTENSION_NAME))
+         continue;
+      if (!physical_dev->EXT_image_drm_format_modifier &&
+            !strcmp(args->pCreateInfo->ppEnabledExtensionNames[i], VK_EXT_IMAGE_DRM_FORMAT_MODIFIER_EXTENSION_NAME))
+         continue;
+      if (!physical_dev->EXT_queue_family_foreign &&
+            !strcmp(args->pCreateInfo->ppEnabledExtensionNames[i], VK_EXT_QUEUE_FAMILY_FOREIGN_EXTENSION_NAME))
+         continue;
+      exts[ext_count++] = args->pCreateInfo->ppEnabledExtensionNames[i];
    }
 
+   if (physical_dev->KHR_external_memory_fd)
+      exts[ext_count++] = VK_KHR_EXTERNAL_MEMORY_FD_EXTENSION_NAME;
+   if (physical_dev->EXT_external_memory_dma_buf)
+      exts[ext_count++] = VK_EXT_EXTERNAL_MEMORY_DMA_BUF_EXTENSION_NAME;
+   if (physical_dev->KHR_external_fence_fd)
+      exts[ext_count++] = VK_KHR_EXTERNAL_FENCE_FD_EXTENSION_NAME;
+   if (physical_dev->EXT_external_memory_metal)
+      exts[ext_count++] = VK_EXT_EXTERNAL_MEMORY_METAL_EXTENSION_NAME;
+   if (physical_dev->KHR_portability_subset)
+      exts[ext_count++] = VK_KHR_PORTABILITY_SUBSET_EXTENSION_NAME;
+
+   ((VkDeviceCreateInfo *)args->pCreateInfo)->ppEnabledExtensionNames = exts;
+   ((VkDeviceCreateInfo *)args->pCreateInfo)->enabledExtensionCount = ext_count;
+
    struct vkr_device *dev =
       vkr_context_alloc_object(ctx, sizeof(*dev), VK_OBJECT_TYPE_DEVICE, args->pDevice);
    if (!dev) {
diff --git a/src/venus/vkr_device_memory.c b/src/venus/vkr_device_memory.c
index ffe621b..504b655 100644
--- a/src/venus/vkr_device_memory.c
+++ b/src/venus/vkr_device_memory.c
@@ -47,6 +47,31 @@ vkr_get_fd_info_from_resource_info(struct vkr_context *ctx,
    return true;
 }
 
+static bool
+vkr_get_metal_info_from_resource_info(struct vkr_context *ctx,
+                                      const VkImportMemoryResourceInfoMESA *res_info,
+                                      VkImportMemoryMetalHandleInfoEXT *out)
+{
+   struct vkr_resource *res = vkr_context_get_resource(ctx, res_info->resourceId);
+   if (!res) {
+      vkr_log("failed to import resource: invalid res_id %u", res_info->resourceId);
+      vkr_context_set_fatal(ctx);
+      return false;
+   }
+
+   if (res->fd_type != VIRGL_RESOURCE_METAL_HEAP) {
+      return false;
+   }
+
+   *out = (VkImportMemoryMetalHandleInfoEXT){
+      .sType = VK_STRUCTURE_TYPE_IMPORT_MEMORY_METAL_HANDLE_INFO_EXT,
+      .pNext = res_info->pNext,
+      .handle = res->u.metal_heap,
+      .handleType = VK_EXTERNAL_MEMORY_HANDLE_TYPE_MTLHEAP_BIT_EXT,
+   };
+   return true;
+}
+
 #if defined(HAVE_LINUX_UDMABUF_H) && defined(HAVE_MEMFD_CREATE)
 #include <fcntl.h>
 #include <linux/udmabuf.h>
@@ -245,17 +270,22 @@ vkr_dispatch_vkAllocateMemory(struct vn_dispatch_context *dispatch,
 
    /* translate VkImportMemoryResourceInfoMESA into VkImportMemoryFdInfoKHR in place */
    VkImportMemoryFdInfoKHR local_import_info = { .fd = -1 };
+   VkImportMemoryMetalHandleInfoEXT local_metal_import_info = { 0 };
    VkImportMemoryResourceInfoMESA *res_info = NULL;
    VkBaseInStructure *prev_of_res_info = vkr_find_prev_struct(
       alloc_info, VK_STRUCTURE_TYPE_IMPORT_MEMORY_RESOURCE_INFO_MESA);
    if (prev_of_res_info) {
       res_info = (VkImportMemoryResourceInfoMESA *)prev_of_res_info->pNext;
       if (!vkr_get_fd_info_from_resource_info(ctx, res_info, &local_import_info)) {
-         args->ret = VK_ERROR_INVALID_EXTERNAL_HANDLE;
-         return;
+         if (!vkr_get_metal_info_from_resource_info(ctx, res_info, &local_metal_import_info)) {
+            args->ret = VK_ERROR_INVALID_EXTERNAL_HANDLE;
+            return;
+         } else {
+            prev_of_res_info->pNext = (const struct VkBaseInStructure *)&local_metal_import_info;
+         }
+      } else {
+         prev_of_res_info->pNext = (const struct VkBaseInStructure *)&local_import_info;
       }
-
-      prev_of_res_info->pNext = (const struct VkBaseInStructure *)&local_import_info;
    }
 
    VkExportMemoryAllocateInfo *export_info =
@@ -355,14 +385,33 @@ vkr_dispatch_vkAllocateMemory(struct vn_dispatch_context *dispatch,
 
          alloc_info->pNext = &local_import_info;
          valid_fd_types = 1 << VIRGL_RESOURCE_FD_DMABUF;
+      } else if (physical_dev->is_metal_export_supported) {
+         assert(physical_dev->is_dma_buf_emulated);
+         /* Align to 4KiB, which is what Linux expects */
+         alloc_info->allocationSize = align(alloc_info->allocationSize, 0x1000);
+         if (!export_info) {
+            local_export_info = (const VkExportMemoryAllocateInfo){
+               .sType = VK_STRUCTURE_TYPE_EXPORT_MEMORY_ALLOCATE_INFO,
+               .pNext = alloc_info->pNext,
+               .handleTypes = VK_EXTERNAL_MEMORY_HANDLE_TYPE_MTLHEAP_BIT_EXT,
+            };
+            export_info = &local_export_info;
+            alloc_info->pNext = &local_export_info;
+         }
       }
    }
 
    if (export_info) {
+      if (physical_dev->is_dma_buf_emulated && physical_dev->is_metal_export_supported) {
+         export_info->handleTypes &= ~VK_EXTERNAL_MEMORY_HANDLE_TYPE_DMA_BUF_BIT_EXT;
+         export_info->handleTypes |= VK_EXTERNAL_MEMORY_HANDLE_TYPE_MTLHEAP_BIT_EXT;
+      }
       if (export_info->handleTypes & VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_FD_BIT)
          valid_fd_types |= 1 << VIRGL_RESOURCE_FD_OPAQUE;
       if (export_info->handleTypes & VK_EXTERNAL_MEMORY_HANDLE_TYPE_DMA_BUF_BIT_EXT)
          valid_fd_types |= 1 << VIRGL_RESOURCE_FD_DMABUF;
+      if (export_info->handleTypes & VK_EXTERNAL_MEMORY_HANDLE_TYPE_MTLHEAP_BIT_EXT)
+         valid_fd_types |= 1 << VIRGL_RESOURCE_METAL_HEAP;
    }
 
    struct vkr_device_memory *mem = vkr_device_memory_create_and_add(ctx, args);
@@ -438,25 +487,40 @@ vkr_dispatch_vkGetMemoryResourcePropertiesMESA(
       return;
    }
 
-   if (res->fd_type != VIRGL_RESOURCE_FD_DMABUF) {
+   uint32_t memoryTypeBits;
+   vn_replace_vkGetMemoryResourcePropertiesMESA_args_handle(args);
+   if (res->fd_type == VIRGL_RESOURCE_FD_DMABUF) {
+      static const VkExternalMemoryHandleTypeFlagBits handle_type =
+         VK_EXTERNAL_MEMORY_HANDLE_TYPE_DMA_BUF_BIT_EXT;
+      VkMemoryFdPropertiesKHR mem_fd_props = {
+         .sType = VK_STRUCTURE_TYPE_MEMORY_FD_PROPERTIES_KHR,
+         .pNext = NULL,
+         .memoryTypeBits = 0,
+      };
+      args->ret =
+         vk->GetMemoryFdPropertiesKHR(args->device, handle_type, res->u.fd, &mem_fd_props);
+      if (args->ret != VK_SUCCESS)
+         return;
+      memoryTypeBits = mem_fd_props.memoryTypeBits;
+   } else if (res->fd_type == VIRGL_RESOURCE_METAL_HEAP) {
+      static const VkExternalMemoryHandleTypeFlagBits handle_type =
+         VK_EXTERNAL_MEMORY_HANDLE_TYPE_MTLHEAP_BIT_EXT;
+      VkMemoryMetalHandlePropertiesEXT mem_metal_props = {
+         .sType = VK_STRUCTURE_TYPE_MEMORY_METAL_HANDLE_PROPERTIES_EXT,
+         .pNext = NULL,
+         .memoryTypeBits = 0,
+      };
+      args->ret =
+         vk->GetMemoryMetalHandlePropertiesEXT(args->device, handle_type, res->u.metal_heap, &mem_metal_props);
+      if (args->ret != VK_SUCCESS)
+         return;
+      memoryTypeBits = mem_metal_props.memoryTypeBits;
+   } else {
       args->ret = VK_ERROR_INVALID_EXTERNAL_HANDLE;
       return;
    }
 
-   static const VkExternalMemoryHandleTypeFlagBits handle_type =
-      VK_EXTERNAL_MEMORY_HANDLE_TYPE_DMA_BUF_BIT_EXT;
-   VkMemoryFdPropertiesKHR mem_fd_props = {
-      .sType = VK_STRUCTURE_TYPE_MEMORY_FD_PROPERTIES_KHR,
-      .pNext = NULL,
-      .memoryTypeBits = 0,
-   };
-   vn_replace_vkGetMemoryResourcePropertiesMESA_args_handle(args);
-   args->ret =
-      vk->GetMemoryFdPropertiesKHR(args->device, handle_type, res->u.fd, &mem_fd_props);
-   if (args->ret != VK_SUCCESS)
-      return;
-
-   args->pMemoryResourceProperties->memoryTypeBits = mem_fd_props.memoryTypeBits;
+   args->pMemoryResourceProperties->memoryTypeBits = memoryTypeBits;
 
    VkMemoryResourceAllocationSizePropertiesMESA *alloc_size_props =
       vkr_find_struct(args->pMemoryResourceProperties->pNext,
@@ -527,6 +591,7 @@ vkr_device_memory_export_blob(struct vkr_device_memory *mem,
 
    const bool can_export_dma_buf = mem->valid_fd_types & (1 << VIRGL_RESOURCE_FD_DMABUF);
    const bool can_export_opaque = mem->valid_fd_types & (1 << VIRGL_RESOURCE_FD_OPAQUE);
+   const bool can_export_metal = mem->valid_fd_types & (1 << VIRGL_RESOURCE_METAL_HEAP);
    enum virgl_resource_fd_type fd_type;
    VkExternalMemoryHandleTypeFlagBits handle_type;
    struct virgl_resource_vulkan_info vulkan_info;
@@ -541,10 +606,16 @@ vkr_device_memory_export_blob(struct vkr_device_memory *mem,
       /* prefer dmabuf for easier mapping? */
       fd_type = VIRGL_RESOURCE_FD_DMABUF;
       handle_type = VK_EXTERNAL_MEMORY_HANDLE_TYPE_DMA_BUF_BIT_EXT;
-   } else if (can_export_opaque) {
+   } else if (can_export_opaque || can_export_metal) {
       /* prefer opaque for performance? */
-      fd_type = VIRGL_RESOURCE_FD_OPAQUE;
-      handle_type = VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_FD_BIT;
+      if (can_export_opaque) {
+         fd_type = VIRGL_RESOURCE_FD_OPAQUE;
+         handle_type = VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_FD_BIT;
+      } else {
+         assert(can_export_metal);
+         fd_type = VIRGL_RESOURCE_METAL_HEAP;
+         handle_type = VK_EXTERNAL_MEMORY_HANDLE_TYPE_MTLHEAP_BIT_EXT;
+      }
 
       STATIC_ASSERT(sizeof(vulkan_info.device_uuid) == VK_UUID_SIZE);
       STATIC_ASSERT(sizeof(vulkan_info.driver_uuid) == VK_UUID_SIZE);
@@ -562,6 +633,7 @@ vkr_device_memory_export_blob(struct vkr_device_memory *mem,
    }
 
    int fd;
+   MTLResource_id metal_heap;
    if (mem->udmabuf_fd >= 0) {
       fd = os_dupfd_cloexec(mem->udmabuf_fd);
       if (fd < 0) {
@@ -577,6 +649,19 @@ vkr_device_memory_export_blob(struct vkr_device_memory *mem,
          vkr_log("mem gbm bo export failed (ret %d)", fd);
          return false;
       }
+   } else if (can_export_metal) {
+      assert(handle_type == VK_EXTERNAL_MEMORY_HANDLE_TYPE_MTLHEAP_BIT_EXT);
+      struct vn_device_proc_table *vk = &mem->device->proc_table;
+      const VkMemoryGetMetalHandleInfoEXT metal_info = {
+         .sType = VK_STRUCTURE_TYPE_MEMORY_GET_METAL_HANDLE_INFO_EXT,
+         .memory = mem->base.handle.device_memory,
+         .handleType = handle_type,
+      };
+      VkResult ret = vk->GetMemoryMetalHandleEXT(mem->device->base.handle.device, &metal_info, &metal_heap);
+      if (ret != VK_SUCCESS) {
+         vkr_log("metal export failed (vk ret %d)", ret);
+         return false;
+      }
    } else {
       struct vn_device_proc_table *vk = &mem->device->proc_table;
       const VkMemoryGetFdInfoKHR fd_info = {
@@ -605,10 +690,15 @@ vkr_device_memory_export_blob(struct vkr_device_memory *mem,
 
    *out_blob = (struct virgl_context_blob){
       .type = fd_type,
-      .u.fd = fd,
       .map_info = map_info,
       .vulkan_info = vulkan_info,
    };
 
+   if (fd_type == VIRGL_RESOURCE_METAL_HEAP) {
+      out_blob->u.metal_heap = metal_heap;
+   } else {
+      out_blob->u.fd = fd;
+   }
+
    return true;
 }
diff --git a/src/venus/vkr_image.c b/src/venus/vkr_image.c
index 2b627e3..770440a 100644
--- a/src/venus/vkr_image.c
+++ b/src/venus/vkr_image.c
@@ -8,10 +8,82 @@
 #include "vkr_image_gen.h"
 #include "vkr_physical_device.h"
 
+static void
+vkr_image_fix_create_info(struct vkr_device *dev,
+                          VkImageCreateInfo *pCreateInfo)
+{
+   VkExternalMemoryImageCreateInfo *ext_create_info;
+
+   ext_create_info = vkr_find_struct(
+            pCreateInfo, VK_STRUCTURE_TYPE_EXTERNAL_MEMORY_IMAGE_CREATE_INFO);
+   if (ext_create_info) {
+      /* strip out dmabuf */
+      if ((ext_create_info->handleTypes & VK_EXTERNAL_MEMORY_HANDLE_TYPE_DMA_BUF_BIT_EXT) != 0) {
+         ext_create_info->handleTypes &= ~VK_EXTERNAL_MEMORY_HANDLE_TYPE_DMA_BUF_BIT_EXT;
+         /* add in supported handles */
+         if (dev->physical_device->is_metal_export_supported) {
+            ext_create_info->handleTypes |= VK_EXTERNAL_MEMORY_HANDLE_TYPE_MTLTEXTURE_BIT_EXT;
+         }
+      }
+   }
+}
+
+static VkResult
+vkr_image_fix_drm_format(struct vkr_device *dev,
+                         VkImageCreateInfo *pCreateInfo)
+{
+   const VkImageDrmFormatModifierExplicitCreateInfoEXT* drm_format_info =
+            vkr_find_struct(pCreateInfo, VK_STRUCTURE_TYPE_IMAGE_DRM_FORMAT_MODIFIER_EXPLICIT_CREATE_INFO_EXT);
+   const VkImageDrmFormatModifierListCreateInfoEXT* drm_format_list =
+            vkr_find_struct(pCreateInfo, VK_STRUCTURE_TYPE_IMAGE_DRM_FORMAT_MODIFIER_LIST_CREATE_INFO_EXT);
+
+   if (pCreateInfo->tiling != VK_IMAGE_TILING_DRM_FORMAT_MODIFIER_EXT || (!drm_format_info && !drm_format_list)) {
+      return VK_SUCCESS;
+   }
+
+   if (drm_format_info && drm_format_info->drmFormatModifier == DRM_FORMAT_MOD_LINEAR) {
+      pCreateInfo->tiling = VK_IMAGE_TILING_LINEAR;
+      return VK_SUCCESS;
+   }
+
+   for (int i = 0; drm_format_list && i < drm_format_list->drmFormatModifierCount; i++) {
+      if (drm_format_list->pDrmFormatModifiers[i] == DRM_FORMAT_MOD_LINEAR) {
+         pCreateInfo->tiling = VK_IMAGE_TILING_LINEAR;
+         return VK_SUCCESS;
+      }
+   }
+
+   vkr_log("only DRM_FORMAT_MOD_LINEAR is supported");
+   return VK_ERROR_FORMAT_NOT_SUPPORTED;
+}
+
+static VkResult
+vkr_image_emulate_drm_format_modifier_properties(UNUSED struct vkr_device *dev,
+                                                 UNUSED VkImage image,
+                                                 VkImageDrmFormatModifierPropertiesEXT* pProperties)
+{
+   pProperties->drmFormatModifier = DRM_FORMAT_MOD_LINEAR;
+   return VK_SUCCESS;
+}
+
 static void
 vkr_dispatch_vkCreateImage(struct vn_dispatch_context *dispatch,
                            struct vn_command_vkCreateImage *args)
 {
+   struct vkr_device *dev = vkr_device_from_handle(args->device);
+
+   /* if host does not natively support dmabuf we need to patch create info */
+   if (dev->physical_device->is_dma_buf_emulated) {
+      vkr_image_fix_create_info(dev, (VkImageCreateInfo *)args->pCreateInfo);
+   }
+
+   if (!dev->physical_device->EXT_image_drm_format_modifier) {
+      args->ret = vkr_image_fix_drm_format(dev, (VkImageCreateInfo *)args->pCreateInfo);
+      if (args->ret != VK_SUCCESS) {
+         return;
+      }
+   }
+
    /* XXX If VkExternalMemoryImageCreateInfo is chained by the app, all is
     * good.  If it is not chained, we might still bind an external memory to
     * the image, because vkr_dispatch_vkAllocateMemory makes any HOST_VISIBLE
@@ -150,6 +222,11 @@ vkr_dispatch_vkGetDeviceImageSubresourceLayout(
    struct vkr_device *dev = vkr_device_from_handle(args->device);
    struct vn_device_proc_table *vk = &dev->proc_table;
 
+   /* if host does not natively support dmabuf we need to patch create info */
+   if (dev->physical_device->is_dma_buf_emulated) {
+      vkr_image_fix_create_info(dev, (VkImageCreateInfo *)args->pInfo->pCreateInfo);
+   }
+
    vn_replace_vkGetDeviceImageSubresourceLayout_args_handle(args);
    vk->GetDeviceImageSubresourceLayout(args->device, args->pInfo, args->pLayout);
 }
@@ -163,8 +240,14 @@ vkr_dispatch_vkGetImageDrmFormatModifierPropertiesEXT(
    struct vn_device_proc_table *vk = &dev->proc_table;
 
    vn_replace_vkGetImageDrmFormatModifierPropertiesEXT_args_handle(args);
-   args->ret = vk->GetImageDrmFormatModifierPropertiesEXT(args->device, args->image,
-                                                          args->pProperties);
+
+   if (dev->physical_device->EXT_image_drm_format_modifier) {
+      args->ret = vk->GetImageDrmFormatModifierPropertiesEXT(args->device, args->image,
+                                                            args->pProperties);
+   } else {
+      args->ret = vkr_image_emulate_drm_format_modifier_properties(dev, args->image,
+                                                                   args->pProperties);
+   }
 }
 
 static void
@@ -219,6 +302,11 @@ vkr_dispatch_vkGetDeviceImageMemoryRequirements(
    struct vkr_device *dev = vkr_device_from_handle(args->device);
    struct vn_device_proc_table *vk = &dev->proc_table;
 
+   /* if host does not natively support dmabuf we need to patch create info */
+   if (dev->physical_device->is_dma_buf_emulated) {
+      vkr_image_fix_create_info(dev, (VkImageCreateInfo *)args->pInfo->pCreateInfo);
+   }
+
    vn_replace_vkGetDeviceImageMemoryRequirements_args_handle(args);
    vk->GetDeviceImageMemoryRequirements(args->device, args->pInfo,
                                         args->pMemoryRequirements);
@@ -232,6 +320,11 @@ vkr_dispatch_vkGetDeviceImageSparseMemoryRequirements(
    struct vkr_device *dev = vkr_device_from_handle(args->device);
    struct vn_device_proc_table *vk = &dev->proc_table;
 
+   /* if host does not natively support dmabuf we need to patch create info */
+   if (dev->physical_device->is_dma_buf_emulated) {
+      vkr_image_fix_create_info(dev, (VkImageCreateInfo *)args->pInfo->pCreateInfo);
+   }
+
    vn_replace_vkGetDeviceImageSparseMemoryRequirements_args_handle(args);
    vk->GetDeviceImageSparseMemoryRequirements(args->device, args->pInfo,
                                               args->pSparseMemoryRequirementCount,
diff --git a/src/venus/vkr_instance.c b/src/venus/vkr_instance.c
index a5002a3..6052fea 100644
--- a/src/venus/vkr_instance.c
+++ b/src/venus/vkr_instance.c
@@ -177,6 +177,11 @@ vkr_dispatch_vkCreateInstance(struct vn_dispatch_context *dispatch,
       create_info->pNext = &messenger_create_info;
    }
 
+   if (vkr_library_has_portability_enumeration(vk->EnumerateInstanceExtensionProperties)) {
+      ext_names[ext_count++] = VK_KHR_PORTABILITY_ENUMERATION_EXTENSION_NAME;
+      create_info->flags = VK_INSTANCE_CREATE_ENUMERATE_PORTABILITY_BIT_KHR;
+   }
+
    assert(layer_count <= ARRAY_SIZE(layer_names));
    create_info->enabledLayerCount = layer_count;
    create_info->ppEnabledLayerNames = layer_names;
diff --git a/src/venus/vkr_library.c b/src/venus/vkr_library.c
index 5cd3996..ea5c2c5 100644
--- a/src/venus/vkr_library.c
+++ b/src/venus/vkr_library.c
@@ -35,15 +35,23 @@ vkr_library_preload_icd(void)
 
 #if defined(ENABLE_VULKAN_DLOAD)
 
+#ifdef __APPLE__
+#define LIBVULKAN1 "libvulkan.1.dylib"
+#define LIBVULKAN  "libvulkan.dylib"
+#else
+#define LIBVULKAN1 "libvulkan.so.1"
+#define LIBVULKAN  "libvulkan.so"
+#endif
+
 bool
 vkr_library_load(struct vulkan_library *lib)
 {
    if (lib->handle)
       return true;
 
-   lib->handle = dlopen("libvulkan.so.1", RTLD_NOW | RTLD_LOCAL);
+   lib->handle = dlopen(LIBVULKAN1, RTLD_NOW | RTLD_LOCAL);
    if (lib->handle == NULL)
-      lib->handle = dlopen("libvulkan.so", RTLD_NOW | RTLD_LOCAL);
+      lib->handle = dlopen(LIBVULKAN, RTLD_NOW | RTLD_LOCAL);
    if (lib->handle == NULL) {
       vkr_log("failed to open libvulkan: %s", dlerror());
       return false;
@@ -89,3 +97,27 @@ vkr_library_unload(struct vulkan_library *lib)
 }
 
 #endif /* ENABLE_VULKAN_DLOAD */
+
+bool
+vkr_library_has_portability_enumeration(PFN_vkEnumerateInstanceExtensionProperties enum_inst_ext_props)
+{
+   uint32_t propertyCount = 0;
+   VkExtensionProperties *properties;
+   VkResult ret;
+   bool has_portability_enumeration = false;
+
+   ret = enum_inst_ext_props(NULL, &propertyCount, NULL);
+   if (ret != VK_SUCCESS) {
+      return false;
+   }
+   properties = calloc(propertyCount, sizeof(*properties));
+   ret = enum_inst_ext_props(NULL, &propertyCount, properties);
+   for (int i = 0; ret == VK_SUCCESS && i < propertyCount; i++) {
+      if (!strcmp(properties[i].extensionName, VK_KHR_PORTABILITY_ENUMERATION_EXTENSION_NAME)) {
+         has_portability_enumeration = true;
+         break;
+      }
+   }
+   free(properties);
+   return has_portability_enumeration;
+}
diff --git a/src/venus/vkr_library.h b/src/venus/vkr_library.h
index 718f891..8d66f5d 100644
--- a/src/venus/vkr_library.h
+++ b/src/venus/vkr_library.h
@@ -44,4 +44,7 @@ vkr_library_unload(struct vulkan_library *lib)
 
 #endif /* ENABLE_VULKAN_DLOAD */
 
+bool
+vkr_library_has_portability_enumeration(PFN_vkEnumerateInstanceExtensionProperties enum_inst_ext_props);
+
 #endif /* VKR_LIBRARY_H */
diff --git a/src/venus/vkr_physical_device.c b/src/venus/vkr_physical_device.c
index e2c3baa..f4a7dc4 100644
--- a/src/venus/vkr_physical_device.c
+++ b/src/venus/vkr_physical_device.c
@@ -227,6 +227,16 @@ vkr_physical_device_init_memory_properties(struct vkr_physical_device *physical_
           VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_FD_BIT);
    }
 
+   if (physical_dev->EXT_external_memory_metal) {
+      info.handleType = VK_EXTERNAL_MEMORY_HANDLE_TYPE_MTLHEAP_BIT_EXT,
+      vk->GetPhysicalDeviceExternalBufferProperties(handle, &info, &props);
+      physical_dev->is_metal_export_supported =
+         (props.externalMemoryProperties.externalMemoryFeatures &
+          VK_EXTERNAL_MEMORY_FEATURE_EXPORTABLE_BIT) &&
+         (props.externalMemoryProperties.exportFromImportedHandleTypes &
+          VK_EXTERNAL_MEMORY_HANDLE_TYPE_MTLHEAP_BIT_EXT);
+   }
+
    /* fallback to gbm allocation with dma-buf import */
    if (!physical_dev->is_dma_buf_fd_export_supported &&
        !physical_dev->is_opaque_fd_export_supported &&
@@ -269,12 +279,25 @@ vkr_physical_device_init_extensions(struct vkr_physical_device *physical_dev)
    for (uint32_t i = 0; i < count; i++) {
       VkExtensionProperties *props = &exts[i];
 
-      if (!strcmp(props->extensionName, "VK_KHR_external_memory_fd"))
+      if (!strcmp(props->extensionName, VK_KHR_EXTERNAL_MEMORY_FD_EXTENSION_NAME)) {
          physical_dev->KHR_external_memory_fd = true;
-      else if (!strcmp(props->extensionName, "VK_EXT_external_memory_dma_buf"))
+      } else if (!strcmp(props->extensionName, VK_EXT_EXTERNAL_MEMORY_DMA_BUF_EXTENSION_NAME)) {
          physical_dev->EXT_external_memory_dma_buf = true;
-      else if (!strcmp(props->extensionName, "VK_KHR_external_fence_fd"))
+      } else if (!strcmp(props->extensionName, VK_KHR_EXTERNAL_FENCE_FD_EXTENSION_NAME)) {
          physical_dev->KHR_external_fence_fd = true;
+      } else if (!strcmp(props->extensionName, VK_EXT_IMAGE_DRM_FORMAT_MODIFIER_EXTENSION_NAME)) {
+         physical_dev->EXT_image_drm_format_modifier = true;
+      } else if (!strcmp(props->extensionName, VK_EXT_QUEUE_FAMILY_FOREIGN_EXTENSION_NAME)) {
+         physical_dev->EXT_queue_family_foreign = true;
+      } else if (!strcmp(props->extensionName, VK_EXT_EXTERNAL_MEMORY_METAL_EXTENSION_NAME)) {
+         physical_dev->EXT_external_memory_metal = true;
+         /* hide from guest */
+         continue;
+      } else if (!strcmp(props->extensionName, VK_KHR_PORTABILITY_SUBSET_EXTENSION_NAME)) {
+         physical_dev->KHR_portability_subset = true;
+         /* hide from guest */
+         continue;
+      }
 
       const uint32_t spec_ver = vkr_extension_get_spec_version(props->extensionName);
       if (spec_ver) {
@@ -284,6 +307,34 @@ vkr_physical_device_init_extensions(struct vkr_physical_device *physical_dev)
       }
    }
 
+   /* add any emulated properties to show to the guest */
+   VkExtensionProperties prop;
+   uint32_t emulated_count = 0;
+   physical_dev->is_dma_buf_emulated = !physical_dev->EXT_external_memory_dma_buf && physical_dev->EXT_external_memory_metal;
+   emulated_count += 2*physical_dev->is_dma_buf_emulated;
+   emulated_count += !physical_dev->EXT_image_drm_format_modifier;
+   emulated_count += !physical_dev->EXT_queue_family_foreign;
+   exts = realloc(exts, sizeof(*exts) * (advertised_count + emulated_count));
+   if (physical_dev->is_dma_buf_emulated) {
+      strcpy(prop.extensionName, VK_EXT_EXTERNAL_MEMORY_DMA_BUF_EXTENSION_NAME);
+      prop.specVersion = vkr_extension_get_spec_version(prop.extensionName);
+      exts[advertised_count++] = prop;
+      strcpy(prop.extensionName, VK_KHR_EXTERNAL_MEMORY_FD_EXTENSION_NAME);
+      prop.specVersion = vkr_extension_get_spec_version(prop.extensionName);
+      exts[advertised_count++] = prop;
+   }
+   if (!physical_dev->EXT_image_drm_format_modifier) {
+      strcpy(prop.extensionName, VK_EXT_IMAGE_DRM_FORMAT_MODIFIER_EXTENSION_NAME);
+      prop.specVersion = vkr_extension_get_spec_version(prop.extensionName);
+      exts[advertised_count++] = prop;
+   }
+   if (!physical_dev->EXT_queue_family_foreign) {
+      /* FIXME: we don't actually emulate this yet as MoltenVK ignores queue family transfers... */
+      strcpy(prop.extensionName, VK_EXT_QUEUE_FAMILY_FOREIGN_EXTENSION_NAME);
+      prop.specVersion = vkr_extension_get_spec_version(prop.extensionName);
+      exts[advertised_count++] = prop;
+   }
+
    if (physical_dev->KHR_external_fence_fd) {
       const VkPhysicalDeviceExternalFenceInfo fence_info = {
          .sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_EXTERNAL_FENCE_INFO,
@@ -343,6 +394,21 @@ vkr_physical_device_init_queue_family_properties(struct vkr_physical_device *phy
    physical_dev->queue_family_properties = props;
 }
 
+static void
+vkr_physical_device_emulate_drm_props(VkDrmFormatModifierPropertiesListEXT *drm_props_list)
+{
+    drm_props_list->drmFormatModifierCount = 1;
+    if (drm_props_list->pDrmFormatModifierProperties) {
+      drm_props_list->pDrmFormatModifierProperties[0] = (VkDrmFormatModifierPropertiesEXT){
+         .drmFormatModifier = DRM_FORMAT_MOD_LINEAR,
+         .drmFormatModifierPlaneCount = 1,
+         .drmFormatModifierTilingFeatures = VK_FORMAT_FEATURE_SAMPLED_IMAGE_BIT |
+                                            VK_FORMAT_FEATURE_SAMPLED_IMAGE_FILTER_LINEAR_BIT |
+                                            VK_FORMAT_FEATURE_COLOR_ATTACHMENT_BIT,
+      };
+    };
+}
+
 static void
 vkr_dispatch_vkEnumeratePhysicalDevices(struct vn_dispatch_context *dispatch,
                                         struct vn_command_vkEnumeratePhysicalDevices *args)
@@ -701,6 +767,15 @@ vkr_dispatch_vkGetPhysicalDeviceFormatProperties2(
    vn_replace_vkGetPhysicalDeviceFormatProperties2_args_handle(args);
    vk->GetPhysicalDeviceFormatProperties2(args->physicalDevice, args->format,
                                           args->pFormatProperties);
+
+   /* emulate support for drm format modifiers */
+   if (!physical_dev->EXT_image_drm_format_modifier) {
+      VkDrmFormatModifierPropertiesListEXT* drm_props_list =
+         vkr_find_struct(args->pFormatProperties, VK_STRUCTURE_TYPE_DRM_FORMAT_MODIFIER_PROPERTIES_LIST_EXT);
+      if (drm_props_list) {
+         vkr_physical_device_emulate_drm_props(drm_props_list);
+      }
+   }
 }
 
 static void
@@ -712,9 +787,71 @@ vkr_dispatch_vkGetPhysicalDeviceImageFormatProperties2(
       vkr_physical_device_from_handle(args->physicalDevice);
    struct vn_physical_device_proc_table *vk = &physical_dev->proc_table;
 
+   /* filter unsupported drm format modifiers */
+   if (!physical_dev->EXT_image_drm_format_modifier) {
+      VkPhysicalDeviceImageFormatInfo2 *pImageFormatInfo =
+         (VkPhysicalDeviceImageFormatInfo2 *)args->pImageFormatInfo;
+      VkBaseInStructure *prev_struct =
+         vkr_find_prev_struct(pImageFormatInfo, VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_IMAGE_DRM_FORMAT_MODIFIER_INFO_EXT);
+      if (prev_struct) {
+         const VkPhysicalDeviceImageDrmFormatModifierInfoEXT *drm_format_mod =
+            (const VkPhysicalDeviceImageDrmFormatModifierInfoEXT *)prev_struct->pNext;
+         if (drm_format_mod->drmFormatModifier == DRM_FORMAT_MOD_LINEAR) {
+            /* Remove the struct from the list */
+            prev_struct->pNext = drm_format_mod->pNext;
+            vkr_log("emulating DRM_FORMAT_MOD_LINEAR with VK_IMAGE_TILING_LINEAR");
+            pImageFormatInfo->tiling = VK_IMAGE_TILING_LINEAR;
+            pImageFormatInfo->usage &=
+               ~(VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT | VK_IMAGE_USAGE_INPUT_ATTACHMENT_BIT);
+         } else {
+            vkr_log("only DRM_FORMAT_MOD_LINEAR is supported");
+            args->ret = VK_ERROR_FORMAT_NOT_SUPPORTED;
+            return;
+         }
+      }
+   }
+
+   /* emulate handle for dmabuf */
+   if (physical_dev->is_dma_buf_emulated && physical_dev->is_metal_export_supported) {
+      VkPhysicalDeviceExternalImageFormatInfo *info =
+         vkr_find_struct(args->pImageFormatInfo, VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_EXTERNAL_IMAGE_FORMAT_INFO);
+      if (info && info->handleType & VK_EXTERNAL_MEMORY_HANDLE_TYPE_DMA_BUF_BIT_EXT) {
+         info->handleType &= ~VK_EXTERNAL_MEMORY_HANDLE_TYPE_DMA_BUF_BIT_EXT;
+         info->handleType |= VK_EXTERNAL_MEMORY_HANDLE_TYPE_MTLTEXTURE_BIT_EXT;
+      }
+      if (info && info->handleType & VK_EXTERNAL_MEMORY_HANDLE_TYPE_DMA_BUF_BIT_EXT) {
+         info->handleType &= ~VK_EXTERNAL_MEMORY_HANDLE_TYPE_DMA_BUF_BIT_EXT;
+         info->handleType |= VK_EXTERNAL_MEMORY_HANDLE_TYPE_MTLTEXTURE_BIT_EXT;
+      }
+   }
+
    vn_replace_vkGetPhysicalDeviceImageFormatProperties2_args_handle(args);
    args->ret = vk->GetPhysicalDeviceImageFormatProperties2(
       args->physicalDevice, args->pImageFormatInfo, args->pImageFormatProperties);
+
+   /* emulate handle for dmabuf */
+   if (physical_dev->is_dma_buf_emulated && physical_dev->is_metal_export_supported) {
+      VkExternalImageFormatProperties *img_props = vkr_find_struct(
+         args->pImageFormatProperties->pNext, VK_STRUCTURE_TYPE_EXTERNAL_IMAGE_FORMAT_PROPERTIES);
+      VkExternalMemoryProperties *props = &img_props->externalMemoryProperties;
+      if (img_props && (props->exportFromImportedHandleTypes & VK_EXTERNAL_MEMORY_HANDLE_TYPE_MTLTEXTURE_BIT_EXT)) {
+         props->exportFromImportedHandleTypes &= ~VK_EXTERNAL_MEMORY_HANDLE_TYPE_MTLTEXTURE_BIT_EXT;
+         props->exportFromImportedHandleTypes |= VK_EXTERNAL_MEMORY_HANDLE_TYPE_DMA_BUF_BIT_EXT;
+      }
+      if (img_props && (props->compatibleHandleTypes & VK_EXTERNAL_MEMORY_HANDLE_TYPE_MTLTEXTURE_BIT_EXT)) {
+         props->compatibleHandleTypes &= ~VK_EXTERNAL_MEMORY_HANDLE_TYPE_MTLTEXTURE_BIT_EXT;
+         props->compatibleHandleTypes |= VK_EXTERNAL_MEMORY_HANDLE_TYPE_DMA_BUF_BIT_EXT;
+      }
+   }
+
+   /* emulate support for drm format modifiers */
+   if (!physical_dev->EXT_image_drm_format_modifier) {
+      VkDrmFormatModifierPropertiesListEXT* drm_props_list =
+         vkr_find_struct(args->pImageFormatProperties, VK_STRUCTURE_TYPE_DRM_FORMAT_MODIFIER_PROPERTIES_LIST_EXT);
+      if (drm_props_list) {
+         vkr_physical_device_emulate_drm_props(drm_props_list);
+      }
+   }
 }
 
 static void
@@ -740,9 +877,35 @@ vkr_dispatch_vkGetPhysicalDeviceExternalBufferProperties(
       vkr_physical_device_from_handle(args->physicalDevice);
    struct vn_physical_device_proc_table *vk = &physical_dev->proc_table;
 
+   /* emulate handle for dmabuf */
+   if (physical_dev->is_dma_buf_emulated && physical_dev->is_metal_export_supported) {
+      VkPhysicalDeviceExternalBufferInfo *info = (VkPhysicalDeviceExternalBufferInfo *)&args->pExternalBufferInfo;
+      if (info->handleType & VK_EXTERNAL_MEMORY_HANDLE_TYPE_DMA_BUF_BIT_EXT) {
+         info->handleType &= ~VK_EXTERNAL_MEMORY_HANDLE_TYPE_DMA_BUF_BIT_EXT;
+         info->handleType |= VK_EXTERNAL_MEMORY_HANDLE_TYPE_MTLHEAP_BIT_EXT;
+      }
+      if (info->handleType & VK_EXTERNAL_MEMORY_HANDLE_TYPE_DMA_BUF_BIT_EXT) {
+         info->handleType &= ~VK_EXTERNAL_MEMORY_HANDLE_TYPE_DMA_BUF_BIT_EXT;
+         info->handleType |= VK_EXTERNAL_MEMORY_HANDLE_TYPE_MTLHEAP_BIT_EXT;
+      }
+   }
+
    vn_replace_vkGetPhysicalDeviceExternalBufferProperties_args_handle(args);
    vk->GetPhysicalDeviceExternalBufferProperties(
       args->physicalDevice, args->pExternalBufferInfo, args->pExternalBufferProperties);
+
+   /* emulate handle for dmabuf */
+   if (physical_dev->is_dma_buf_emulated && physical_dev->is_metal_export_supported) {
+      VkExternalMemoryProperties *props = &args->pExternalBufferProperties->externalMemoryProperties;
+      if (props->exportFromImportedHandleTypes & VK_EXTERNAL_MEMORY_HANDLE_TYPE_MTLHEAP_BIT_EXT) {
+         props->exportFromImportedHandleTypes &= ~VK_EXTERNAL_MEMORY_HANDLE_TYPE_MTLHEAP_BIT_EXT;
+         props->exportFromImportedHandleTypes |= VK_EXTERNAL_MEMORY_HANDLE_TYPE_DMA_BUF_BIT_EXT;
+      }
+      if (props->compatibleHandleTypes & VK_EXTERNAL_MEMORY_HANDLE_TYPE_MTLHEAP_BIT_EXT) {
+         props->compatibleHandleTypes &= ~VK_EXTERNAL_MEMORY_HANDLE_TYPE_MTLHEAP_BIT_EXT;
+         props->compatibleHandleTypes |= VK_EXTERNAL_MEMORY_HANDLE_TYPE_DMA_BUF_BIT_EXT;
+      }
+   }
 }
 
 static void
diff --git a/src/venus/vkr_physical_device.h b/src/venus/vkr_physical_device.h
index b0b6691..1b7d2ac 100644
--- a/src/venus/vkr_physical_device.h
+++ b/src/venus/vkr_physical_device.h
@@ -23,14 +23,19 @@ struct vkr_physical_device {
 
    bool KHR_external_memory_fd;
    bool EXT_external_memory_dma_buf;
+   bool EXT_external_memory_metal;
+   bool KHR_portability_subset;
+   bool EXT_image_drm_format_modifier;
+   bool EXT_queue_family_foreign;
 
    bool KHR_external_fence_fd;
-   bool KHR_external_semaphore_fd;
 
    VkPhysicalDeviceMemoryProperties memory_properties;
    VkPhysicalDeviceIDProperties id_properties;
    bool is_dma_buf_fd_export_supported;
    bool is_opaque_fd_export_supported;
+   bool is_metal_export_supported;
+   bool is_dma_buf_emulated;
    void *gbm_device;
    int udmabuf_dev_fd;
 
diff --git a/src/venus/vkr_queue.c b/src/venus/vkr_queue.c
index 61b94be..e098430 100644
--- a/src/venus/vkr_queue.c
+++ b/src/venus/vkr_queue.c
@@ -481,19 +481,23 @@ vkr_dispatch_vkResetFenceResourceMESA(struct vn_dispatch_context *dispatch,
 
    vn_replace_vkResetFenceResourceMESA_args_handle(args);
 
-   const VkFenceGetFdInfoKHR info = {
-      .sType = VK_STRUCTURE_TYPE_FENCE_GET_FD_INFO_KHR,
-      .fence = args->fence,
-      .handleType = VK_EXTERNAL_FENCE_HANDLE_TYPE_SYNC_FD_BIT,
-   };
-   VkResult result = vk->GetFenceFdKHR(args->device, &info, &fd);
-   if (result != VK_SUCCESS) {
-      vkr_context_set_fatal(ctx);
-      return;
-   }
+   if (dev->physical_device->KHR_external_fence_fd) {
+      const VkFenceGetFdInfoKHR info = {
+         .sType = VK_STRUCTURE_TYPE_FENCE_GET_FD_INFO_KHR,
+         .fence = args->fence,
+         .handleType = VK_EXTERNAL_FENCE_HANDLE_TYPE_SYNC_FD_BIT,
+      };
+      VkResult result = vk->GetFenceFdKHR(args->device, &info, &fd);
+      if (result != VK_SUCCESS) {
+         vkr_context_set_fatal(ctx);
+         return;
+      }
 
-   if (fd >= 0)
-      close(fd);
+      if (fd >= 0)
+         close(fd);
+   } else {
+      vk->ResetFences(args->device, 1, &args->fence);
+   }
 }
 
 static void
diff --git a/src/venus/vkr_renderer.c b/src/venus/vkr_renderer.c
index c43a96e..42f08df 100644
--- a/src/venus/vkr_renderer.c
+++ b/src/venus/vkr_renderer.c
@@ -69,7 +69,9 @@ vkr_renderer_init(uint32_t flags, const struct vkr_renderer_callbacks *cbs)
       return false;
 
    vkr_debug_init();
-   virgl_log_set_handler(cbs->debug_logger, NULL, NULL);
+
+   if (cbs->debug_logger)
+      virgl_log_set_handler(cbs->debug_logger, NULL, NULL);
 
    vkr_state.cbs = cbs;
    list_inithead(&vkr_state.contexts);
@@ -177,6 +179,7 @@ vkr_renderer_create_resource(uint32_t ctx_id,
                              uint32_t blob_flags,
                              enum virgl_resource_fd_type *out_fd_type,
                              int *out_res_fd,
+                             void **out_res_ptr,
                              uint32_t *out_map_info,
                              struct virgl_resource_vulkan_info *out_vulkan_info)
 {
@@ -194,13 +197,17 @@ vkr_renderer_create_resource(uint32_t ctx_id,
       return false;
 
    assert(blob.type == VIRGL_RESOURCE_FD_SHM || blob.type == VIRGL_RESOURCE_FD_DMABUF ||
-          blob.type == VIRGL_RESOURCE_FD_OPAQUE);
+          blob.type == VIRGL_RESOURCE_FD_OPAQUE || blob.type == VIRGL_RESOURCE_METAL_HEAP);
 
    *out_fd_type = blob.type;
-   *out_res_fd = blob.u.fd;
+   if (blob.type == VIRGL_RESOURCE_METAL_HEAP) {
+      *out_res_ptr = blob.u.metal_heap;
+   } else {
+      *out_res_fd = blob.u.fd;
+   }
    *out_map_info = blob.map_info;
 
-   if (blob.type == VIRGL_RESOURCE_FD_OPAQUE) {
+   if (blob.type == VIRGL_RESOURCE_FD_OPAQUE || blob.type == VIRGL_RESOURCE_METAL_HEAP) {
       assert(out_vulkan_info);
       *out_vulkan_info = blob.vulkan_info;
    }
diff --git a/src/venus/vkr_renderer.h b/src/venus/vkr_renderer.h
index be3ae04..1ec376b 100644
--- a/src/venus/vkr_renderer.h
+++ b/src/venus/vkr_renderer.h
@@ -61,6 +61,7 @@ vkr_renderer_create_resource(uint32_t ctx_id,
                              uint32_t blob_flags,
                              enum virgl_resource_fd_type *out_fd_type,
                              int *out_res_fd,
+                             void **out_res_ptr,
                              uint32_t *out_map_info,
                              struct virgl_resource_vulkan_info *out_vulkan_info);
 
diff --git a/src/venus/vkr_ring.c b/src/venus/vkr_ring.c
index 0e46df8..08d8ef4 100644
--- a/src/venus/vkr_ring.c
+++ b/src/venus/vkr_ring.c
@@ -204,7 +204,11 @@ vkr_ring_relax(uint32_t *iter)
       .tv_sec = us / 1000000,
       .tv_nsec = (us % 1000000) * 1000,
    };
+#ifdef __APPLE__
+   nanosleep(&ts, NULL);
+#else
    clock_nanosleep(CLOCK_MONOTONIC, 0, &ts, NULL);
+#endif
 }
 
 static bool
diff --git a/src/virgl_context.h b/src/virgl_context.h
index 25bc106..df137ec 100644
--- a/src/virgl_context.h
+++ b/src/virgl_context.h
@@ -44,6 +44,7 @@ struct virgl_context_blob {
       int fd;
       uint32_t opaque_handle;
       struct pipe_resource *pipe_resource;
+      void *metal_heap;
    } u;
 
    uint32_t map_info;
diff --git a/src/virgl_resource.c b/src/virgl_resource.c
index 0ee5468..e7fd3f3 100644
--- a/src/virgl_resource.c
+++ b/src/virgl_resource.c
@@ -36,6 +36,13 @@
 #include "virgl_util.h"
 #include "virgl_context.h"
 
+#ifdef __APPLE__
+#include <CoreFoundation/CoreFoundation.h>
+#else
+#define CFRetain(x) (x)
+#define CFRelease(x)
+#endif
+
 static struct util_hash_table *virgl_resource_table;
 static struct virgl_resource_pipe_callbacks pipe_callbacks;
 
@@ -46,7 +53,9 @@ virgl_resource_destroy_func(void *val)
 
    if (res->pipe_resource)
       pipe_callbacks.unref(res->pipe_resource, pipe_callbacks.data);
-   if ((res->fd_type != VIRGL_RESOURCE_FD_INVALID) &&
+   if (res->fd_type == VIRGL_RESOURCE_METAL_HEAP)
+      CFRelease(res->metal_heap);
+   else if ((res->fd_type != VIRGL_RESOURCE_FD_INVALID) &&
        (res->fd_type != VIRGL_RESOURCE_OPAQUE_HANDLE))
       close(res->fd);
 
@@ -206,6 +215,25 @@ virgl_resource_create_from_iov(uint32_t res_id,
    return res;
 }
 
+struct virgl_resource *
+virgl_resource_create_from_metal_heap(UNUSED struct virgl_context *ctx,
+                                      uint32_t res_id,
+                                      void *metal_heap,
+                                      const struct virgl_resource_vulkan_info *vulkan_info)
+{
+   struct virgl_resource *res;
+
+   res = virgl_resource_create(res_id);
+   if (!res)
+      return NULL;
+
+   res->fd_type = VIRGL_RESOURCE_METAL_HEAP;
+   res->metal_heap = (void *)CFRetain(metal_heap);
+   res->vulkan_info = *vulkan_info;
+
+   return res;
+}
+
 void
 virgl_resource_remove(uint32_t res_id)
 {
@@ -263,6 +291,8 @@ virgl_resource_export_fd(struct virgl_resource *res, int *fd)
          return VIRGL_RESOURCE_FD_INVALID;
 
       return ctx->export_opaque_handle(ctx, res, fd);
+   } else if (res->fd_type == VIRGL_RESOURCE_METAL_HEAP) {
+      return VIRGL_RESOURCE_FD_INVALID;
    } else if (res->fd_type != VIRGL_RESOURCE_FD_INVALID) {
       *fd = os_dupfd_cloexec(res->fd);
       return *fd >= 0 ? res->fd_type : VIRGL_RESOURCE_FD_INVALID;
diff --git a/src/virgl_resource.h b/src/virgl_resource.h
index 8eb770e..2e830c6 100644
--- a/src/virgl_resource.h
+++ b/src/virgl_resource.h
@@ -50,6 +50,12 @@ enum virgl_resource_fd_type {
     */
    VIRGL_RESOURCE_OPAQUE_HANDLE,
 
+   /**
+    * A MTLHeap resource represents an opaque buffer of memory that can
+    * be shared across CPU and GPU. It can be the backing storage for textures.
+    */
+   VIRGL_RESOURCE_METAL_HEAP,
+
    VIRGL_RESOURCE_FD_INVALID = -1,
 };
 
@@ -94,6 +100,9 @@ struct virgl_resource {
    uint32_t opaque_handle_context_id;
    uint32_t opaque_handle;
 
+   /* When fd_type == VIRGL_RESOURCE_METAL_HEAP */
+   void *metal_heap;
+
    const struct iovec *iov;
    int iov_count;
 
@@ -157,6 +166,12 @@ virgl_resource_create_from_iov(uint32_t res_id,
                                const struct iovec *iov,
                                int iov_count);
 
+struct virgl_resource *
+virgl_resource_create_from_metal_heap(struct virgl_context *ctx,
+                                      uint32_t res_id,
+                                      void *metal_heap,
+                                      const struct virgl_resource_vulkan_info *vulkan_info);
+
 void
 virgl_resource_remove(uint32_t res_id);
 
diff --git a/src/virgl_util.c b/src/virgl_util.c
index 8184650..8988bb7 100644
--- a/src/virgl_util.c
+++ b/src/virgl_util.c
@@ -257,10 +257,15 @@ void virgl_prefixed_logv(const char *domain,
                          va_list va)
 {
    char *prefixed_fmt = NULL;
+   char line_end = '\0';
 
    assert(strchr(domain,'%') == NULL);
 
-   if (asprintf(&prefixed_fmt, "%s: %s", domain, fmt) < 0)
+   /* add new line if needed */
+   if (fmt[strlen(fmt)-1] != '\n')
+      line_end = '\n';
+
+   if (asprintf(&prefixed_fmt, "%s: %s%c", domain, fmt, line_end) < 0)
       return;
 
    virgl_logv(log_level, prefixed_fmt, va);
diff --git a/src/virglrenderer.c b/src/virglrenderer.c
index 79717fc..05ab90d 100644
--- a/src/virglrenderer.c
+++ b/src/virglrenderer.c
@@ -46,6 +46,10 @@
 #include "vrend/vrend_renderer.h"
 #include "vrend/vrend_winsys.h"
 
+#ifdef ENABLE_METAL
+#include "vrend/vrend_metal.h"
+#endif
+
 #ifndef WIN32
 #include "util/libsync.h"
 #endif
@@ -178,11 +182,37 @@ void virgl_renderer_resource_unref(uint32_t res_handle)
 void virgl_renderer_fill_caps(uint32_t set, uint32_t version,
                               void *caps)
 {
+   if (getenv("VIRGL_DEBUG_CAPS")) {
+      fprintf(stderr, "DEBUG virgl_renderer_fill_caps: set=%u version=%u\n", set, version);
+      fflush(stderr);
+   }
+   
    switch (set) {
    case VIRTGPU_DRM_CAPSET_VIRGL:
    case VIRTGPU_DRM_CAPSET_VIRGL2:
-      if (state.vrend_initialized)
+      if (state.vrend_initialized) {
          vrend_renderer_fill_caps(set, version, (union virgl_caps *)caps);
+         if (getenv("VIRGL_DEBUG_CAPS")) {
+            union virgl_caps *vcaps = (union virgl_caps *)caps;
+            fprintf(stderr, "DEBUG virgl_renderer_fill_caps returning to QEMU:\n");
+            fprintf(stderr, "  max_version=%u\n", vcaps->max_version);
+            fprintf(stderr, "  v1.glsl_level=%u at address %p\n", vcaps->v1.glsl_level, (void*)&vcaps->v1.glsl_level);
+            fprintf(stderr, "  v2.v1.glsl_level=%u at address %p\n", vcaps->v2.v1.glsl_level, (void*)&vcaps->v2.v1.glsl_level);
+            fprintf(stderr, "  caps base address=%p\n", (void*)caps);
+            
+            /* Calculate actual offset */
+            size_t glsl_offset = (uint8_t*)&vcaps->v1.glsl_level - (uint8_t*)caps;
+            fprintf(stderr, "  glsl_level offset from caps base: %zu bytes\n", glsl_offset);
+            
+            /* Dump bytes at that offset */
+            uint8_t *bytes = (uint8_t *)caps;
+            fprintf(stderr, "  Bytes at glsl_level offset (%zu-%zu): ", glsl_offset, glsl_offset+7);
+            for (size_t i = glsl_offset; i < glsl_offset + 8 && i < 1024; i++) 
+               fprintf(stderr, "%02x ", bytes[i]);
+            fprintf(stderr, "\n");
+            fflush(stderr);
+         }
+      }
       break;
    case VIRTGPU_DRM_CAPSET_VENUS:
       if (state.proxy_initialized)
@@ -482,7 +512,8 @@ void virgl_renderer_ctx_detach_resource(int ctx_id, int res_handle)
 
 static int virgl_renderer_resource_get_info_common(int res_handle,
                                                    struct virgl_renderer_resource_info *info,
-                                                   UNUSED void **d3d_tex2d)
+                                                   enum virgl_renderer_native_handle_type *type,
+                                                   virgl_renderer_native_handle *handle)
 {
    int ret = 0;
 
@@ -504,8 +535,19 @@ static int virgl_renderer_resource_get_info_common(int res_handle,
                                     (struct vrend_renderer_resource_info *)info);
 
 #ifdef WIN32
-   if (d3d_tex2d)
-      ret = vrend_renderer_resource_d3d11_texture2d(res->pipe_resource, d3d_tex2d);
+   if (type && handle) {
+      *handle = vrend_renderer_resource_d3d11_texture2d(res->pipe_resource);
+      if (*handle) {
+         *type = VIRGL_NATIVE_HANDLE_D3D_TEX2D;
+      }
+   }
+#elif defined(ENABLE_METAL)
+   if (type && handle) {
+      *handle = vrend_renderer_resource_metal_texture(res->pipe_resource);
+      if (*handle) {
+         *type = VIRGL_NATIVE_HANDLE_METAL_TEXTURE;
+      }
+   }
 #endif
 
    return ret;
@@ -517,7 +559,7 @@ int virgl_renderer_resource_get_info(int res_handle,
    TRACE_FUNC();
    int ret;
 
-   if ((ret = virgl_renderer_resource_get_info_common(res_handle, info, NULL)) != 0)
+   if ((ret = virgl_renderer_resource_get_info_common(res_handle, info, NULL, NULL)) != 0)
        return ret;
 
    if (state.winsys_initialized) {
@@ -540,7 +582,8 @@ int virgl_renderer_resource_get_info_ext(int res_handle,
 
    if ((ret = virgl_renderer_resource_get_info_common(res_handle,
                                                       &info_ext->base,
-                                                      &info_ext->d3d_tex2d)) != 0)
+                                                      &info_ext->native_type,
+                                                      &info_ext->native_handle)) != 0)
       return ret;
 
    info_ext->version = VIRGL_RENDERER_RESOURCE_INFO_EXT_VERSION;
@@ -556,6 +599,25 @@ int virgl_renderer_resource_get_info_ext(int res_handle,
 
    return 0;
 }
+ 
+int virgl_renderer_borrow_texture_for_scanout(int res_handle,
+                                              struct virgl_renderer_resource_info_ext *info)
+{
+   TRACE_FUNC();
+   struct virgl_resource *res = virgl_resource_lookup(res_handle);
+ 
+   if (!res)
+      return EINVAL;
+   if (!info)
+      return EINVAL;
+
+   if (!res->pipe_resource)
+      return 0;
+
+   vrend_renderer_borrow_texture_for_scanout(res->pipe_resource);
+
+   return virgl_renderer_resource_get_info_ext(res_handle, info);
+}
 
 void virgl_renderer_get_cap_set(uint32_t cap_set, uint32_t *max_ver,
                                 uint32_t *max_size)
@@ -911,8 +973,8 @@ int virgl_renderer_init(void *cookie, int flags, struct virgl_renderer_callbacks
          renderer_flags |= VREND_USE_EXTERNAL_BLOB;
       if (flags & VIRGL_RENDERER_USE_VIDEO)
          renderer_flags |= VREND_USE_VIDEO;
-      if (flags & VIRGL_RENDERER_D3D11_SHARE_TEXTURE)
-         renderer_flags |= VREND_D3D11_SHARE_TEXTURE;
+      if (flags & VIRGL_RENDERER_NATIVE_SHARE_TEXTURE)
+         renderer_flags |= VREND_NATIVE_SHARE_TEXTURE;
       if (flags & VIRGL_RENDERER_COMPAT_PROFILE)
          renderer_flags |= VREND_USE_COMPAT_CONTEXT;
       if (flags & VIRGL_RENDERER_USE_GLES)
@@ -1203,6 +1265,10 @@ int virgl_renderer_resource_create_blob(const struct virgl_renderer_resource_cre
       res = virgl_resource_create_from_opaque_handle(ctx, args->res_handle, blob.u.opaque_handle);
       if (!res)
          return -ENOMEM;
+   } else if (blob.type == VIRGL_RESOURCE_METAL_HEAP) {
+      res = virgl_resource_create_from_metal_heap(ctx, args->res_handle, blob.u.metal_heap, &blob.vulkan_info);
+      if (!res)
+         return -ENOMEM;
    } else if (blob.type != VIRGL_RESOURCE_FD_INVALID) {
       res = virgl_resource_create_from_fd(args->res_handle,
                                           blob.type,
@@ -1267,6 +1333,7 @@ int virgl_renderer_resource_map(uint32_t res_handle, void **out_map, uint64_t *o
          map_size = res->map_size;
          break;
       case VIRGL_RESOURCE_FD_OPAQUE:
+      case VIRGL_RESOURCE_METAL_HEAP:
          ret = vkr_allocator_resource_map(res, &map, &map_size);
          break;
       case VIRGL_RESOURCE_OPAQUE_HANDLE:
@@ -1327,6 +1394,7 @@ int virgl_renderer_resource_map_fixed(uint32_t res_handle, void *addr)
                                  MAP_FIXED | MAP_SHARED);
          break;
       case VIRGL_RESOURCE_FD_OPAQUE:
+      case VIRGL_RESOURCE_METAL_HEAP:
       case VIRGL_RESOURCE_FD_INVALID:
          /* Avoid a default case so that -Wswitch will tell us at compile time
           * if a new virgl resource type is added without being handled here.
@@ -1365,6 +1433,7 @@ int virgl_renderer_resource_unmap(uint32_t res_handle)
          ret = munmap(res->mapped, res->map_size);
          break;
       case VIRGL_RESOURCE_FD_OPAQUE:
+      case VIRGL_RESOURCE_METAL_HEAP:
          ret = vkr_allocator_resource_unmap(res);
          break;
       case VIRGL_RESOURCE_FD_INVALID:
@@ -1415,6 +1484,7 @@ virgl_renderer_resource_export_blob(uint32_t res_id, uint32_t *fd_type, int *fd)
       *fd_type = VIRGL_RENDERER_BLOB_FD_TYPE_SHM;
       break;
    case VIRGL_RESOURCE_OPAQUE_HANDLE:
+   case VIRGL_RESOURCE_METAL_HEAP:
    case VIRGL_RESOURCE_FD_INVALID:
       /* Avoid a default case so that -Wswitch will tell us at compile time if a
        * new virgl resource type is added without being handled here.
@@ -1482,6 +1552,60 @@ virgl_renderer_resource_import_blob(const struct virgl_renderer_resource_import_
    return 0;
 }
 
+enum virgl_renderer_native_handle_type
+virgl_renderer_create_handle_for_scanout(uint32_t res_id,
+                                         uint32_t width,
+                                         uint32_t height,
+                                         uint32_t virgl_format,
+                                         uint32_t padding,
+                                         uint32_t stride,
+                                         uint32_t offset,
+                                         virgl_renderer_native_handle *handle)
+{
+   TRACE_FUNC();
+#ifdef ENABLE_METAL
+   struct virgl_resource *res = virgl_resource_lookup(res_id);
+
+   if (!res)
+      return VIRGL_NATIVE_HANDLE_NONE;
+
+   if (res->fd_type != VIRGL_RESOURCE_METAL_HEAP)
+      return VIRGL_NATIVE_HANDLE_NONE;
+
+   struct vrend_metal_texture_description desc = {
+      .width = width,
+      .height = height,
+      .stride = stride,
+      .offset = offset,
+      .usage = PIPE_USAGE_IMMUTABLE,
+      .format = virgl_format,
+   };
+   MTLTexture_id tex;
+
+   if (!virgl_metal_create_texture_from_heap(res->metal_heap,
+                                             &desc,
+                                             &tex))
+      return VIRGL_NATIVE_HANDLE_NONE;
+
+   *handle = tex;
+   return VIRGL_NATIVE_HANDLE_METAL_TEXTURE;
+#else /* !ENABLE_METAL */
+   return VIRGL_NATIVE_HANDLE_NONE;
+#endif
+}
+
+void
+virgl_renderer_release_handle_for_scanout(enum virgl_renderer_native_handle_type type,
+                                          virgl_renderer_native_handle handle)
+{
+   TRACE_FUNC();
+#ifdef ENABLE_METAL
+   if (type == VIRGL_NATIVE_HANDLE_METAL_TEXTURE) {
+      virgl_metal_release_texture(handle);
+   }
+#endif
+}
+
 int
 virgl_renderer_export_fence(uint64_t client_fence_id, int *fd)
 {
diff --git a/src/virglrenderer.h b/src/virglrenderer.h
index 0bcafef..d52db74 100644
--- a/src/virglrenderer.h
+++ b/src/virglrenderer.h
@@ -163,7 +163,9 @@ struct virgl_renderer_callbacks {
 #endif /* VIRGL_RENDERER_UNSTABLE_APIS */
 
 
-#define VIRGL_RENDERER_D3D11_SHARE_TEXTURE (1 << 12)
+#define VIRGL_RENDERER_NATIVE_SHARE_TEXTURE (1 << 12)
+/* Compatibility with older versions */
+#define VIRGL_RENDERER_D3D11_SHARE_TEXTURE (VIRGL_RENDERER_NATIVE_SHARE_TEXTURE)
 #define VIRGL_RENDERER_COMPAT_PROFILE (1 << 13)
 
 /* Blob allocations must be done by guest from dedicated heap (Host visible memory). */
@@ -363,7 +365,23 @@ struct virgl_renderer_resource_info {
    int fd;
 };
 
-#define VIRGL_RENDERER_RESOURCE_INFO_EXT_VERSION 0
+#define VIRGL_RENDERER_RESOURCE_INFO_EXT_VERSION 1
+
+/**
+ * Describes a handle type used in the native graphics API
+ */
+enum virgl_renderer_native_handle_type {
+   VIRGL_NATIVE_HANDLE_NONE,
+   /* handle is a valid pointer to a ID3D11Texture2D */
+   VIRGL_NATIVE_HANDLE_D3D_TEX2D,
+   /* handle is a valid pointer to a MTLTexture */
+   VIRGL_NATIVE_HANDLE_METAL_TEXTURE,
+};
+
+/**
+ * The actual type is determined by `virgl_renderer_native_handle_type`
+ */
+typedef void *virgl_renderer_native_handle;
 
 struct virgl_renderer_resource_info_ext {
    int version;
@@ -371,7 +389,12 @@ struct virgl_renderer_resource_info_ext {
    bool has_dmabuf_export;
    int planes;
    uint64_t modifiers;
-   void *d3d_tex2d;
+   union {
+      /* this is for backwards compatibility */
+      void *d3d_tex2d;
+      virgl_renderer_native_handle native_handle;
+   };
+   enum virgl_renderer_native_handle_type native_type;
 };
 
 VIRGL_EXPORT int virgl_renderer_resource_get_info(int res_handle,
@@ -380,6 +403,9 @@ VIRGL_EXPORT int virgl_renderer_resource_get_info(int res_handle,
 VIRGL_EXPORT int virgl_renderer_resource_get_info_ext(int res_handle,
                                                       struct virgl_renderer_resource_info_ext *info);
 
+VIRGL_EXPORT int virgl_renderer_borrow_texture_for_scanout(int res_handle,
+                                                           struct virgl_renderer_resource_info_ext *info);
+ 
 VIRGL_EXPORT void virgl_renderer_cleanup(void *cookie);
 
 /* reset the rendererer - destroy all contexts and resource */
@@ -510,6 +536,38 @@ virgl_renderer_submit_cmd2(void *buffer,
                            uint64_t *in_fence_ids,
                            uint32_t num_in_fences);
 
+/**
+ * Blob resources are untyped but we may wish to create a native texture handle
+ * for scanout. Not all blobs support exporting to a file-descriptor so this
+ * can be used even in cases where `virgl_renderer_resource_export_blob` is not
+ * supported.
+ * 
+ * The user should assume the returned handle is immutable.
+ * 
+ * If a handle cannot be created, `VIRGL_RESOURCE_NATIVE_TYPE_NONE` will be
+ * returned.
+ * 
+ * If the return value is not `VIRGL_RESOURCE_NATIVE_TYPE_NONE`, the user MUST
+ * call `virgl_renderer_release_handle_for_scanout` with the returned handle
+ * and type when they are done using it. Otherwise, memory will be leaked.
+ */
+VIRGL_EXPORT enum virgl_renderer_native_handle_type
+virgl_renderer_create_handle_for_scanout(uint32_t res_id,
+                                         uint32_t width,
+                                         uint32_t height,
+                                         uint32_t virgl_format,
+                                         uint32_t padding,
+                                         uint32_t stride,
+                                         uint32_t offset,
+                                         virgl_renderer_native_handle *handle);
+
+/**
+ * This frees a handle acquired from `virgl_renderer_create_handle_for_scanout`
+ */
+VIRGL_EXPORT void
+virgl_renderer_release_handle_for_scanout(enum virgl_renderer_native_handle_type type,
+                                          virgl_renderer_native_handle handle);
+
 /* vtest semi-private APIs: */
 VIRGL_EXPORT int virgl_renderer_attach_fence(int ctx_id, int fence_fd);
 VIRGL_EXPORT int virgl_renderer_get_fence_fd(uint64_t fence_id);
diff --git a/src/virgl_hw.h b/src/virgl_hw.h
index 078da7d..d51b16d 100644
--- a/src/virgl_hw.h
+++ b/src/virgl_hw.h
@@ -629,4 +629,5 @@
 #define VIRGL_CAP_V2_MIRROR_CLAMP         (1u << 17)
 #define VIRGL_CAP_V2_RESOURCE_LAYOUT      (1u << 18)
 
+#define VIRGL_CAP_V2_CORE_PROFILE_ONLY    (1u << 19)
 
 /* virgl bind flags - these are compatible with mesa 10.5 gallium.
diff --git a/src/vrend/vrend_blitter.h b/src/vrend/vrend_blitter.h
index 91e2435..7a03571 100644
--- a/src/vrend/vrend_blitter.h
+++ b/src/vrend/vrend_blitter.h
@@ -35,6 +35,12 @@
    "%s"                                         \
 
 #define FS_HEADER_GLES                             \
+   "#version 300 es\n"                          \
+   "// Blitter\n"                               \
+   "%s"                                         \
+   "precision mediump float;\n"                 \
+
+#define FS_HEADER_GLES_MS                       \
    "#version 310 es\n"                          \
    "// Blitter\n"                               \
    "%s"                                         \
@@ -52,6 +58,11 @@
    "// Blitter\n"                               \
 
 #define HEADER_GLES                             \
+   "#version 300 es\n"                          \
+   "// Blitter\n"                               \
+   "precision mediump float;\n"                 \
+
+#define HEADER_GLES_MS                          \
    "#version 310 es\n"                          \
    "// Blitter\n"                               \
    "precision mediump float;\n"                 \
@@ -145,7 +156,7 @@
    "}\n"
 
 #define FS_TEXFETCH_COL_MSAA_GL FS_HEADER_GL FS_TEXFETCH_COL_MSAA_BODY
-#define FS_TEXFETCH_COL_MSAA_GLES FS_HEADER_GLES FS_TEXFETCH_COL_MSAA_BODY
+#define FS_TEXFETCH_COL_MSAA_GLES FS_HEADER_GLES_MS FS_TEXFETCH_COL_MSAA_BODY
 #define FS_TEXFETCH_COL_MSAA_ARRAY_GLES FS_HEADER_GLES_MS_ARRAY FS_TEXFETCH_COL_MSAA_BODY
 
 #define FS_TEXFETCH_DS_BODY                             \
@@ -178,7 +189,7 @@ struct vrend_context;
 struct vrend_resource;
 struct vrend_blit_info;
 #define FS_TEXFETCH_DS_MSAA_GL HEADER_GL FS_TEXFETCH_DS_MSAA_BODY
-#define FS_TEXFETCH_DS_MSAA_GLES HEADER_GLES FS_TEXFETCH_DS_MSAA_BODY_GLES
+#define FS_TEXFETCH_DS_MSAA_GLES HEADER_GLES_MS FS_TEXFETCH_DS_MSAA_BODY_GLES
 #define FS_TEXFETCH_DS_MSAA_ARRAY_GLES HEADER_GLES_MS_ARRAY FS_TEXFETCH_DS_MSAA_BODY_GLES
 
 /* implement blitting using OpenGL. */
diff --git a/src/vrend/vrend_decode.c b/src/vrend/vrend_decode.c
index 7774a25..277cde9 100644
--- a/src/vrend/vrend_decode.c
+++ b/src/vrend/vrend_decode.c
@@ -888,6 +888,17 @@ static int vrend_decode_create_object(struct vrend_context *ctx, const uint32_t
       return EINVAL;
    }
 
+   /* Surface GL errors with object metadata to pinpoint bad creations. */
+   if (!vrend_check_no_error(ctx) && ret == 0) {
+      virgl_error("GL error during CREATE_OBJECT type=%s handle=0x%x len=%u\n",
+                  vrend_get_object_type_name(obj_type), handle, length);
+      /* Dump a small slice of the payload for quick diagnosis. */
+      for (uint32_t i = 0; i < length && i < 12; i++) {
+         virgl_error("  dword[%u]=0x%x\n", i, buf[i]);
+      }
+      ret = EINVAL;
+   }
+
    return ret;
 }
 
@@ -2095,9 +2106,22 @@ static int vrend_decode_ctx_submit_cmd(struct virgl_context *ctx,
 
       TRACE_SCOPE_SLOW(vrend_get_comand_name(cmd));
 
+      /* If video is disabled at runtime, drop video commands quietly to avoid
+       * noisy errors from guest probes (e.g., gst-plugin-scan).
+       */
+      if (!vrend_renderer_video_available() &&
+          cmd >= VIRGL_CCMD_CREATE_VIDEO_CODEC && cmd <= VIRGL_CCMD_END_FRAME) {
+         continue;
+      }
+
       ret = decode_table[cmd](gdctx->grctx, buf, len);
-      if (!vrend_check_no_error(gdctx->grctx) && !ret)
+      if (!vrend_check_no_error(gdctx->grctx) && !ret) {
+         /* Surface the offending command when a GL error is observed. */
+         virgl_error("GL error after %s (ctx %d cmd=0x%x len=%u offset=%u)\n",
+                     vrend_get_comand_name(cmd), gdctx->base.ctx_id,
+                     cmd, len, cur_offset);
          ret = EINVAL;
+      }
       if (ret) {
          virgl_error("context %d failed to dispatch %s: %d\n",
                gdctx->base.ctx_id, vrend_get_comand_name(cmd), ret);
diff --git a/src/vrend/vrend_formats.c b/src/vrend/vrend_formats.c
index 670b3a4..90df028 100644
--- a/src/vrend/vrend_formats.c
+++ b/src/vrend/vrend_formats.c
@@ -357,6 +357,19 @@ static struct vrend_format_table gl_bgra_formats[] = {
   { VIRGL_FORMAT_B8G8R8A8_SRGB, GL_SRGB8_ALPHA8, GL_BGRA, GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_32 },
 };
 
+#ifdef __APPLE__
+/*
+ * macOS with Metal backend has broken GL_BGRA format as a render target.
+ * Use GL_RGBA format (which works) with the same swizzle as GLES.
+ */
+static struct vrend_format_table macos_bgra_formats[] = {
+  { VIRGL_FORMAT_B8G8R8X8_UNORM, GL_RGBA8,        GL_BGRA,     GL_UNSIGNED_BYTE, RGB1_SWIZZLE, view_class_32 },
+  { VIRGL_FORMAT_B8G8R8A8_UNORM, GL_RGBA8,        GL_BGRA,     GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_32 },
+  { VIRGL_FORMAT_B8G8R8X8_SRGB,  GL_SRGB8_ALPHA8, GL_BGRA,     GL_UNSIGNED_BYTE, RGB1_SWIZZLE, view_class_32 },
+  { VIRGL_FORMAT_B8G8R8A8_SRGB,  GL_SRGB8_ALPHA8, GL_BGRA,     GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_32 }
+};
+#endif
+
 static struct vrend_format_table gles_bgra_formats[] = {
   { VIRGL_FORMAT_B8G8R8X8_UNORM, GL_RGBA8,        GL_RGBA,     GL_UNSIGNED_BYTE, RGB1_SWIZZLE, view_class_32 },
   { VIRGL_FORMAT_B8G8R8A8_UNORM, GL_RGBA8,        GL_RGBA,     GL_UNSIGNED_BYTE, NO_SWIZZLE, view_class_32 },
@@ -593,7 +606,12 @@ void vrend_build_format_list_gl(void)
    * transfer operations. So we only register support for it in GL.
    */
   add_formats(gl_base_rgba_formats);
+  /* Use GL_BGRA format directly for macOS Core Profile */
+#ifdef __APPLE__
+   add_formats(macos_bgra_formats);
+#else
   add_formats(gl_bgra_formats);
+#endif
   add_formats(gl_bit10_formats);
 }
 
@@ -716,48 +734,183 @@ unsigned vrend_renderer_query_multisample_caps(unsigned max_samples, struct virg
    assert(glGetError() == GL_NO_ERROR &&
           "Stale error state detected, please check for failures in initialization");
 
+   /* glTexStorage2DMultisample availability check with graceful downgrade:
+    * 
+    * glTexStorage2DMultisample requires:
+    *   - OpenGL 4.3+ or GL_ARB_texture_storage_multisample (desktop GL)
+    *   - OpenGL ES 3.1+ (mobile/ANGLE)
+    * 
+    * Fallback alternatives available on older versions:
+    *   - glTexImage2DMultisample: GL 3.2+ / ES 3.1+ (works on GL 4.1 Core)
+    *   - glRenderbufferStorageMultisample: GL 3.0+ / ES 3.0+ (works on ANGLE)
+    * 
+    * We'll use glTexStorage2DMultisample if available, otherwise fall back to
+    * glTexImage2DMultisample for proper MSAA capability testing. */
+   
+   const char *renderer = (const char *)glGetString(GL_RENDERER);
+   const char *version_str = (const char *)glGetString(GL_VERSION);
+   
+   /* Check multisample function availability with multiple fallback options */
+   bool has_tex_storage_ms = false;
+   bool has_tex_image_ms = false;
+   bool has_rbo_storage_ms = false;
+   
+   if (epoxy_is_desktop_gl()) {
+      /* Desktop OpenGL path */
+      if (epoxy_gl_version() >= 43) {
+         has_tex_storage_ms = true;
+      } else if (epoxy_has_gl_extension("GL_ARB_texture_storage_multisample")) {
+         has_tex_storage_ms = true;
+      }
+      /* glTexImage2DMultisample available since GL 3.2 */
+      if (epoxy_gl_version() >= 32) {
+         has_tex_image_ms = true;
+      }
+      /* glRenderbufferStorageMultisample available since GL 3.0 */
+      if (epoxy_gl_version() >= 30) {
+         has_rbo_storage_ms = true;
+      }
+   } else {
+      /* OpenGL ES path */
+      if (epoxy_gl_version() >= 31) {
+         has_tex_storage_ms = true;
+         has_tex_image_ms = true;
+      }
+      /* glRenderbufferStorageMultisample available since ES 3.0 (ANGLE/Metal) */
+      if (epoxy_gl_version() >= 30) {
+         has_rbo_storage_ms = true;
+      }
+   }
+   
+   /* If no multisample functions available at all, disable MSAA */
+   if (!has_tex_storage_ms && !has_tex_image_ms && !has_rbo_storage_ms) {
+      virgl_debug("[VREND FORMATS] No multisample functions available "
+                  "(GL version: %s, renderer: %s, is_desktop: %d). "
+                  "Disabling MSAA support.\n", 
+                  version_str ? version_str : "unknown",
+                  renderer ? renderer : "unknown",
+                  epoxy_is_desktop_gl());
+      memset(caps->sample_locations, 0, 8 * sizeof(uint32_t));
+      return 0;  /* Return 0 to indicate MSAA not supported */
+   }
+   
+   /* Log which multisample method we're using */
+   if (has_tex_storage_ms) {
+      virgl_debug("[VREND FORMATS] Testing MSAA with glTexStorage2DMultisample\n");
+   } else if (has_tex_image_ms) {
+      virgl_debug("[VREND FORMATS] Testing MSAA with glTexImage2DMultisample fallback\n");
+   } else if (has_rbo_storage_ms) {
+      virgl_debug("[VREND FORMATS] Testing MSAA with glRenderbufferStorageMultisample fallback "
+                      "(GL version: %s, renderer: %s)\n",
+                      version_str ? version_str : "unknown",
+                      renderer ? renderer : "unknown");
+   }
+
+   virgl_debug("[VREND FORMATS] Starting MSAA capability test with max_samples=%u\n", max_samples);
+   
    glGenFramebuffers( 1, &fbo );
    memset(caps->sample_locations, 0, 8 * sizeof(uint32_t));
 
    for (int i = 3; i >= 0; i--) {
-      if (test_num_samples[i] > max_samples)
+      if (test_num_samples[i] > max_samples) {
+         virgl_debug("[VREND FORMATS] Skipping %u samples (exceeds max %u)\n", 
+                     test_num_samples[i], max_samples);
          continue;
-      glGenTextures(1, &tex);
-      glBindTexture(GL_TEXTURE_2D_MULTISAMPLE, tex);
-      glTexStorage2DMultisample(GL_TEXTURE_2D_MULTISAMPLE, test_num_samples[i], GL_RGBA32F, 64, 64, GL_TRUE);
+      }
+      
+      virgl_debug("[VREND FORMATS] Testing %u samples...\n", test_num_samples[i]);
+      
+      /* Clear any stale errors before testing */
+      while (glGetError() != GL_NO_ERROR);
+      
+      if (has_tex_storage_ms || has_tex_image_ms) {
+         /* Texture-based MSAA testing - use GL_RGBA8 for better compatibility */
+         glGenTextures(1, &tex);
+         GLenum err1 = glGetError();
+         
+         glBindTexture(GL_TEXTURE_2D_MULTISAMPLE, tex);
+         GLenum err2 = glGetError();
+         
+         if (has_tex_storage_ms) {
+            glTexStorage2DMultisample(GL_TEXTURE_2D_MULTISAMPLE, test_num_samples[i], GL_RGBA8, 64, 64, GL_TRUE);
+         } else {
+            glTexImage2DMultisample(GL_TEXTURE_2D_MULTISAMPLE, test_num_samples[i], GL_RGBA8, 64, 64, GL_TRUE);
+         }
+         GLenum err3 = glGetError();
+         
+         if (err1 != GL_NO_ERROR || err2 != GL_NO_ERROR || err3 != GL_NO_ERROR) {
+            virgl_debug("[VREND FORMATS]   glGenTextures err=0x%x, glBindTexture err=0x%x, glTex*Multisample err=0x%x\n",
+                        err1, err2, err3);
+         }
+      } else {
+         /* Renderbuffer-based MSAA testing (fallback for ES 3.0 / ANGLE) */
+         GLuint rbo;
+         glGenRenderbuffers(1, &rbo);
+         glBindRenderbuffer(GL_RENDERBUFFER, rbo);
+         glRenderbufferStorageMultisample(GL_RENDERBUFFER, test_num_samples[i], GL_RGBA8, 64, 64);
+         tex = rbo;  /* Store RBO handle in tex variable for cleanup */
+      }
+      
       status = glGetError();
       if (status == GL_NO_ERROR) {
          glBindFramebuffer(GL_FRAMEBUFFER, fbo);
-         glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D_MULTISAMPLE, tex, 0);
+         
+         if (has_tex_storage_ms || has_tex_image_ms) {
+            glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D_MULTISAMPLE, tex, 0);
+         } else {
+            /* For renderbuffer fallback */
+            glFramebufferRenderbuffer(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_RENDERBUFFER, tex);
+         }
+         
          status = glCheckFramebufferStatus(GL_FRAMEBUFFER);
          if (status == GL_FRAMEBUFFER_COMPLETE) {
+            virgl_debug("[VREND FORMATS]  %u samples COMPLETE\n", test_num_samples[i]);
             if (max_samples_confirmed < test_num_samples[i])
                max_samples_confirmed = test_num_samples[i];
 
-            for (unsigned k = 0; k < test_num_samples[i]; ++k) {
-               float msp[2];
-               uint32_t compressed;
-               glGetMultisamplefv(GL_SAMPLE_POSITION, k, msp);
-               compressed = ((unsigned)(floor(msp[0] * 16.0f)) & 0xf) << 4;
-               compressed |= ((unsigned)(floor(msp[1] * 16.0f)) & 0xf);
-               caps->sample_locations[out_buf_offsets[i] + (k >> 2)] |= compressed  << (8 * (k & 3));
+            /* glGetMultisamplefv only available in desktop GL (since 3.2), not in GL ES */
+            if (epoxy_is_desktop_gl()) {
+               for (unsigned k = 0; k < test_num_samples[i]; ++k) {
+                  float msp[2];
+                  uint32_t compressed;
+                  glGetMultisamplefv(GL_SAMPLE_POSITION, k, msp);
+                  compressed = ((unsigned)(floor(msp[0] * 16.0f)) & 0xf) << 4;
+                  compressed |= ((unsigned)(floor(msp[1] * 16.0f)) & 0xf);
+                  caps->sample_locations[out_buf_offsets[i] + (k >> 2)] |= compressed  << (8 * (k & 3));
+               }
+            } else {
+               /* OpenGL ES: sample locations not available, leave them zero-initialized */
+               virgl_debug("[VREND FORMATS]   (OpenGL ES: sample locations not available)\n");
             }
             lowest_working_ms_count_idx = i;
          } else {
+            virgl_debug("[VREND FORMATS]  %u samples INCOMPLETE (status=0x%x)\n", 
+                    test_num_samples[i], status);
             /* If a framebuffer doesn't support low sample counts,
              * use the sample position from the last working larger count. */
             if (lowest_working_ms_count_idx > 0) {
                for (unsigned k = 0; k < test_num_samples[i]; ++k) {
                   caps->sample_locations[out_buf_offsets[i] + (k >> 2)] =
-                        caps->sample_locations[out_buf_offsets[lowest_working_ms_count_idx]  + (k >> 2)];
+                     caps->sample_locations[out_buf_offsets[lowest_working_ms_count_idx]  + (k >> 2)];
                }
             }
          }
          glBindFramebuffer(GL_FRAMEBUFFER, 0);
+      } else {
+         virgl_debug("[VREND FORMATS]  %u samples GL_ERROR=0x%x\n", test_num_samples[i], status);
+      }
+      
+      /* Cleanup - delete texture or renderbuffer */
+      if (has_tex_storage_ms || has_tex_image_ms) {
+         glDeleteTextures(1, &tex);
+      } else {
+         glDeleteRenderbuffers(1, &tex);
       }
-      glDeleteTextures(1, &tex);
    }
    glDeleteFramebuffers(1, &fbo);
+   
+   virgl_debug("[VREND FORMATS] MSAA test complete: returning max_samples_confirmed=%u\n", 
+               max_samples_confirmed);
    return max_samples_confirmed;
 }
 
diff --git a/src/vrend/vrend_metal.h b/src/vrend/vrend_metal.h
new file mode 100644
index 0000000..154eae5
--- /dev/null
+++ b/src/vrend/vrend_metal.h
@@ -0,0 +1,34 @@
+/*
+ * Copyright 2025 Turing Software, LLC
+ * SPDX-License-Identifier: MIT
+ */
+#ifndef VIRGL_METAL_H
+#define VIRGL_METAL_H
+
+#include "virglrenderer.h"
+
+typedef void *MTLDevice_id;
+typedef void *MTLTexture_id;
+typedef void *MTLHeap_id;
+
+struct vrend_metal_texture_description {
+   unsigned width;
+   unsigned height;
+   unsigned stride;
+   unsigned offset;
+   unsigned bind;
+   unsigned usage;
+   uint32_t format;
+};
+
+bool virgl_metal_create_texture(MTLDevice_id device,
+                                const struct vrend_metal_texture_description *desc,
+                                MTLTexture_id *tex);
+
+bool virgl_metal_create_texture_from_heap(MTLHeap_id heap,
+                                          const struct vrend_metal_texture_description *desc,
+                                          MTLTexture_id *tex);
+
+void virgl_metal_release_texture(MTLTexture_id tex);
+
+#endif
diff --git a/src/vrend/vrend_metal.m b/src/vrend/vrend_metal.m
new file mode 100644
index 0000000..547d088
--- /dev/null
+++ b/src/vrend/vrend_metal.m
@@ -0,0 +1,150 @@
+/*
+ * Copyright 2025 Turing Software, LLC
+ * SPDX-License-Identifier: MIT
+ */
+#ifdef HAVE_CONFIG_H
+#include "config.h"
+#endif
+#include "virglrenderer.h"
+#include "vrend_metal.h"
+#include "pipe/p_state.h"
+#include "util/u_math.h"
+#include <Metal/Metal.h>
+
+struct metal_format_conversion {
+   uint32_t virgl_format;
+   MTLPixelFormat metal_format;
+};
+
+static bool virgl_format_to_metal_format(uint32_t format, MTLPixelFormat *metal_format)
+{
+   static const struct metal_format_conversion conversions[] = {
+      { VIRGL_FORMAT_R8G8B8A8_UNORM, MTLPixelFormatRGBA8Unorm },
+      { VIRGL_FORMAT_R8G8B8A8_SRGB, MTLPixelFormatRGBA8Unorm_sRGB },
+      { VIRGL_FORMAT_B8G8R8X8_UNORM, MTLPixelFormatBGRA8Unorm },
+      { VIRGL_FORMAT_B8G8R8A8_UNORM, MTLPixelFormatBGRA8Unorm },
+      { VIRGL_FORMAT_B8G8R8A8_SRGB, MTLPixelFormatBGRA8Unorm_sRGB },
+      { VIRGL_FORMAT_R16G16B16A16_FLOAT, MTLPixelFormatRGBA16Float },
+      { VIRGL_FORMAT_R32G32B32A32_FLOAT, MTLPixelFormatRGBA32Float },
+      { VIRGL_FORMAT_R10G10B10A2_UNORM, MTLPixelFormatRGB10A2Unorm },
+      { VIRGL_FORMAT_R8_UNORM, MTLPixelFormatR8Unorm },
+      { VIRGL_FORMAT_A8_UNORM, MTLPixelFormatR8Unorm },
+      { VIRGL_FORMAT_R16_UNORM, MTLPixelFormatR16Unorm },
+      { VIRGL_FORMAT_R8G8_UNORM, MTLPixelFormatRG8Unorm },
+      { VIRGL_FORMAT_R16G16_UNORM, MTLPixelFormatRG16Unorm },
+   };
+
+   for (uint32_t i = 0; i < ARRAY_SIZE(conversions); i++) {
+      if (conversions[i].virgl_format == format) {
+         *metal_format = conversions[i].metal_format;
+         return true;
+      }
+   }
+
+   return false;
+}
+
+static MTLTextureUsage virgl_bind_to_metal_usage_flags(uint32_t flags)
+{
+   MTLTextureUsage ret = MTLTextureUsageShaderRead | MTLTextureUsageShaderWrite;
+
+   if (flags & PIPE_BIND_RENDER_TARGET)
+      ret |= MTLTextureUsageRenderTarget;
+   if (flags & PIPE_BIND_DEPTH_STENCIL)
+      ret |= MTLTextureUsageRenderTarget;
+
+   return ret;
+}
+
+static MTLResourceOptions virgl_usage_to_metal_resource_options(uint32_t usage)
+{
+   switch (usage) {
+   case PIPE_USAGE_DEFAULT:
+   case PIPE_USAGE_STAGING:
+   default:
+      return MTLResourceStorageModeShared | MTLResourceCPUCacheModeDefaultCache;
+   case PIPE_USAGE_IMMUTABLE:
+      return MTLResourceStorageModeShared | MTLResourceCPUCacheModeWriteCombined | MTLHazardTrackingModeUntracked;
+   case PIPE_USAGE_DYNAMIC:
+   case PIPE_USAGE_STREAM:
+      return MTLResourceStorageModeShared | MTLResourceCPUCacheModeWriteCombined;
+   }
+}
+
+static MTLTextureDescriptor *new_descriptor(const struct vrend_metal_texture_description *desc)
+{
+   MTLPixelFormat pixel_format;
+
+   if (!virgl_format_to_metal_format(desc->format, &pixel_format)) {
+      return NULL;
+   }
+
+   MTLTextureDescriptor *descriptor = [MTLTextureDescriptor new];
+   descriptor.textureType = MTLTextureType2D;
+   descriptor.pixelFormat = pixel_format;
+   descriptor.width = desc->width;
+   descriptor.height = desc->height;
+   descriptor.resourceOptions = virgl_usage_to_metal_resource_options(desc->usage);
+   descriptor.usage = virgl_bind_to_metal_usage_flags(desc->bind);
+   if (desc->usage == PIPE_USAGE_IMMUTABLE) {
+      descriptor.usage &= ~MTLTextureUsageShaderWrite;
+   }
+
+   return descriptor;
+}
+
+bool virgl_metal_create_texture(MTLDevice_id device,
+                                const struct vrend_metal_texture_description *desc,
+                                MTLTexture_id *tex)
+{
+   id<MTLDevice> mtl_device = (id<MTLDevice>)device;
+   MTLTextureDescriptor *descriptor = new_descriptor(desc);
+   if (descriptor) {
+      *tex = [mtl_device newTextureWithDescriptor:descriptor];
+      [descriptor release];
+      return true;
+   }
+
+   return false;
+}
+
+bool virgl_metal_create_texture_from_heap(MTLHeap_id heap,
+                                          const struct vrend_metal_texture_description *desc,
+                                          MTLTexture_id *tex)
+{
+   id<MTLHeap> mtl_heap = (id<MTLHeap>)heap;
+   id<MTLDevice> mtl_device = mtl_heap.device;
+   MTLTextureDescriptor *descriptor = new_descriptor(desc);
+   *tex = nil;
+   if (descriptor) {
+      NSUInteger deviceAlignment, bytesPerRow;
+      /* swap B/R for existing texture */
+      if (desc->format == VIRGL_FORMAT_B8G8R8X8_UNORM || desc->format == VIRGL_FORMAT_B8G8R8A8_UNORM) {
+         descriptor.pixelFormat = MTLPixelFormatRGBA8Unorm;
+      }
+      /* Regardless of what we want, we have to respect the heap's options */
+      descriptor.resourceOptions = mtl_heap.resourceOptions;
+      deviceAlignment = [mtl_device minimumLinearTextureAlignmentForPixelFormat:descriptor.pixelFormat];
+      bytesPerRow = align(desc->stride, deviceAlignment);
+      id<MTLBuffer> mtl_buffer = [mtl_heap newBufferWithLength:bytesPerRow * desc->height
+                                                       options:mtl_heap.resourceOptions
+                                                        offset:0];
+      if (mtl_buffer) {
+         *tex = [mtl_buffer newTextureWithDescriptor:descriptor
+                                              offset:0
+                                         bytesPerRow:bytesPerRow];
+         [mtl_buffer release];
+      }
+      [descriptor release];
+      return !!*tex;
+   }
+
+   return false;
+}
+
+void virgl_metal_release_texture(MTLTexture_id tex)
+{
+   id<MTLTexture> mtl_texture = (id<MTLTexture>)tex;
+
+   [mtl_texture release];
+}
diff --git a/src/vrend/vrend_renderer.c b/src/vrend/vrend_renderer.c
index 4bf88e0..c063dbd 100644
--- a/src/vrend/vrend_renderer.c
+++ b/src/vrend/vrend_renderer.c
@@ -28,6 +28,7 @@
 #include <unistd.h>
 #include <stdatomic.h>
 #include <stdio.h>
+#include <string.h>
 #include <errno.h>
 #include "pipe/p_shader_tokens.h"
 
@@ -226,6 +227,7 @@ enum features_id
    feat_viewport_array,
    feat_implicit_msaa,
    feat_anisotropic_filter,
+   feat_draw_elements_base_vertex,
    feat_seamless_cubemap_per_texture,
    feat_vs_layer_viewport,
    feat_vs_viewport_index,
@@ -323,7 +325,7 @@ static const  struct {
    FEAT(texture_mirror_clamp_to_edge, UNAVAIL, UNAVAIL, "GL_ATI_texture_mirror_once", "GL_EXT_texture_mirror_clamp", "GL_ARB_texture_mirror_clamp_to_edge", "GL_EXT_texture_mirror_clamp_to_edge"),
    FEAT(texture_mirror_clamp, UNAVAIL, UNAVAIL, "GL_ATI_texture_mirror_once", "GL_EXT_texture_mirror_clamp"),
    FEAT(texture_mirror_clamp_to_border, UNAVAIL, UNAVAIL, "GL_EXT_texture_mirror_clamp"),
-   FEAT(texture_multisample, 32, 31,  "GL_ARB_texture_multisample" ),
+   FEAT(texture_multisample, 32, 30,  "GL_ARB_texture_multisample" ),
    FEAT(texture_query_lod, 40, UNAVAIL, "GL_ARB_texture_query_lod", "GL_EXT_texture_query_lod"),
    FEAT(texture_shadow_lod, UNAVAIL, UNAVAIL, "GL_EXT_texture_shadow_lod"),
    FEAT(texture_srgb_decode, UNAVAIL, UNAVAIL,  "GL_EXT_texture_sRGB_decode" ),
@@ -342,6 +344,7 @@ static const  struct {
    FEAT(seamless_cubemap_per_texture, UNAVAIL, UNAVAIL,  "GL_AMD_seamless_cubemap_per_texture" ),
    FEAT(vs_layer_viewport, UNAVAIL, UNAVAIL, "GL_AMD_vertex_shader_layer"),
    FEAT(vs_viewport_index, UNAVAIL, UNAVAIL, "GL_AMD_vertex_shader_viewport_index"),
+   FEAT(draw_elements_base_vertex, 32, 32, "GL_ARB_draw_elements_base_vertex", "GL_EXT_draw_elements_base_vertex", "GL_OES_draw_elements_base_vertex"),
 };
 
 struct global_renderer_state {
@@ -401,8 +404,11 @@ struct global_renderer_state {
 #ifdef HAVE_EPOXY_EGL_H
    bool use_egl_fence : 1;
 #endif
-   bool d3d_share_texture : 1;
+   bool native_share_texture : 1;
    bool gbm_layout_feat : 1;
+
+   /* host-side video acceleration availability */
+   bool video_available;
 };
 
 struct sysval_uniform_block {
@@ -416,6 +422,11 @@ struct sysval_uniform_block {
 
 static struct global_renderer_state vrend_state;
 
+bool vrend_renderer_video_available(void)
+{
+   return vrend_state.video_available;
+}
+
 static inline bool has_feature(enum features_id feature_id)
 {
    int slot = feature_id / 64;
@@ -1389,13 +1400,51 @@ static bool vrend_compile_shader(struct vrend_sub_context *sub_ctx,
 {
    GLint param;
    const char *shader_parts[SHADER_MAX_STRINGS];
-
-   for (int i = 0; i < shader->glsl_strings.num_strings; i++)
-      shader_parts[i] = shader->glsl_strings.strings[i].buf;
+   char *modified_shaders[SHADER_MAX_STRINGS] = {NULL};
+
+   /* Firefox uses GL_EXT_shader_texture_lod (GLES extension), but on OpenGL 4.1 Core Profile
+    * textureLod is built into the core spec - no extension directive needed.
+    * Remove the extension directive entirely. */
+   for (int i = 0; i < shader->glsl_strings.num_strings; i++) {
+      const char *src = shader->glsl_strings.strings[i].buf;
+      const char *ext_line = strstr(src, "#extension GL_EXT_shader_texture_lod");
+
+      if (ext_line) {
+         /* Find the end of the extension line (newline or end of string) */
+         const char *line_end = strchr(ext_line, '\n');
+         if (!line_end)
+            line_end = ext_line + strlen(ext_line);
+
+         /* Calculate new length: skip the extension line */
+         size_t prefix_len = ext_line - src;
+         size_t suffix_len = strlen(line_end);
+         size_t new_len = prefix_len + suffix_len;
+
+         modified_shaders[i] = malloc(new_len + 1);
+         if (modified_shaders[i]) {
+            /* Copy prefix (before extension line) */
+            memcpy(modified_shaders[i], src, prefix_len);
+            /* Copy suffix (after extension line) */
+            strcpy(modified_shaders[i] + prefix_len, line_end);
+            shader_parts[i] = modified_shaders[i];
+         } else {
+            shader_parts[i] = src;
+         }
+      } else {
+         shader_parts[i] = src;
+      }
+   }
 
    shader->id = glCreateShader(conv_shader_type(shader->sel->type));
    glShaderSource(shader->id, shader->glsl_strings.num_strings, shader_parts, NULL);
    glCompileShader(shader->id);
+
+   /* Free temporary modified shader strings after compilation */
+   for (int i = 0; i < shader->glsl_strings.num_strings; i++) {
+      if (modified_shaders[i])
+         free(modified_shaders[i]);
+   }
+
    glGetShaderiv(shader->id, GL_COMPILE_STATUS, &param);
    if (param == GL_FALSE) {
       char infolog[65536];
@@ -2519,30 +2568,32 @@ static GLuint convert_wrap(struct vrend_context *ctx, int wrap)
    case PIPE_TEX_WRAP_CLAMP: if (vrend_state.use_core_profile == false) return GL_CLAMP; else return GL_CLAMP_TO_EDGE;
 
    case PIPE_TEX_WRAP_CLAMP_TO_EDGE: return GL_CLAMP_TO_EDGE;
-   case PIPE_TEX_WRAP_CLAMP_TO_BORDER: return GL_CLAMP_TO_BORDER;
+   case PIPE_TEX_WRAP_CLAMP_TO_BORDER:
+      /* GLES (ANGLE/Metal) does not support clamp-to-border; fall back to edge
+       * to avoid GL_INVALID_ENUM on sampler parameter calls.
+       */
+      if (vrend_state.use_gles || !has_feature(feat_sampler_border_colors)) {
+         return GL_CLAMP_TO_EDGE;
+      }
+      return GL_CLAMP_TO_BORDER;
 
    case PIPE_TEX_WRAP_MIRROR_REPEAT: return GL_MIRRORED_REPEAT;
    case PIPE_TEX_WRAP_MIRROR_CLAMP:
+      /* Not available on GLES; fall back to mirrored repeat without error. */
       if (has_feature(feat_texture_mirror_clamp))
          return GL_MIRROR_CLAMP_EXT;
-      else {
-          vrend_report_context_error(ctx, VIRGL_ERROR_CTX_UNSUPPORTED_TEX_WRAP, wrap);
-          return GL_MIRRORED_REPEAT;
-      }
+      return GL_MIRRORED_REPEAT;
    case PIPE_TEX_WRAP_MIRROR_CLAMP_TO_EDGE:
+      /* ANGLE/Metal lacks this; fall back silently to clamp-to-edge. */
       if (has_feature(feat_texture_mirror_clamp_to_edge))
          return GL_MIRROR_CLAMP_TO_EDGE_EXT;
-      else {
-         vrend_report_context_error(ctx, VIRGL_ERROR_CTX_UNSUPPORTED_TEX_WRAP, wrap);
-         return GL_MIRRORED_REPEAT;
-      }
+      return GL_CLAMP_TO_EDGE;
    case PIPE_TEX_WRAP_MIRROR_CLAMP_TO_BORDER:
       if (has_feature(feat_texture_mirror_clamp_to_border)) {
          return GL_MIRROR_CLAMP_TO_BORDER_EXT;
-      } else {
-         vrend_report_context_error(ctx, VIRGL_ERROR_CTX_UNSUPPORTED_TEX_WRAP, wrap);
-         return GL_MIRRORED_REPEAT;
       }
+      /* No host support: clamp to edge to avoid GL errors. */
+      return GL_CLAMP_TO_EDGE;
    default:
       assert(0);
       return -1;
@@ -3094,6 +3145,11 @@ static void vrend_hw_emit_framebuffer_state(struct vrend_sub_context *sub_ctx)
 
    if (sub_ctx->nr_cbufs == 0) {
       glReadBuffer(GL_NONE);
+      /* In core profile, must explicitly disable draw buffers when no color attachments */
+      if (vrend_state.use_core_profile) {
+         GLenum none_buf = GL_NONE;
+         glDrawBuffers(1, &none_buf);
+      }
       if (has_feature(feat_srgb_write_control)) {
          glDisable(GL_FRAMEBUFFER_SRGB_EXT);
          sub_ctx->framebuffer_srgb_enabled = false;
@@ -3455,7 +3511,8 @@ void vrend_bind_vertex_elements_state(struct vrend_context *ctx,
       return;
    }
 
-   if (has_feature(feat_gles31_vertex_attrib_binding) && v->id == 0) {
+   if (has_feature(feat_gles31_vertex_attrib_binding)) {
+      if (v->id == 0) {
       glGenVertexArrays(1, &v->id);
       glBindVertexArray(v->id);
       for (uint32_t i = 0; i < v->count; i++) {
@@ -3473,6 +3530,17 @@ void vrend_bind_vertex_elements_state(struct vrend_context *ctx,
          glVertexAttribBinding(i, ve->base.vertex_buffer_index);
          glVertexBindingDivisor(i, ve->base.instance_divisor);
          glEnableVertexAttribArray(i);
+         }
+      }
+   } else {
+      for (uint32_t i = 0; i < v->count; i++) {
+         struct vrend_vertex_element *ve = &v->elements[i];
+
+         if (util_format_is_pure_integer(ve->base.src_format)) {
+            UPDATE_INT_SIGN_MASK(ve->base.src_format, i,
+                                 v->signed_int_bitmask,
+                                 v->unsigned_int_bitmask);
+         }
       }
    }
 }
@@ -3716,7 +3784,7 @@ void vrend_set_single_sampler_view(struct vrend_context *ctx,
                glTexParameteri(view->texture->target, GL_TEXTURE_BASE_LEVEL, view->u.tex.first_level);
                tex->cur_base = view->u.tex.first_level;
             }
-            if (tex->cur_max != view->u.tex.last_level) {
+            if (view->u.tex.last_level && tex->cur_max != view->u.tex.last_level) {
                glTexParameteri(view->texture->target, GL_TEXTURE_MAX_LEVEL, view->u.tex.last_level);
                tex->cur_max = view->u.tex.last_level;
             }
@@ -4871,6 +4939,10 @@ int vrend_clear_texture(struct vrend_context* ctx,
    format = tex_conv_table[fmt].glformat;
    type = tex_conv_table[fmt].gltype;
 
+   if (!has_feature(feat_clear_texture)) {
+      return EINVAL;
+   }
+
    /* 32-bit BGRA resources are always reordered to RGBA ordering before
     * submission to the host driver. Reorder red/blue color bytes in
     * the clear color to match. */
@@ -6138,14 +6210,18 @@ int vrend_draw_vbo(struct vrend_context *ctx,
             if (info->start_instance > 0)
                glDrawElementsInstancedBaseVertexBaseInstance(mode, info->count, elsz, (void *)(uintptr_t)sub_ctx->ib.offset,
                                                              info->instance_count, info->index_bias, info->start_instance);
-            else
+            else if (has_feature(feat_draw_elements_base_vertex))
                glDrawElementsInstancedBaseVertex(mode, info->count, elsz, (void *)(uintptr_t)sub_ctx->ib.offset, info->instance_count, info->index_bias);
+            else
+               return EINVAL;
 
-
-         } else if (info->min_index != 0 || info->max_index != (unsigned)-1)
-            glDrawRangeElementsBaseVertex(mode, info->min_index, info->max_index, info->count, elsz, (void *)(uintptr_t)sub_ctx->ib.offset, info->index_bias);
-         else
-            glDrawElementsBaseVertex(mode, info->count, elsz, (void *)(uintptr_t)sub_ctx->ib.offset, info->index_bias);
+         } else if (has_feature(feat_draw_elements_base_vertex)) {
+            if (info->min_index != 0 || info->max_index != (unsigned)-1)
+               glDrawRangeElementsBaseVertex(mode, info->min_index, info->max_index, info->count, elsz, (void *)(uintptr_t)sub_ctx->ib.offset, info->index_bias);
+            else
+               glDrawElementsBaseVertex(mode, info->count, elsz, (void *)(uintptr_t)sub_ctx->ib.offset, info->index_bias);
+         } else
+            return EINVAL;
       } else if (info->instance_count > 0) {
          if (info->start_instance > 0) {
             glDrawElementsInstancedBaseInstance(mode, info->count, elsz, (void *)(uintptr_t)sub_ctx->ib.offset, info->instance_count, info->start_instance);
@@ -7556,11 +7632,7 @@ static bool use_integer(void) {
       return true;
 
    const char * a = (const char *) glGetString(GL_VENDOR);
-   if (!a)
-       return false;
-   if (strcmp(a, "ARM") == 0)
-      return true;
-   return false;
+   return a && !(strcmp(a, "ARM") && strcmp(a, "Google Inc. (Apple)"));
 }
 
 static bool vrend_use_gbm_layout_feature(UNUSED uint32_t flags)
@@ -7630,6 +7702,30 @@ int vrend_renderer_init(const struct vrend_if_cbs *cbs, uint32_t flags)
    vrend_clicbs->make_current(gl_context);
    gl_ver = epoxy_gl_version();
 
+   /* Surface the full GL strings early for debugging/profile confirmation. */
+   const GLubyte *gl_ver_str = glGetString(GL_VERSION);
+   const GLubyte *gl_renderer_str = glGetString(GL_RENDERER);
+   const GLubyte *glsl_ver_str = glGetString(GL_SHADING_LANGUAGE_VERSION);
+
+   /* On macOS+Metal the GL_VERSION string can start with just the numeric
+    * version and "Metal"; reshape it to a clearer OpenGL 4.x label for logs.
+    */
+   char gl_ver_buf[128];
+   const char *gl_ver_display = gl_ver_str ? (const char *)gl_ver_str : "(null)";
+#ifdef __APPLE__
+   int gl_major_num = gl_ver / 10;
+   int gl_minor_num = gl_ver % 10;
+   if (gl_ver_str && strstr((const char *)gl_ver_str, "Metal")) {
+      snprintf(gl_ver_buf, sizeof(gl_ver_buf), "OpenGL %d.%d (Metal)", gl_major_num, gl_minor_num);
+      gl_ver_display = gl_ver_buf;
+   }
+#endif
+
+      virgl_info("GL strings: version='%s' renderer='%s' glsl='%s'\n",
+            gl_ver_display,
+            gl_renderer_str ? (const char *)gl_renderer_str : "(null)",
+            glsl_ver_str ? (const char *)glsl_ver_str : "(null)");
+
    /* enable error output as early as possible */
    if (vrend_debug(NULL, dbg_khr) && epoxy_has_gl_extension("GL_KHR_debug")) {
       glDebugMessageCallback(vrend_debug_cb, NULL);
@@ -7661,6 +7757,18 @@ int vrend_renderer_init(const struct vrend_if_cbs *cbs, uint32_t flags)
    init_features(gles ? 0 : gl_ver,
                  gles ? gl_ver : 0);
 
+#ifdef __APPLE__
+   /* macOS core GL 4.1 lacks GL_ARB_copy_image; force fallback paths. */
+   clear_feature(feat_copy_image);
+#endif
+
+   /* Disable host video decode/encode paths entirely in this build to avoid
+    * guest CREATE_VIDEO_BUFFER commands on configurations that cannot service
+    * them (e.g., macOS ANGLE/Metal). This also keeps caps consistent with
+    * the advertised zero video caps below.
+    */
+   vrend_state.video_available = false;
+
    if (!vrend_winsys_has_gl_colorspace())
       clear_feature(feat_srgb_write_control) ;
 
@@ -7764,7 +7872,7 @@ int vrend_renderer_init(const struct vrend_if_cbs *cbs, uint32_t flags)
    }
 #endif
 
-   vrend_state.d3d_share_texture = flags & VREND_D3D11_SHARE_TEXTURE;
+   vrend_state.native_share_texture = flags & VREND_NATIVE_SHARE_TEXTURE;
 
    vrend_state.gbm_layout_feat = vrend_use_gbm_layout_feature(flags);
 
@@ -8541,7 +8649,7 @@ static void vrend_resource_d3d_init(UNUSED struct vrend_resource *gr, UNUSED uin
    };
    ID3D11Texture2D* d3d_tex2d = NULL;
 
-   if (!vrend_state.d3d_share_texture)
+   if (!vrend_state.native_share_texture)
       return;
 
    if ((gr->base.bind & VIRGL_RES_BIND_SCANOUT) == 0)
@@ -8570,7 +8678,7 @@ static void vrend_resource_d3d_init(UNUSED struct vrend_resource *gr, UNUSED uin
 
    gr->d3d_tex2d = d3d_tex2d;
 
-   gr->storage_bits |= VREND_STORAGE_D3D_TEXTURE;
+   gr->storage_bits |= VREND_STORAGE_NATIVE_TEXTURE;
    gr->storage_bits |= VREND_STORAGE_EGL_IMAGE;
    return;
 
@@ -8581,6 +8689,44 @@ fail:
 #endif
 }
 
+/*
+ * When using ANGLE/Metal, this function creates a Metal Texture and
+ * EGL image given certain flags.
+ */
+static void vrend_resource_metal_init(UNUSED struct vrend_resource *gr, UNUSED uint32_t format)
+{
+#if defined(ENABLE_METAL) && defined(HAVE_EPOXY_EGL_H)
+   MTLTexture_id tex = NULL;
+
+   if (!vrend_state.native_share_texture)
+      return;
+
+   if ((gr->base.bind & VIRGL_RES_BIND_SCANOUT) == 0)
+      return;
+
+   if (gr->base.depth0 != 1 || gr->base.last_level != 0 || gr->base.nr_samples > 1)
+      return;
+
+   if (!virgl_egl_metal_create_texture(egl, &gr->base, format, &tex))
+      goto fail;
+
+   gr->egl_image = virgl_egl_metal_image_from_texture(egl, tex);
+   if (!gr->egl_image)
+      goto fail;
+
+   gr->metal_texture = tex;
+
+   gr->storage_bits |= VREND_STORAGE_NATIVE_TEXTURE;
+   gr->storage_bits |= VREND_STORAGE_EGL_IMAGE;
+   return;
+
+fail:
+   if (tex)
+      virgl_metal_release_texture(gr->metal_texture);
+   gr->metal_texture = NULL;
+#endif
+}
+
 /*
  * When GBM allocation is enabled, this function creates a GBM buffer and
  * EGL image given certain flags.
@@ -8670,6 +8816,7 @@ static int vrend_resource_alloc_texture(struct vrend_resource *gr,
 
    if (!image_oes) {
       vrend_resource_d3d_init(gr, format);
+      vrend_resource_metal_init(gr, format);
       vrend_resource_gbm_init(gr, format);
       if (gr->gbm_bo && !has_bit(gr->storage_bits, VREND_STORAGE_EGL_IMAGE))
          return 0;
@@ -8737,7 +8884,19 @@ static int vrend_resource_alloc_texture(struct vrend_resource *gr,
       }
 
       if (pr->nr_samples > 1) {
-         if (format_can_texture_storage) {
+         /* Metal backend (macOS): Silently downgrade MSAA to non-MSAA when not supported.
+          * gl=es mode (ANGLE) handles MSAA correctly, so only apply this workaround
+          * for desktop GL where Metal backend doesn't support multisampled textures. */
+         const char *renderer_str = (const char *)glGetString(GL_RENDERER);
+         bool is_metal_backend = (renderer_str && strstr(renderer_str, "Metal"));
+         
+         if (is_metal_backend && !vrend_state.use_gles && !has_feature(feat_storage_multisample)) {
+            /* Metal backend: No MSAA support, downgrade to regular texture */
+            virgl_debug("[VREND] Metal backend: MSAA texture requested (samples=%d, target=0x%x), downgrading to non-MSAA\n",
+                        pr->nr_samples, gr->target);
+            gr->target = (gr->target == GL_TEXTURE_2D_MULTISAMPLE) ? GL_TEXTURE_2D : GL_TEXTURE_2D_ARRAY;
+            pr->nr_samples = 0;
+         } else if (format_can_texture_storage) {
             if (gr->target == GL_TEXTURE_2D_MULTISAMPLE) {
                glTexStorage2DMultisample(gr->target, pr->nr_samples,
                                          internalformat, pr->width0, pr->height0,
@@ -8758,7 +8917,9 @@ static int vrend_resource_alloc_texture(struct vrend_resource *gr,
                                        GL_TRUE);
             }
          }
-      } else if (gr->target == GL_TEXTURE_CUBE_MAP) {
+      }
+      
+      if (pr->nr_samples <= 1 && gr->target == GL_TEXTURE_CUBE_MAP) {
             int i;
             if (format_can_texture_storage)
                glTexStorage2D(GL_TEXTURE_CUBE_MAP, pr->last_level + 1, internalformat, pr->width0, pr->height0);
@@ -8821,7 +8982,9 @@ static int vrend_resource_alloc_texture(struct vrend_resource *gr,
 
    if (!format_can_texture_storage) {
       glTexParameteri(gr->target, GL_TEXTURE_BASE_LEVEL, 0);
-      glTexParameteri(gr->target, GL_TEXTURE_MAX_LEVEL, pr->last_level);
+      if (pr->last_level) {
+         glTexParameteri(gr->target, GL_TEXTURE_MAX_LEVEL, pr->last_level);
+      }
    }
 
    glBindTexture(gr->target, 0);
@@ -8919,7 +9082,7 @@ void vrend_renderer_resource_destroy(struct vrend_resource *res)
       glDeleteMemoryObjectsEXT(1, &res->memobj);
    }
 
-#ifdef ENABLE_GBM
+#if defined(ENABLE_GBM) || defined(ENABLE_METAL)
    if (res->egl_image) {
       virgl_egl_image_destroy(egl, res->egl_image);
       for (unsigned i = 0; i < ARRAY_SIZE(res->aux_plane_egl_image); i++) {
@@ -8936,6 +9099,10 @@ void vrend_renderer_resource_destroy(struct vrend_resource *res)
 #ifdef WIN32
    if (res->d3d_tex2d)
       res->d3d_tex2d->lpVtbl->Release(res->d3d_tex2d);
+#endif
+#ifdef ENABLE_METAL
+   if (res->metal_texture)
+      virgl_metal_release_texture(res->metal_texture);
 #endif
    free(res);
 }
@@ -9975,6 +10142,12 @@ static int vrend_renderer_transfer_send_iov(struct vrend_context *ctx,
    return 0;
 }
 
+/* Forward declaration for MSAA staging path below. */
+static void vrend_renderer_blit_int(struct vrend_context *ctx,
+                                    struct vrend_resource *src_res,
+                                    struct vrend_resource *dst_res,
+                                    const struct pipe_blit_info *info);
+
 static int vrend_renderer_transfer_internal(struct vrend_context *ctx,
                                             struct vrend_resource *res,
                                             const struct vrend_transfer_info *info,
@@ -10105,6 +10278,122 @@ int vrend_transfer_inline_write(struct vrend_context *ctx,
    return vrend_renderer_transfer_write_iov(ctx, res, info->iovec, info->iovec_cnt, info);
 
 }
+static int vrend_renderer_copy_transfer3d_msaa(struct vrend_context *ctx,
+                                               struct vrend_resource *dst_res,
+                                               struct vrend_resource *src_res,
+                                               const struct vrend_transfer_info *info)
+{
+   /* Multisample textures reject TexSubImage uploads; stage into single-sample
+    * texture and resolve via blit to avoid GL_INVALID_OPERATION on macOS core.
+    */
+   if (dst_res->target != GL_TEXTURE_2D_MULTISAMPLE &&
+       dst_res->target != GL_TEXTURE_2D_MULTISAMPLE_ARRAY)
+      return EINVAL;
+
+   const GLenum staging_target = (dst_res->target == GL_TEXTURE_2D_MULTISAMPLE_ARRAY) ?
+                                 GL_TEXTURE_2D_ARRAY : GL_TEXTURE_2D;
+
+   virgl_warn("copy_transfer3d msaa staging: dst target=%u samples=%u level=%u box=[%d,%d,%d %dx%dx%d]\n",
+              dst_res->target, dst_res->base.nr_samples, info->level,
+              info->box->x, info->box->y, info->box->z,
+              info->box->width, info->box->height, info->box->depth);
+
+   struct vrend_resource staging = *dst_res;
+   staging.target = staging_target;
+   staging.base.nr_samples = 1;
+#ifdef PIPE_TEXTURE_2D_MULTISAMPLE_ARRAY
+   if (dst_res->base.target == PIPE_TEXTURE_2D_MULTISAMPLE_ARRAY)
+      staging.base.target = PIPE_TEXTURE_2D_ARRAY;
+   else
+#endif
+      staging.base.target = PIPE_TEXTURE_2D;
+   staging.storage_bits = VREND_STORAGE_GL_TEXTURE;
+   staging.gbm_bo = NULL;
+   staging.egl_image = 0;
+   staging.iov = NULL;
+   staging.num_iovs = 0;
+
+   glGenTextures(1, &staging.gl_id);
+   glBindTexture(staging_target, staging.gl_id);
+   glTexParameteri(staging_target, GL_TEXTURE_MIN_FILTER, GL_NEAREST);
+   glTexParameteri(staging_target, GL_TEXTURE_MAG_FILTER, GL_NEAREST);
+
+   const GLint internalformat = tex_conv_table[staging.base.format].internalformat;
+   const GLsizei level_w = u_minify(staging.base.width0, info->level);
+   const GLsizei level_h = u_minify(staging.base.height0, info->level);
+   const GLsizei level_d = (staging_target == GL_TEXTURE_2D_ARRAY) ? staging.base.array_size : 1;
+
+   if (util_format_is_compressed(staging.base.format)) {
+      const GLsizei comp_size = util_format_get_2d_size(staging.base.format,
+                                                        util_format_get_stride(staging.base.format, level_w),
+                                                        level_h);
+      if (staging_target == GL_TEXTURE_2D_ARRAY) {
+         glCompressedTexImage3D(staging_target, info->level, internalformat,
+                                level_w, level_h, level_d, 0,
+                                comp_size * level_d, NULL);
+      } else {
+         glCompressedTexImage2D(staging_target, info->level, internalformat,
+                                level_w, level_h, 0, comp_size, NULL);
+      }
+   } else {
+      if (staging_target == GL_TEXTURE_2D_ARRAY) {
+         glTexImage3D(staging_target, info->level, internalformat,
+                      level_w, level_h, level_d, 0,
+                      tex_conv_table[staging.base.format].glformat,
+                      tex_conv_table[staging.base.format].gltype,
+                      NULL);
+      } else {
+         glTexImage2D(staging_target, info->level, internalformat,
+                      level_w, level_h, 0,
+                      tex_conv_table[staging.base.format].glformat,
+                      tex_conv_table[staging.base.format].gltype,
+                      NULL);
+      }
+   }
+
+   int ret = vrend_renderer_transfer_write_iov(ctx, &staging, src_res->iov,
+                                               src_res->num_iovs, info);
+   if (ret) {
+      glDeleteTextures(1, &staging.gl_id);
+      return ret;
+   }
+
+   struct pipe_blit_info blit = { 0 };
+   blit.src.resource = &staging.base;
+   blit.src.level = info->level;
+   blit.src.box.x = info->box->x;
+   blit.src.box.y = info->box->y;
+   blit.src.box.z = info->box->z;
+   blit.src.box.width = info->box->width;
+   blit.src.box.height = info->box->height;
+   blit.src.box.depth = info->box->depth;
+   blit.src.format = staging.base.format;
+
+   blit.dst.resource = &dst_res->base;
+   blit.dst.level = info->level;
+   blit.dst.box = blit.src.box;
+   blit.dst.format = dst_res->base.format;
+
+   blit.mask = PIPE_MASK_RGBA;
+   if (vrend_format_is_ds(dst_res->base.format)) {
+      blit.mask = PIPE_MASK_Z;
+      if (util_format_has_stencil(util_format_description(dst_res->base.format)))
+         blit.mask |= PIPE_MASK_S;
+   }
+   blit.filter = PIPE_TEX_FILTER_NEAREST;
+   blit.scissor_enable = false;
+   blit.render_condition_enable = false;
+   blit.alpha_blend = false;
+
+   /* Force shader-based blit to MSAA target; GL forbids single->multi FBO blits. */
+   vrend_renderer_blit_int(ctx, &staging, dst_res, &blit);
+
+   virgl_warn("copy_transfer3d msaa staging: shader blit completed\n");
+
+   glDeleteTextures(1, &staging.gl_id);
+   return 0;
+}
+
 
 int vrend_renderer_copy_transfer3d(struct vrend_context *ctx,
                                    uint32_t dst_handle,
@@ -10113,6 +10402,16 @@ int vrend_renderer_copy_transfer3d(struct vrend_context *ctx,
                                    struct vrend_resource *src_res,
                                    const struct vrend_transfer_info *info)
 {
+   static int copy_log_budget = 8;
+   if (copy_log_budget > 0) {
+      virgl_warn("copy_transfer3d: dst target=%u base.target=%u samples=%u format=%s level=%u box=[%d,%d,%d %dx%dx%d]\n",
+                 dst_res->target, dst_res->base.target, dst_res->base.nr_samples,
+                 util_format_name(dst_res->base.format), info->level,
+                 info->box->x, info->box->y, info->box->z,
+                 info->box->width, info->box->height, info->box->depth);
+      copy_log_budget--;
+   }
+
    if (!resource_contains_box(dst_res, info->box, info->level)) {
       vrend_report_context_error(ctx, VIRGL_ERROR_CTX_ILLEGAL_CMD_BUFFER, dst_handle);
       return EINVAL;
@@ -10159,6 +10458,14 @@ int vrend_renderer_copy_transfer3d(struct vrend_context *ctx,
    }
 #endif
 
+  if (dst_res->target == GL_TEXTURE_2D_MULTISAMPLE ||
+      dst_res->target == GL_TEXTURE_2D_MULTISAMPLE_ARRAY) {
+     int ret = vrend_renderer_copy_transfer3d_msaa(ctx, dst_res, src_res, info);
+     if (!ret)
+        return 0;
+     virgl_warn("copy_transfer3d MSAA staging fallback failed (%d), trying direct upload\n", ret);
+  }
+
   return vrend_renderer_transfer_write_iov(ctx, dst_res, src_res->iov,
                                            src_res->num_iovs, info);
 }
@@ -10550,7 +10857,10 @@ static void vrend_resource_copy_fallback(struct vrend_resource *src_res,
    slice_offset = src_box->z * slice_size;
    cube_slice = (src_res->target == GL_TEXTURE_CUBE_MAP) ? src_box->z + src_box->depth : cube_slice;
    i = (src_res->target == GL_TEXTURE_CUBE_MAP) ? src_box->z : 0;
-   if (slice_offset + src_box->width * src_box->height + cube_slice * slice_size > total_size) {
+   /* Allow depth==0 (treated as 1 slice) and avoid width/height product overflow. */
+   uint32_t slices_to_copy = src_box->depth ? src_box->depth : 1;
+   uint64_t required_size = (uint64_t)slice_offset + (uint64_t)slices_to_copy * slice_size;
+   if (required_size > total_size) {
       virgl_error("Offset out of bound: %d\n", src_box->z);
       goto cleanup;
    }
@@ -10714,9 +11024,32 @@ void vrend_renderer_resource_copy_region(struct vrend_context *ctx,
    if (dst_res->egl_image)
       comp_flags ^= VREND_COPY_COMPAT_FLAG_ONE_IS_EGL_IMAGE;
 
-   if (has_feature(feat_copy_image) &&
-       format_is_copy_compatible(src_res->base.format,dst_res->base.format, comp_flags) &&
-       src_res->base.nr_samples == dst_res->base.nr_samples) {
+   bool allow_copy_image = has_feature(feat_copy_image) &&
+                           format_is_copy_compatible(src_res->base.format,
+                                                     dst_res->base.format,
+                                                     comp_flags) &&
+                           src_res->base.nr_samples == dst_res->base.nr_samples;
+
+   /* ANGLE/Metal on macOS returns GL_INVALID_ENUM for multisample copy_image
+    * targets when running in GLES mode. Prefer the blit fallback instead.
+    */
+   if (allow_copy_image &&
+       (src_res->target == GL_TEXTURE_2D_MULTISAMPLE ||
+        src_res->target == GL_TEXTURE_2D_MULTISAMPLE_ARRAY ||
+        dst_res->target == GL_TEXTURE_2D_MULTISAMPLE ||
+        dst_res->target == GL_TEXTURE_2D_MULTISAMPLE_ARRAY)) {
+      allow_copy_image = false;
+   }
+#ifdef __APPLE__
+   /* ANGLE GLES on macOS can still advertise copy_image but fail at runtime;
+    * force shader blit when running GLES on Apple to avoid GL_INVALID_ENUM.
+    */
+   if (allow_copy_image && vrend_state.use_gles) {
+      allow_copy_image = false;
+   }
+#endif
+
+   if (allow_copy_image) {
       VREND_DEBUG(dbg_copy_resource, ctx, "COPY_REGION: use glCopyImageSubData\n");
       vrend_copy_sub_image(src_res, dst_res, src_level, src_box,
                            dst_level, dstx, dsty, dstz);
@@ -11296,7 +11629,7 @@ void vrend_renderer_blit(struct vrend_context *ctx,
     * to resource_copy_region, in this case and if no render states etx need
     * to be applied, forward the call to glCopyImageSubData, otherwise do a
     * normal blit. */
-   if (has_feature(feat_copy_image) &&
+   bool allow_copy_image = has_feature(feat_copy_image) &&
        (!info->render_condition_enable || !ctx->sub->cond_render_gl_mode) &&
        format_is_copy_compatible(info->src.format,info->dst.format, comp_flags) &&
        eglimage_copy_compatible &&
@@ -11309,7 +11642,26 @@ void vrend_renderer_blit(struct vrend_context *ctx,
        info->dst.box.y + info->dst.box.height <= dst_height &&
        info->src.box.width == info->dst.box.width &&
        info->src.box.height == info->dst.box.height &&
-       info->src.box.depth == info->dst.box.depth) {
+       info->src.box.depth == info->dst.box.depth;
+
+   /* ANGLE/Metal GLES path returns GL_INVALID_ENUM for copy_image on MSAA
+    * targets; avoid copy_image there. Also prefer shader blit when running
+    * GLES on macOS even for non-MSAA to steer clear of driver quirks.
+    */
+   if (allow_copy_image &&
+       (src_res->target == GL_TEXTURE_2D_MULTISAMPLE ||
+        src_res->target == GL_TEXTURE_2D_MULTISAMPLE_ARRAY ||
+        dst_res->target == GL_TEXTURE_2D_MULTISAMPLE ||
+        dst_res->target == GL_TEXTURE_2D_MULTISAMPLE_ARRAY)) {
+      allow_copy_image = false;
+   }
+#ifdef __APPLE__
+   if (allow_copy_image && vrend_state.use_gles) {
+      allow_copy_image = false;
+   }
+#endif
+
+   if (allow_copy_image) {
       VREND_DEBUG(dbg_blit, ctx,  "  Use glCopyImageSubData\n");
       vrend_copy_sub_image(src_res, dst_res, info->src.level, &info->src.box,
                            info->dst.level, info->dst.box.x, info->dst.box.y,
@@ -12100,6 +12452,16 @@ static int get_glsl_version(void)
 static void vrend_fill_caps_glsl_version(int gl_ver, int gles_ver,
                                          union virgl_caps *caps)
 {
+#ifdef __APPLE__
+   /* macOS Metal reports GL 4.1 core but Mesa needs explicit GLSL 4.10.
+    * Force this for any desktop GL context on macOS to avoid fallback to GL 2.1.
+    */
+   if (gl_ver >= 30 && gles_ver == 0) {
+      caps->v1.glsl_level = 410;
+      return;
+   }
+#endif
+
    if (gles_ver > 0) {
       caps->v1.glsl_level = 120;
 
@@ -12166,6 +12528,10 @@ static void vrend_renderer_fill_caps_v1(int gl_ver, int gles_ver, union virgl_ca
 {
    int i;
    GLint max;
+   const char *gl_version_str = (const char *)glGetString(GL_VERSION);
+   const char *gl_renderer_str = (const char *)glGetString(GL_RENDERER);
+   const bool is_angle = ((gl_version_str && strstr(gl_version_str, "ANGLE")) ||
+                          (gl_renderer_str && strstr(gl_renderer_str, "ANGLE")));
 
    /*
     * We can't fully support this feature on GLES,
@@ -12210,9 +12576,27 @@ static void vrend_renderer_fill_caps_v1(int gl_ver, int gles_ver, union virgl_ca
 
    if (has_feature(feat_ubo)) {
       glGetIntegerv(GL_MAX_VERTEX_UNIFORM_BLOCKS, &max);
+      const char *version_str = (const char *)glGetString(GL_VERSION);
+      bool is_angle_local = (version_str && strstr(version_str, "ANGLE"));
       /* GL_MAX_VERTEX_UNIFORM_BLOCKS is omitting the ordinary uniform block, add it
-       * also reduce by 1 as we might generate a VirglBlock helper uniform block */
-      caps->v1.max_uniform_blocks = max + 1 - 1;
+       * also reduce by 1 as we might generate a VirglBlock helper uniform block.
+       * Mesa needs at least 12 per shader after its own adjustments, so report max+1.
+       * 
+       * Special handling for ANGLE/Metal: ANGLE clamps reported values to ES spec minimums
+       * (12 for UBOs) even though Metal backend supports 14. Detect ANGLE and report a
+       * higher value to ensure Mesa gets enough after its adjustments. */
+      if (is_angle_local && max <= 12) {
+         caps->v1.max_uniform_blocks = 14;  // ANGLE Metal internal limit
+         virgl_debug("[VREND CAPS] ANGLE backend with UBO limit %d (ES spec minimum), "
+                     "overriding to 14 for Mesa compatibility\n", max);
+      } else {
+         caps->v1.max_uniform_blocks = max + 1;
+         virgl_debug("[VREND CAPS] feat_ubo=YES, GL_MAX_VERTEX_UNIFORM_BLOCKS=%d, reporting max_uniform_blocks=%d\n", 
+                     max, caps->v1.max_uniform_blocks);
+      }
+   } else {
+      virgl_debug("[VREND CAPS] feat_ubo=NO (gl_ver=%d, gles_ver=%d, epoxy_gl_version=%d, epoxy_is_desktop_gl=%d)\n",
+                  gl_ver, gles_ver, epoxy_gl_version(), epoxy_is_desktop_gl());
    }
 
    if (has_feature(feat_depth_clamp))
@@ -12231,8 +12615,13 @@ static void vrend_renderer_fill_caps_v1(int gl_ver, int gles_ver, union virgl_ca
    if (has_feature(feat_seamless_cubemap_per_texture))
       caps->v1.bset.seamless_cube_map_per_texture = 1;
 
-   if (has_feature(feat_texture_multisample))
+   if (has_feature(feat_texture_multisample)) {
       caps->v1.bset.texture_multisample = 1;
+      virgl_debug("[VREND CAPS] feat_texture_multisample enabled, setting caps->v1.bset.texture_multisample=1\n");
+   } else {
+      virgl_debug("[VREND CAPS] feat_texture_multisample NOT enabled (gl_ver=%d, gles_ver=%d)\n",
+              vrend_state.use_gles ? 0 : gl_ver, vrend_state.use_gles ? gl_ver : 0);
+   }
 
    if (has_feature(feat_tessellation))
       caps->v1.bset.has_tessellation_shaders = 1;
@@ -12349,6 +12738,7 @@ static void vrend_renderer_fill_caps_v1(int gl_ver, int gles_ver, union virgl_ca
 
    glGetIntegerv(GL_MAX_SAMPLES, &max);
    caps->v1.max_samples = max;
+   virgl_debug("[VREND] GL_MAX_SAMPLES from glGetIntegerv: %d\n", max);
 
    /* All of the formats are common. */
    for (i = 0; i < VIRGL_FORMAT_MAX; i++) {
@@ -12376,6 +12766,7 @@ static void vrend_renderer_fill_caps_v2(int gl_ver, int gles_ver,  union virgl_c
    GLfloat range[2];
    uint32_t video_memory;
    const char *renderer = (const char *)glGetString(GL_RENDERER);
+   const bool angle_in_renderer = (renderer && strstr(renderer, "ANGLE"));
 
    /* Count this up when you add a feature flag that is used to set a CAP in
     * the guest that was set unconditionally before. Then check that flag and
@@ -12383,9 +12774,133 @@ static void vrend_renderer_fill_caps_v2(int gl_ver, int gles_ver,  union virgl_c
     * run on an old virgl host. Use it also to indicate non-cap fixes on the
     * host that help enable features in the guest. */
    caps->v2.host_feature_check_version = 23;
+   if (gles_ver > 0 && angle_in_renderer)
+      caps->v2.host_feature_check_version = 4;
 
-   /* Forward host GL_RENDERER to the guest. */
-   strncpy(caps->v2.renderer, renderer, sizeof(caps->v2.renderer) - 1);
+   /* Forward host GL_RENDERER to the guest.
+    *
+    * Firefox has an ANGLE-specific GL_RENDERER parser that triggers on the
+    * substring "ANGLE" and may fail hard if the string doesn't match its
+    * expected formats.
+    *
+    * Additionally, Firefox's WebGL renderer sanitizer recognizes a limited set
+    * of device names (e.g. strings starting with "Apple"). The plain "virgl"
+    * renderer name does not match those heuristics.
+    *
+    * For ANGLE-on-Metal, extract the Apple SoC / device name and forward that
+    * (without the "ANGLE" token) so the guest GL_RENDERER can be both parse- and
+    * sanitize-friendly.
+    */
+   if (renderer && strstr(renderer, "ANGLE Metal Renderer:")) {
+      const char *metal_start = strstr(renderer, "ANGLE Metal Renderer:");
+      const char *device_start = metal_start + strlen("ANGLE Metal Renderer:");
+      while (*device_start == ' ')
+         device_start++;
+
+      const char *device_end = device_start;
+      while (*device_end && *device_end != ',' && *device_end != ')')
+         device_end++;
+
+      while (device_end > device_start && device_end[-1] == ' ')
+         device_end--;
+
+      /* Forward the real Metal device name.
+       * If it isn't vendor-prefixed, try to prefix it with the ANGLE vendor
+       * field from the leading "ANGLE (vendor, ...)" without including the
+       * "ANGLE" token itself.
+       */
+      if (device_end > device_start) {
+         const size_t device_len = (size_t)(device_end - device_start);
+
+         /* If already vendor-prefixed (common on Apple), keep as-is. */
+         if (device_len >= 5 && !strncmp(device_start, "Apple", 5)) {
+            snprintf(caps->v2.renderer, sizeof(caps->v2.renderer), "%.*s",
+                     (int)device_len, device_start);
+         } else {
+            const char *angle_prefix = "ANGLE (";
+            const char *vendor_start = strstr(renderer, angle_prefix);
+            if (vendor_start == renderer) {
+               vendor_start += strlen(angle_prefix);
+               const char *vendor_end = strchr(vendor_start, ',');
+               if (vendor_end && vendor_end > vendor_start) {
+                  while (*vendor_start == ' ')
+                     vendor_start++;
+                  while (vendor_end > vendor_start && vendor_end[-1] == ' ')
+                     vendor_end--;
+               }
+
+               if (vendor_end && vendor_end > vendor_start) {
+                  snprintf(caps->v2.renderer, sizeof(caps->v2.renderer), "%.*s %.*s",
+                           (int)(vendor_end - vendor_start), vendor_start,
+                           (int)device_len, device_start);
+               } else {
+                  snprintf(caps->v2.renderer, sizeof(caps->v2.renderer), "%.*s",
+                           (int)device_len, device_start);
+               }
+            } else {
+               snprintf(caps->v2.renderer, sizeof(caps->v2.renderer), "%.*s",
+                        (int)device_len, device_start);
+            }
+         }
+      } else {
+         strncpy(caps->v2.renderer, "Generic Renderer", sizeof(caps->v2.renderer) - 1);
+         caps->v2.renderer[sizeof(caps->v2.renderer) - 1] = '\0';
+      }
+   } else if (renderer && !strncmp(renderer, "ANGLE (", 7)) {
+      /* Common ANGLE format:
+       *   "ANGLE (Apple, Apple M4 Pro, OpenGL ES 3.2 ... )"
+       * Extract the renderer field (second CSV field) and forward it without
+       * the "ANGLE" token.
+       */
+      const char *p = renderer + 7; /* after "ANGLE (" */
+      while (*p == ' ')
+         p++;
+
+      const char *vendor_start = p;
+      const char *vendor_end = strchr(vendor_start, ',');
+      if (!vendor_end)
+         goto angle_generic;
+
+      const char *device_start = vendor_end + 1;
+      while (*device_start == ' ')
+         device_start++;
+
+      const char *device_end = strchr(device_start, ',');
+      if (!device_end)
+         goto angle_generic;
+
+      while (device_end > device_start && device_end[-1] == ' ')
+         device_end--;
+
+      const size_t device_len = (size_t)(device_end - device_start);
+      if (!device_len)
+         goto angle_generic;
+
+      if (device_len >= 5 && !strncmp(device_start, "Apple", 5)) {
+         snprintf(caps->v2.renderer, sizeof(caps->v2.renderer), "%.*s",
+                  (int)device_len, device_start);
+      } else {
+         /* If not Apple-prefixed, keep the renderer field as-is (still
+          * avoiding the "ANGLE" token).
+          */
+         snprintf(caps->v2.renderer, sizeof(caps->v2.renderer), "%.*s",
+                  (int)device_len, device_start);
+      }
+   } else {
+      if (renderer)
+         strncpy(caps->v2.renderer, renderer, sizeof(caps->v2.renderer) - 1);
+      else
+         strncpy(caps->v2.renderer, "(null)", sizeof(caps->v2.renderer) - 1);
+      caps->v2.renderer[sizeof(caps->v2.renderer) - 1] = '\0';
+   }
+
+   goto angle_done;
+
+angle_generic:
+   strncpy(caps->v2.renderer, "Generic Renderer", sizeof(caps->v2.renderer) - 1);
+   caps->v2.renderer[sizeof(caps->v2.renderer) - 1] = '\0';
+
+angle_done:
 
    /* glamor reject llvmpipe, and since the renderer string is
     * composed of "virgl" and this renderer string we have to
@@ -12507,8 +13022,25 @@ static void vrend_renderer_fill_caps_v2(int gl_ver, int gles_ver,  union virgl_c
          glGetIntegerv(GL_MAX_IMAGE_SAMPLES, (GLint*)&caps->v2.max_image_samples);
    }
 
-   if (has_feature(feat_storage_multisample))
-      caps->v1.max_samples = vrend_renderer_query_multisample_caps(caps->v1.max_samples, &caps->v2);
+   /* Always call query_multisample_caps - it will handle the case where
+    * GL_ARB_texture_storage_multisample isn't available by skipping the test
+    * and returning the faked max_samples value with graceful downgrade. */
+   caps->v1.max_samples = vrend_renderer_query_multisample_caps(caps->v1.max_samples, &caps->v2);
+   virgl_debug("[VREND CAPS] After query_multisample_caps: max_samples=%u\n", caps->v1.max_samples);
+   
+   /* For macOS Metal backend: Override to max_samples=1 to trigger Mesa's fake_sw_msaa.
+    * Even though MSAA tests confirm 4 samples work, Mesa's format queries can't verify
+    * multisample support, causing MaxSamples=0. Reporting 1 triggers fake_sw_msaa workaround.
+    * Apply to both desktop GL and GLES (ANGLE) modes. */
+   const char *version = (const char *)glGetString(GL_VERSION);
+   bool is_metal = (version && strstr(version, "Metal"));
+   bool is_angle = (version && strstr(version, "ANGLE"));
+   if ((is_metal || is_angle) && caps->v1.max_samples > 1) {
+      virgl_debug("[VREND CAPS] %s backend: Overriding max_samples %u -> 1 for fake_sw_msaa\n", 
+                  is_angle ? "ANGLE" : "Metal",
+                  caps->v1.max_samples);
+      caps->v1.max_samples = 1;
+   }
 
    caps->v2.capability_bits |= VIRGL_CAP_TGSI_INVARIANT | VIRGL_CAP_SET_MIN_SAMPLES |
                                VIRGL_CAP_TGSI_PRECISE | VIRGL_CAP_APP_TWEAK_SUPPORT;
@@ -12670,6 +13202,10 @@ static void vrend_renderer_fill_caps_v2(int gl_ver, int gles_ver,  union virgl_c
             readback_str = "readback";
             set_format_bit(&caps->v2.supported_readback_formats, fmt);
          }
+         /* Only report MSAA support for formats that actually support it.
+          * Even though we fake max_samples=4 for GL 3.0 requirements, we don't
+          * advertise MSAA format support to prevent Mesa from trying to use MSAA
+          * (which would fail on Metal backend). */
          if (vrend_format_can_multisample(fmt)) {
             log_texture_feature = true;
             multisample_str = "multisample";
@@ -12782,6 +13318,10 @@ static void vrend_renderer_fill_caps_v2(int gl_ver, int gles_ver,  union virgl_c
    // we can pass "1" as size on drm path, but not on vtest.
    caps->v2.capability_bits_v2 |= VIRGL_CAP_V2_COPY_TRANSFER_BOTH_DIRECTIONS;
 
+   /* On Core Profile hosts, advertise this so guest can cap compatibility level */
+   if (vrend_state.use_core_profile)
+      caps->v2.capability_bits_v2 |= VIRGL_CAP_V2_CORE_PROFILE_ONLY;
+
    if (has_feature(feat_anisotropic_filter)) {
       float max_aniso;
       glGetFloatv(GL_MAX_TEXTURE_MAX_ANISOTROPY, &max_aniso);
@@ -12801,6 +13341,7 @@ static void vrend_renderer_fill_caps_v2(int gl_ver, int gles_ver,  union virgl_c
    if (has_feature(feat_ubo)) {
       glGetIntegerv(GL_MAX_UNIFORM_BLOCK_SIZE, &max);
       caps->v2.max_uniform_block_size = max;
+      virgl_debug("[VREND CAPS] GL_MAX_UNIFORM_BLOCK_SIZE=%d (Mesa needs >=16384 for UBO)\n", max);
    }
 
    /* Propagate the max of Uniform Components */
@@ -12911,6 +13452,11 @@ void vrend_renderer_fill_caps(uint32_t set, uint32_t version,
       return;
 
    vrend_renderer_fill_caps_v2(gl_ver, gles_ver, caps);
+
+   /* Final caps report */
+   virgl_debug("[VREND CAPS] FINAL VALUES: glsl_level=%u, max_samples=%u\n", 
+               caps->v1.glsl_level, caps->v1.max_samples);
+
 }
 
 GLint64 vrend_renderer_get_timestamp(void)
@@ -13109,27 +13655,35 @@ void vrend_renderer_resource_get_info(struct pipe_resource *pres,
    info->stride = util_format_get_nblocksx(res->base.format, u_minify(res->base.width0, 0)) * elsize;
 }
 
-int
-vrend_renderer_resource_d3d11_texture2d(struct pipe_resource *pres, void **d3d_tex2d)
+void *
+vrend_renderer_resource_d3d11_texture2d(struct pipe_resource *pres)
 {
 #ifdef WIN32
    struct vrend_resource *res = (struct vrend_resource *)pres;
 
-   if (!vrend_state.d3d_share_texture)
-      return 0;
-
-   if (!res->d3d_tex2d)
-      return EINVAL;
-
-   *d3d_tex2d = res->d3d_tex2d;
-   return 0;
+   if (!vrend_state.native_share_texture)
+      return NULL;
+   else
+      return res->d3d_tex2d;
 #else
    (void)pres;
-   (void)d3d_tex2d;
-   return ENOTSUP;
+   return NULL;
 #endif
 }
 
+#ifdef ENABLE_METAL
+MTLTexture_id
+vrend_renderer_resource_metal_texture(struct pipe_resource *pres)
+{
+   struct vrend_resource *res = (struct vrend_resource *)pres;
+
+   if (!vrend_state.native_share_texture)
+      return NULL;
+   else
+      return res->metal_texture;
+}
+#endif
+
 void vrend_renderer_get_cap_set(uint32_t cap_set, uint32_t *max_ver,
                                 uint32_t *max_size)
 {
@@ -13341,6 +13895,35 @@ struct pipe_resource *vrend_get_blob_pipe(struct vrend_context *ctx, uint64_t bl
    return NULL;
 }
 
+void vrend_renderer_borrow_texture_for_scanout(struct pipe_resource *pres)
+{
+   struct vrend_texture *tex = (struct vrend_texture *)pres;
+   struct vrend_format_table *tex_conv = &tex_conv_table[tex->base.base.format];
+
+   assert(tex->base.target == GL_TEXTURE_2D);
+   assert(!util_format_is_depth_or_stencil(tex->base.base.format));
+
+   glBindTexture(GL_TEXTURE_2D, tex->base.gl_id);
+
+   if (tex_conv->flags & VIRGL_TEXTURE_NEED_SWIZZLE) {
+      for (unsigned i = 0; i < ARRAY_SIZE(tex->cur_swizzle); ++i) {
+         GLint next_swizzle = to_gl_swizzle(tex_conv->swizzle[i]);
+         if (tex->cur_swizzle[i] != next_swizzle) {
+            glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_SWIZZLE_R + i, next_swizzle);
+            tex->cur_swizzle[i] = next_swizzle;
+         }
+      }
+   }
+
+   if (tex->cur_srgb_decode != GL_DECODE_EXT && util_format_is_srgb(tex->base.base.format)) {
+      if (has_feature(feat_texture_srgb_decode)) {
+         glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_SRGB_DECODE_EXT,
+                         GL_DECODE_EXT);
+         tex->cur_srgb_decode = GL_DECODE_EXT;
+      }
+   }
+}
+
 int
 vrend_renderer_pipe_resource_set_type(struct vrend_context *ctx,
                                       uint32_t res_id,
@@ -13389,9 +13972,6 @@ vrend_renderer_pipe_resource_set_type(struct vrend_context *ctx,
       };
       struct vrend_resource *gr;
 
-      if (res->fd_type != VIRGL_RESOURCE_FD_DMABUF)
-         return EINVAL;
-
       gr = vrend_resource_create(&create_args);
       if (!gr)
          return ENOMEM;
@@ -13399,6 +13979,10 @@ vrend_renderer_pipe_resource_set_type(struct vrend_context *ctx,
 #ifdef HAVE_EPOXY_EGL_H
       if (egl) {
 #ifdef ENABLE_GBM
+         if (res->fd_type != VIRGL_RESOURCE_FD_DMABUF) {
+            FREE(gr);
+            return EINVAL;
+         }
          int plane_fds[VIRGL_GBM_MAX_PLANES];
          uint32_t virgl_format;
          uint32_t drm_format;
@@ -13439,7 +14023,53 @@ vrend_renderer_pipe_resource_set_type(struct vrend_context *ctx,
             return ret;
          }
 
-#else /* ENABLE_GBM */
+#elif defined(ENABLE_METAL)
+         int ret;
+
+         if (res->fd_type != VIRGL_RESOURCE_METAL_HEAP) {
+            FREE(gr);
+            return EINVAL;
+         }
+         if (args->plane_count > 1) {
+            virgl_warn("%s: ignoring plane_count = %d and using the first one\n",
+                       __func__, args->plane_count);
+         }
+         const struct vrend_metal_texture_description desc = {
+            .width = args->width,
+            .height = args->height,
+            .stride = args->plane_strides[0],
+            .offset = args->plane_offsets[0],
+            .bind = args->bind,
+            .usage = args->usage,
+            .format = args->format,
+         };
+         MTLTexture_id texture;
+         
+         if (!virgl_metal_create_texture_from_heap(res->metal_heap,
+                                                   &desc,
+                                                   &texture)) {
+            FREE(gr);
+            virgl_error("%s: failed to create texture from MTLHeap\n", __func__);
+            return EINVAL;
+         }
+         gr->egl_image = virgl_egl_metal_image_from_texture(egl, texture);
+         virgl_metal_release_texture(texture);
+         if (!gr->egl_image) {
+            virgl_error("%s: failed to create egl image\n", __func__);
+            FREE(gr);
+            return EINVAL;
+         }
+
+         gr->storage_bits |= VREND_STORAGE_EGL_IMAGE;
+         gr->is_imported = true;
+
+         ret = vrend_resource_alloc_texture(gr, args->format, gr->egl_image);
+         if (ret) {
+            virgl_egl_image_destroy(egl, gr->egl_image);
+            FREE(gr);
+            return ret;
+         }
+#else /* !ENABLE_METAL && !ENABLE_GBM */
          FREE(gr);
          virgl_error("%s: no EGL/GBM support \n", __func__);
          return EINVAL;
diff --git a/src/vrend/vrend_renderer.h b/src/vrend/vrend_renderer.h
index adb1213..3ca2c9e 100644
--- a/src/vrend/vrend_renderer.h
+++ b/src/vrend/vrend_renderer.h
@@ -42,6 +42,9 @@
 #ifdef WIN32
 #include <d3d11.h>
 #endif
+#ifdef ENABLE_METAL
+#include "vrend_metal.h"
+#endif
 
 #ifdef ENABLE_TESTS
 /* With this flag set, the transfer will not try to use GBM mappings.
@@ -77,7 +80,7 @@ struct vrend_context;
 #define VREND_STORAGE_HOST_SYSTEM_MEMORY BIT(5)
 #define VREND_STORAGE_GL_IMMUTABLE       BIT(6)
 #define VREND_STORAGE_GL_MEMOBJ          BIT(7)
-#define VREND_STORAGE_D3D_TEXTURE        BIT(8)
+#define VREND_STORAGE_NATIVE_TEXTURE     BIT(8)
 
 struct vrend_resource {
    struct pipe_resource base;
@@ -107,6 +110,9 @@ struct vrend_resource {
 #ifdef WIN32
    ID3D11Texture2D *d3d_tex2d;
 #endif
+#ifdef ENABLE_METAL
+   MTLTexture_id metal_texture;
+#endif
 
    uint64_t size;
    GLbitfield buffer_storage_flags;
@@ -194,7 +200,7 @@ struct vrend_if_cbs {
 #define VREND_USE_EXTERNAL_BLOB (1 << 1)
 #define VREND_USE_ASYNC_FENCE_CB (1 << 2)
 #define VREND_USE_VIDEO          (1 << 3)
-#define VREND_D3D11_SHARE_TEXTURE (1 << 4)
+#define VREND_NATIVE_SHARE_TEXTURE (1 << 4)
 #define VREND_USE_COMPAT_CONTEXT (1 << 5)
 #define VREND_USE_GLES (1 << 6)
 #define VREND_USE_GBM_LAYOUT (1 << 7)
@@ -564,6 +570,8 @@ struct vrend_blit_info {
 void vrend_renderer_resource_get_info(struct pipe_resource *pres,
                                       struct vrend_renderer_resource_info *info);
 
+void vrend_renderer_borrow_texture_for_scanout(struct pipe_resource *pres);
+
 void vrend_renderer_get_cap_set(uint32_t cap_set, uint32_t *max_ver,
                                 uint32_t *max_size);
 
@@ -612,6 +620,8 @@ static const struct gl_version gl_versions[] = { {4,6}, {4,5}, {4,4}, {4,3}, {4,
 
 extern const struct vrend_if_cbs *vrend_clicbs;
 
+bool vrend_renderer_video_available(void);
+
 int vrend_renderer_export_query(struct pipe_resource *pres,
                                 struct virgl_renderer_export_query *export_query);
 
@@ -640,11 +650,16 @@ void vrend_context_emit_string_marker(struct vrend_context *ctx, GLsizei length,
 
 struct vrend_video_context *vrend_context_get_video_ctx(struct vrend_context *ctx);
 
-int
-vrend_renderer_resource_d3d11_texture2d(struct pipe_resource *res, void **handle);
+void *
+vrend_renderer_resource_d3d11_texture2d(struct pipe_resource *res);
 
 int
 vrend_renderer_pipe_resource_get_layout(struct vrend_context *ctx,
                                         uint32_t out_res_id, uint32_t res_id);
 
+#ifdef ENABLE_METAL
+MTLTexture_id
+vrend_renderer_resource_metal_texture(struct pipe_resource *pres);
+#endif
+
 #endif
diff --git a/src/vrend/vrend_shader.c b/src/vrend/vrend_shader.c
index 6a26fdf..1a4e30a 100644
--- a/src/vrend/vrend_shader.c
+++ b/src/vrend/vrend_shader.c
@@ -6301,10 +6301,12 @@ static void emit_header(const struct dump_ctx *ctx, struct vrend_glsl_strbufs *g
 
       if (ctx->prog_type == TGSI_PROCESSOR_VERTEX && ctx->cfg->use_explicit_locations)
          emit_ext(glsl_strbufs, "ARB_explicit_attrib_location", "require");
-      if (ctx->prog_type == TGSI_PROCESSOR_FRAGMENT && fs_emit_layout(ctx))
+      /* Core GLSL 150 already includes fragment coord layouts; avoid extension require on hosts that omit the ARB string (e.g., macOS core GL). */
+      if (ctx->prog_type == TGSI_PROCESSOR_FRAGMENT && fs_emit_layout(ctx) && ctx->glsl_ver_required < 150)
          emit_ext(glsl_strbufs, "ARB_fragment_coord_conventions", "require");
 
-      if (ctx->ubo_used_mask)
+      /* Uniform buffers are core in GLSL 1.40+; only request the ARB extension when targeting older versions. */
+      if (ctx->ubo_used_mask && ctx->glsl_ver_required < 140)
          emit_ext(glsl_strbufs, "ARB_uniform_buffer_object", "require");
 
       if (ctx->num_cull_dist_prop || ctx->key->num_in_cull || ctx->key->num_out_cull)
@@ -6389,7 +6391,7 @@ static const char *get_interp_string(const struct vrend_shader_cfg *cfg, enum tg
       else
          return "";
    case TGSI_INTERPOLATE_PERSPECTIVE:
-      return "smooth ";
+      return "";
    case TGSI_INTERPOLATE_CONSTANT:
       return "flat ";
    case TGSI_INTERPOLATE_COLOR:
@@ -8179,6 +8181,9 @@ bool vrend_convert_shader(const struct vrend_context *rctx,
    if (bret == false)
       goto fail;
 
+   if (ctx.shader_req_bits & SHADER_REQ_INTS)
+      ctx.glsl_ver_required = require_glsl_ver(&ctx, 150);
+
    if (ctx.shader_req_bits & SHADER_REQ_FP64)
       ctx.glsl_ver_required = require_glsl_ver(&ctx, 150);
 
diff --git a/src/vrend/vrend_winsys_egl.c b/src/vrend/vrend_winsys_egl.c
index 4ddcd0d..418f66f 100644
--- a/src/vrend/vrend_winsys_egl.c
+++ b/src/vrend/vrend_winsys_egl.c
@@ -32,6 +32,22 @@
 
 #define EGL_EGLEXT_PROTOTYPES
 #include <errno.h>
+
+/* ANGLE-specific EGL extension attributes for Metal.
+ * These are defined in ANGLE's eglext_angle.h but we can't include it
+ * directly because epoxy/egl.h sets __eglext_h_ which prevents the
+ * system eglext.h from being included. Define them here instead.
+ */
+#ifdef ENABLE_METAL
+#ifndef EGL_METAL_DEVICE_ANGLE
+#define EGL_METAL_DEVICE_ANGLE 0x34A6
+#endif
+
+#ifndef EGL_METAL_TEXTURE_ANGLE
+#define EGL_METAL_TEXTURE_ANGLE 0x34A7
+#endif
+#endif
+
 #include <fcntl.h>
 #ifdef WIN32
 #include <d3d11.h>
@@ -110,6 +126,9 @@ struct virgl_egl {
 #ifdef WIN32
    ID3D11Device *d3d11_device;
 #endif
+#ifdef ENABLE_METAL
+   MTLDevice_id metal_device;
+#endif
 };
 
 static bool virgl_egl_has_extension_in_string(const char *haystack, const char *needle)
@@ -575,6 +594,66 @@ virgl_egl_win32_init(UNUSED struct virgl_egl *egl)
 #endif
 }
 
+static void
+virgl_egl_metal_init(UNUSED struct virgl_egl *egl)
+{
+#ifdef ENABLE_METAL
+   EGLDeviceEXT device;
+   const char* device_ext = NULL;
+   MTLDevice_id metal_device;
+
+   if (!has_bits(egl->extension_bits, EGL_EXT_DEVICE_QUERY))
+      return;
+
+   if (!egl->funcs.eglQueryDisplayAttrib(egl->egl_display, EGL_DEVICE_EXT, (EGLAttrib*)&device))
+      return;
+
+   device_ext = egl->funcs.eglQueryDeviceString(device, EGL_EXTENSIONS);
+   if (!device_ext)
+      return;
+
+   if (!virgl_egl_has_extension_in_string(device_ext, "EGL_ANGLE_device_metal"))
+      return;
+
+   if (!egl->funcs.eglQueryDeviceAttrib(device, EGL_METAL_DEVICE_ANGLE, (EGLAttrib*)&metal_device))
+      return;
+
+   egl->metal_device = metal_device;
+#endif
+}
+
+#ifdef ENABLE_METAL
+bool virgl_egl_metal_create_texture(struct virgl_egl *egl, struct pipe_resource *res,
+                                    uint32_t format, MTLTexture_id *tex)
+{
+   MTLDevice_id device = egl->metal_device;
+   struct vrend_metal_texture_description desc = {
+      .width = res->width0,
+      .height = res->height0,
+      .bind = res->bind,
+      .usage = res->usage,
+      .format = format,
+   };
+
+   return virgl_metal_create_texture(device, &desc, tex);
+}
+
+EGLImageKHR
+virgl_egl_metal_image_from_texture(struct virgl_egl *egl, MTLTexture_id tex)
+{
+   const EGLint attribs[] = {
+      EGL_NONE
+   };
+
+   if (!egl)
+      return NULL;
+
+   return eglCreateImageKHR(egl->egl_display, EGL_NO_CONTEXT,
+                            EGL_METAL_TEXTURE_ANGLE, (EGLClientBuffer)tex,
+                            attribs);
+}
+#endif
+
 struct virgl_egl *virgl_egl_init_external(EGLDisplay egl_display)
 {
    const char *extensions;
@@ -610,6 +689,7 @@ struct virgl_egl *virgl_egl_init_external(EGLDisplay egl_display)
 #endif
 
    virgl_egl_win32_init(egl);
+   virgl_egl_metal_init(egl);
    return egl;
 fail:
    free(egl);
@@ -838,7 +918,9 @@ void *virgl_egl_image_from_dmabuf(struct virgl_egl *egl,
                                     (EGLClientBuffer)NULL,
                                     attrs);
 }
+#endif
 
+#if defined(ENABLE_GBM) || defined(ENABLE_METAL)
 void virgl_egl_image_destroy(struct virgl_egl *egl, void *image)
 {
    eglDestroyImageKHR(egl->egl_display, image);
diff --git a/src/vrend/vrend_winsys_egl.h b/src/vrend/vrend_winsys_egl.h
index 4af897b..0e8eb7a 100644
--- a/src/vrend/vrend_winsys_egl.h
+++ b/src/vrend/vrend_winsys_egl.h
@@ -32,6 +32,9 @@
 #ifdef WIN32
 #include <d3d11.h>
 #endif
+#ifdef ENABLE_METAL
+#include "vrend_metal.h"
+#endif
 
 struct virgl_egl;
 
@@ -80,12 +83,15 @@ void *virgl_egl_image_from_dmabuf(struct virgl_egl *egl,
                                   const int *plane_fds,
                                   const uint32_t *plane_strides,
                                   const uint32_t *plane_offsets);
-void virgl_egl_image_destroy(struct virgl_egl *egl, void *image);
 
 void *virgl_egl_image_from_gbm_bo(struct virgl_egl *egl, struct gbm_bo *bo);
 void *virgl_egl_aux_plane_image_from_gbm_bo(struct virgl_egl *egl, struct gbm_bo *bo, int plane);
 #endif
 
+#if defined(ENABLE_GBM) || defined(ENABLE_METAL)
+void virgl_egl_image_destroy(struct virgl_egl *egl, void *image);
+#endif
+
 bool virgl_egl_supports_fences(struct virgl_egl *egl);
 EGLSyncKHR virgl_egl_fence_create(struct virgl_egl *egl);
 void virgl_egl_fence_destroy(struct virgl_egl *egl, EGLSyncKHR fence);
@@ -101,4 +107,10 @@ bool virgl_egl_win32_create_d3d11_texture2d(struct virgl_egl *egl,
 EGLImageKHR virgl_egl_win32_image_from_d3d11_texture2d(struct virgl_egl *egl, ID3D11Texture2D *tex);
 #endif
 
+#ifdef ENABLE_METAL
+bool virgl_egl_metal_create_texture(struct virgl_egl *egl, struct pipe_resource *res,
+                                    uint32_t format, MTLTexture_id *tex);
+EGLImageKHR virgl_egl_metal_image_from_texture(struct virgl_egl *egl, MTLTexture_id tex);
+#endif
+
 #endif
